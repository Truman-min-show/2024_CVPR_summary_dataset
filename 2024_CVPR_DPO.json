[
  {
    "prompt": "summary: Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial net works (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representa tions, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsam plers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network archi tectures with emerging 3D Gaussian rendering primitives using an articulable multi shell–based scaffold. In this set ting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose at tributes are encoded in the texture features. These Gaus sians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary user defined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves high quality multi-view consistent renderings at a native resolu tion of 512 × 512 pixels. We demonstrate that GSMs suc cessfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion.",
    "chosen": "Gaussian Shell Maps (GSMs) are introduced to enhance 3D human generation efficiency. This framework combines generator network architectures with 3D Gaussian rendering primitives using an articulable multi-shell scaffold. A CNN generates a 3D texture stack, mapping features to shells representing inflated/deflated versions of a template surface. Instead of direct rasterization, 3D Gaussians are sampled on the shells, with attributes encoded in texture features. This method enables efficient, differentiable rendering, bypassing the need for view-inconsistent upsamplers and achieving high-quality multi-view consistent renderings at 512 × 512 pixels. GSMs successfully generate 3D humans when trained on single-view datasets like SHHQ and DeepFashion.",
    "rejected": "Efficient generation of 3D digital humans using Gaussian Shell Maps that connect generator networks with 3D Gaussian rendering primitives. This approach allows for fast, differentiable rendering and supports multi-view consistent outputs at high resolutions without requiring upsamplers."
  },
  {
    "prompt": "summary: Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-ﬁnders (judges and juries) can demonstrate considerable variabil ity in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas im ages of completely different scenes can be deemed similar enough to support a claim of copying. We seek to deﬁne and compute a notion of ‘conceptual similarity’ among images that captures high-level relations even among images that do not share repeated elements or visually similar compo nents. The idea is to use a base multi-modal model to gen erate ‘explanations’ (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be dis criminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this deﬁnition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks. Beyond just providing a number, our method also offers interpretability by pointing to the speciﬁc level of granularity of the description where the source data are differentiated.",
    "chosen": "A novel method called Complexity-Constrained Descriptive Autoencoding (CC:DAE) is proposed to define and compute 'conceptual similarity' among images. This method uses a base multi-modal model to generate 'explanations' (captions) of visual data at increasing complexity levels. Similarity is measured by the caption length needed to discriminate between two images. CC:DAE correlates with subjective human assessments and outperforms existing baselines on image-to-image and text-to-text similarity benchmarks. It also offers interpretability by indicating the description granularity level where source data differentiation occurs.",
    "rejected": "This work introduces a method to quantify 'conceptual similarity' among images by generating explanations of increasing complexity. The length of these descriptions needed to differentiate between two images defines their similarity, providing an interpretable metric aligned with human assessments."
  },
  {
    "prompt": "summary: Vision-Language Models (VLMs) such as CLIP are trained on large amounts of image-text pairs, resulting in remarkable generalization across several data distribu tions. However, in several cases, their expensive train ing and data collection/curation costs do not justify the end application. This motivates a vendor-client paradigm, where a vendor trains a large-scale VLM and grants only input-output access to clients on a pay-per-query basis in a black-box setting. The client aims to minimize inference cost by distilling the VLM to a student model using the limited available task-specific data, and further deploying this student model in the downstream application. While naive distillation largely improves the In-Domain (ID) ac curacy of the student, it fails to transfer the superior out of-distribution (OOD) generalization of the VLM teacher using the limited available labeled images. To mitigate this, we propose Vision-Language to Vision  Align, Dis till, Predict (VL2V-ADiP), which first aligns the vision and language modalities of the teacher model with the vision modality of a pre-trained student model, and further distills the aligned VLM representations to the student. This max imally retains the pre-trained features of the student, while also incorporating the rich representations of the VLM im age encoder and the superior generalization of the text em beddings. The proposed approach achieves state-of-the-art results on the standard Domain Generalization benchmarks in a black-box teacher setting as well as a white-box setting where the weights of the VLM are accessible.",
    "chosen": "VL2V-ADiP is introduced to improve domain generalization in image classification by leveraging Vision-Language Models (VLMs). This method first aligns the vision and language modalities of a VLM teacher with a pre-trained student model's vision modality, then distills the aligned VLM representations to the student. This approach retains the student's pre-trained features while incorporating the rich representations of the VLM image encoder and superior generalization of text embeddings. VL2V-ADiP achieves state-of-the-art results on standard Domain Generalization benchmarks in both black-box and white-box teacher settings.",
    "rejected": "Vision-Language Models trained on large datasets achieve remarkable generalization but can be costly. A proposed distillation method called VL2V-ADiP aligns and transfers knowledge from a VLM teacher to a student model, maintaining pre-trained features while incorporating rich representations for improved out-of-distribution performance."
  },
  {
    "prompt": "summary: Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural ra diance ﬁelds, i.e., the densities double when scene size is halved, and vice versa. We call this property alpha invari ance. For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic ini tialization strategy to guarantee high ray transmittance. We revisit a few popular radiance ﬁeld models and ﬁnd that these systems use various heuristics to deal with issues aris ing from scene scaling. We test their behaviors and show our recipe to be more robust. Visit our",
    "chosen": "This paper addresses alpha invariance in Neural Radiance Fields (NeRFs), where scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities. To maintain alpha invariance, the authors recommend parameterizing both distance and volume densities in log space and using a discretization-agnostic initialization strategy for high ray transmittance. They revisit popular radiance field models, finding varied heuristics for handling scene scaling issues. Testing shows their approach is more robust across different NeRF methods and scene scalings.",
    "rejected": "Addressing scale-ambiguity in neural radiance fields, this paper recommends parameterizing distance and volume densities in log space for robustness. An initialization strategy ensuring high ray transmittance is also proposed, enhancing consistency across different scene scalings."
  },
  {
    "prompt": "summary: Unsupervised (US) video anomaly detection (VAD) in surveillance applications is gaining more popularity re cently due to its practical real-world applications. As surveillance videos are privacy sensitive and the availabil ity of large-scale video data may enable better US-VAD sys tems, collaborative learning can be highly rewarding in this setting. However, due to the extremely challenging nature of the US-VAD task, where learning is carried out without any annotations, privacy-preserving collaborative learning of US-VAD systems has not been studied yet. In this pa per, we propose a new baseline for anomaly detection ca pable of localizing anomalous events in complex surveil lance videos in a fully unsupervised fashion without any la bels on a privacy-preserving participant-based distributed training configuration. Additionally, we propose three new evaluation protocols to benchmark anomaly detection ap proaches on various scenarios of collaborations and data availability. Based on these protocols, we modify existing VAD datasets to extensively evaluate our approach as well as existing US SOTA methods on two large-scale datasets including UCF-Crime and XD-Violence.",
    "chosen": "CLAP is proposed as a new baseline for unsupervised video anomaly detection in surveillance, capable of localizing anomalies without labels in a privacy-preserving distributed training setup. The authors also introduce three new evaluation protocols to benchmark anomaly detection approaches across various collaboration scenarios and data availabilities. They modify existing VAD datasets to extensively evaluate their approach and existing US SOTA methods on large-scale datasets like UCF-Crime and XD-Violence. All evaluation protocols, dataset splits, and codes are made available.",
    "rejected": "An unsupervised video anomaly detection technique suitable for privacy-preserving collaborative learning is introduced. It localizes anomalies in surveillance videos without labels and proposes new evaluation protocols tested on large-scale datasets including UCF-Crime and XD-Violence."
  },
  {
    "prompt": "summary: This paper addresses complex challenges in histopatho logical image analysis through three key contributions. Firstly, it introduces a fast patch selection method, FPS, for whole-slide image (WSI) analysis, significantly reduc ing computational cost while maintaining accuracy. Sec ondly, it presents PathDino, a lightweight histopathol ogy feature extractor with a minimal configuration of five Transformer blocks and only ≈9 million parameters, markedly fewer than alternatives. Thirdly, it introduces a rotation-agnostic representation learning paradigm using self-supervised learning, effectively mitigating overfitting. We also show that our compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including both internal datasets spanning four sites (breast, liver, skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS, DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training dataset of ≈6 million histopathol ogy patches from The Cancer Genome Atlas (TCGA), our approach demonstrates an average 8.5% improvement in patch-level majority vote performance. These contributions provide a robust framework for enhancing image analysis in digital pathology, rigorously validated through extensive evaluation. 1",
    "chosen": "This paper presents three key contributions for histopathological image analysis: a fast patch selection method (FPS) for whole-slide image (WSI) analysis, PathDino (a lightweight histopathology feature extractor with five Transformer blocks and ≈9 million parameters), and a rotation-agnostic representation learning paradigm using self-supervised learning. The compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including internal and public datasets. It shows an average 8.5% improvement in patch-level majority vote performance with a training dataset of ≈6 million histopathology patches from TCGA.",
    "rejected": "A framework for histopathological image analysis through a fast patch selection method (FPS), PathDino feature extractor, and rotation-agnostic representation learning reduces computational costs and overfitting. The compact model outperforms state-of-the-art alternatives on various datasets, demonstrating significant improvements in accuracy."
  },
  {
    "prompt": "summary: Image and video analysis requires not only accurate ob ject detection but also the understanding of relationships among detected objects. Common solutions to relation mod eling typically resort to stand-alone object detectors fol lowed by non-differentiable post-processing techniques. Re cently introduced detection transformers (DETR) perform end-to-end object detection based on a bipartite matching loss. Such methods, however, lack the ability to jointly detect objects and resolve object associations. In this paper, we build on the DETR approach and extend it to the joint de tection of objects and their relationships by introducing an approximated bipartite matching. While our method can gen eralize to an arbitrary number of objects, we here focus on the modeling of object pairs and their relations. In particular, we apply our method PairDETR to the problem of detect ing human bodies and faces, and associating them for the same person. Our approach not only eliminates the need for hand-designed post-processing but also achieves excellent results for body-face associations. We evaluate PairDETR on the challenging CrowdHuman and CityPersons datasets and demonstrate a large improvement over the state of the art.",
    "chosen": "PairDETR extends the DETR approach to jointly detect objects and resolve their associations by introducing an approximated bipartite matching. The method focuses on modeling object pairs and their relations, particularly for detecting human bodies and faces and associating them for the same person. PairDETR eliminates the need for hand-designed post-processing and achieves excellent results for body-face associations. It is evaluated on CrowdHuman and CityPersons datasets, demonstrating significant improvement over the state of the art.",
    "rejected": "Building upon DETR, PairDETR extends object detection to include relationship modeling via approximated bipartite matching. This end-to-end approach eliminates the need for post-processing steps and achieves superior results for detecting and associating human bodies and faces."
  },
  {
    "prompt": "summary: The reflective nature of the human eye is an underappre ciated source of information about what the world around us looks like. By imaging the eyes of a moving person, we capture multiple views of a scene outside the camera’s di rect line of sight through the reflections in the eyes. In this paper, we reconstruct a radiance field beyond the camera’s line of sight using portrait images containing eye reflec tions. This task is challenging due to 1) the difficulty of ac curately estimating eye poses and 2) the entangled appear ance of the iris textures and the scene reflections. To address these, our method jointly optimizes the cornea poses, the ra diance field depicting the scene, and the observer’s eye iris texture. We further present a regularization prior on the iris texture to improve scene reconstruction quality. Through various experiments on synthetic and real-world captures featuring people with varied eye colors, and lighting con ditions, we demonstrate the feasibility of our approach to recover the radiance field using cornea reflections. *Equal contribution",
    "chosen": "This paper reconstructs a radiance field beyond the camera’s line of sight using portrait images containing eye reflections. The method jointly optimizes cornea poses, the scene’s radiance field, and the observer’s eye iris texture, with a regularization prior on the iris texture for improved scene reconstruction quality. Experiments on synthetic and real-world captures show the approach’s feasibility in recovering the radiance field using cornea reflections, regardless of eye color and lighting conditions.",
    "rejected": "Reconstructing radiance fields beyond the camera's line of sight using eye reflections captured in portrait images. By optimizing cornea poses, radiance field, and iris texture, this method demonstrates feasibility in recovering scenes reflected in eyes under varied conditions."
  },
  {
    "prompt": "summary: We introduce FaceTalk1, a novel generative approach de signed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural para metric head models to create high-fidelity, temporally co herent motion sequences. We propose a new latent diffu sion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realis tic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the 1",
    "chosen": "FaceTalk is a novel generative approach for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio. It couples speech signals with the latent space of neural parametric head models, using a new latent diffusion model in the expression space of these models. In the absence of a dataset with NPHM expressions corresponding to audio, the authors optimize for these correspondences. FaceTalk generates plausible motion sequences for high-fidelity head animation, outperforming existing methods by 75% in perceptual user study evaluation.",
    "rejected": "FaceTalk1 synthesizes high-fidelity 3D motion sequences of talking heads from audio signals using a latent diffusion model in expression space. It couples speech with neural parametric head models to create realistic, temporally coherent sequences optimized from audio-video data."
  },
  {
    "prompt": "summary: Learning representations to capture the very fundamental understanding of the world is a key challenge in machine learning. The hierarchical structure of explanatory factors hidden in data is such a general representation and could be potentially achieved with a hierarchical VAE. However, train ing a hierarchical VAE always suffers from the “posterior collapse”, where the data information is hard to propagate to the higher-level latent variables, hence resulting in a bad hierarchical representation. To address this issue, we first an alyze the shortcomings of existing methods for mitigating the posterior collapse from an information theory perspective, then highlight the necessity of regularization for explicitly propagating data information to higher-level latent variables while maintaining the dependency between different levels. This naturally leads to formulating the inference of the hi erarchical latent representation as a sequential decision process, which could benefit from applying reinforcement learning (RL). Aligning RL’s objective with the regulariza tion, we first introduce a skip-generative path to acquire a reward for evaluating the information content of an in ferred latent representation, and then the developed Q-value function based on it could have a consistent optimization direction of the regularization. Finally, policy gradient, one of the typical RL methods, is employed to train a hierarchical VAE without introducing a gradient estimator. Experimental results firmly support our analysis and demonstrate that our proposed method effectively mitigates the posterior collapse issue, learns an informative hierarchy, acquires explainable latent representations, and significantly outperforms other hierarchical VAE-based methods in downstream tasks.",
    "chosen": "A method combining reinforcement learning (RL) is proposed to improve unsupervised hierarchical representation learning in hierarchical VAEs, addressing the posterior collapse issue. The approach involves analyzing existing methods' shortcomings, emphasizing regularization for data information propagation to higher-level latent variables, and formulating hierarchical latent representation inference as a sequential decision process. RL is applied using a skip-generative path for reward acquisition and a Q-value function for consistent optimization. Policy gradient trains the hierarchical VAE without introducing a gradient estimator. Experimental results show the method effectively mitigates posterior collapse, learns informative hierarchies, and significantly outperforms other hierarchical VAE-based methods in downstream tasks.",
    "rejected": "Innovative methods in generating 3D digital humans, measuring image similarity, improving Vision-Language Models, addressing scale ambiguities, detecting anomalies, analyzing histopathological images, extending object detection, reconstructing scenes, and synthesizing head motions are highlighted. Each addresses specific challenges within its domain."
  },
  {
    "prompt": "summary: Human perception and understanding is a major domain of computer vision which, like many other vision subdo mains recently, stands to gain from the use of large mod els pre-trained on large datasets. We hypothesize that the most common pre-training strategy of relying on general purpose, object-centric image datasets such as ImageNet, is limited by an important domain shift. On the other hand, collecting domain-specific ground truth such as 2D or 3D labels does not scale well. Therefore, we propose a pre-training approach based on self-supervised learning that works on human-centric data using only images. Our method uses pairs of images of humans: the first is par tially masked and the model is trained to reconstruct the masked parts given the visible ones and a second image. It relies on both stereoscopic (cross-view) pairs, and tempo ral (cross-pose) pairs taken from videos, in order to learn priors about 3D as well as human motion. We pre-train a model for body-centric tasks and one for hand-centric tasks. With a generic transformer architecture, these models out perform existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and obtain state-of-the-art performance for instance when fine-tuning for model-based and model-free human mesh recovery.",
    "chosen": "A self-supervised pre-training method for human-centric vision tasks is proposed, using pairs of human images with masking and reconstruction. It leverages stereoscopic and temporal pairs to learn 3D and motion priors, outperforming existing methods on human-centric downstream tasks.",
    "rejected": "We propose a self-supervised learning approach for human-centric tasks using transformer models. By training on image pairs of humans with one partially masked, our method learns from cross-view and cross-pose data, outperforming existing techniques in human mesh recovery."
  },
  {
    "prompt": "summary: We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video’s current path of execution in a certain way, the goal is to find a re lated “detour video” that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to’s us ing video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model’s significant improve ments over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.",
    "chosen": "VidDetours is introduced to solve the video detours problem, retrieving targeted temporal segments from how-to videos using video-and-text conditioned queries. It creates weakly supervised training data from video narration text, significantly improving video retrieval and question answering in cooking videos.",
    "rejected": "Introducing VidDetours, a novel video-language model designed to navigate instructional videos by retrieving related segments based on user queries. This system leverages weakly supervised training data derived from how-to video narrations, significantly improving recall rates over state-of-the-art methods."
  },
  {
    "prompt": "summary: Generative AI (GenAI) is transforming creative work flows through the capability to synthesize and manipulate images via high-level prompts. Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a syn thetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input train ing images using imperceptible watermarks, and the diffu sion models (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as 216 unique water marks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attri bution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images.",
    "chosen": "ProMark is a causal attribution technique for GenAI, embedding watermarks in training images to attribute synthetic images to their training data concepts. It outperforms correlation-based attribution while maintaining image quality.",
    "rejected": "ProMark is a causal attribution technique that embeds imperceptible watermarks into training images for generative AI models. These watermarks allow attributing synthetic images back to their original concepts, providing recognition or reward to creatives whose work contributed to the training dataset."
  },
  {
    "prompt": "summary: Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly rel evant for evaluating computer vision algorithms. Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential. To ad dress this issue, we introduce OpenStreetView-5M, a large scale, open-access dataset comprising over 5.1 million geo referenced street view images, covering 225 countries and territories. In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization. To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-the art image encoders, spatial representations, and training strategies. All associated codes and models can be found at github.com/gastruc/osv5m.",
    "chosen": "OpenStreetView-5M, a large-scale, open-access geolocation dataset with over 5.1 million geo-referenced street view images, is introduced. It enables evaluation of geographical feature learning beyond memorization, benchmarking various image encoders, spatial representations, and training strategies.",
    "rejected": "OpenStreetView-5M (OSV-5M) offers a large-scale open-access dataset of 5.1 million georeferenced street view images for global geolocation tasks. The dataset enables robust evaluation of computer vision algorithms beyond memorization, enhancing geographical feature learning."
  },
  {
    "prompt": "summary: We introduce SAOR, a novel approach for estimating the 3D shape, texture, and viewpoint of an articulated object from a single image captured in the wild. Unlike prior approaches that rely on pre-defined category-specific 3D templates or tailored 3D skeletons, SAOR learns to ar ticulate shapes from single-view image collections with a skeleton-free part-based model without requiring any 3D object shape priors. To prevent ill-posed solutions, we pro pose a cross-instance consistency loss that exploits disen tangled object shape deformation and articulation. This is helped by a new silhouette-based sampling mechanism to enhance viewpoint diversity during training. Our method only requires estimated object silhouettes and relative depth maps from off-the-shelf pre-trained networks during train ing. At inference time, given a single-view image, it effi ciently outputs an explicit mesh representation. We obtain improved qualitative and quantitative results on challeng ing quadruped animals compared to relevant existing work.",
    "chosen": "SAOR is a novel approach for estimating 3D shape, texture, and viewpoint of articulated objects from a single image. It uses a skeleton-free part-based model with a cross-instance consistency loss and silhouette-based sampling, requiring only object silhouettes and depth maps during training.",
    "rejected": "SAOR introduces a novel approach for estimating 3D shape, texture, and viewpoint of articulated objects from single images. Without relying on predefined 3D templates, SAOR uses a skeleton-free part-based model, achieving improved results compared to existing methods on challenging categories like quadruped animals."
  },
  {
    "prompt": "summary: In this work, we study a novel problem which focuses on person identification while performing daily activities. Learning biometric features from RGB videos is challeng ing due to spatio-temporal complexity and presence of ap pearance biases such as clothing color and background. We propose ABNet, a novel framework which leverages dis entanglement of biometric and non-biometric features to perform effective person identification from daily activi ties. ABNet relies on a bias-less teacher to learn biometric features from RGB videos and explicitly disentangle non biometric features with the help of biometric distortion. In addition, ABNet also exploits activity prior for biometrics which is enabled by joint biometric and activity learning. We perform comprehensive evaluation of the proposed ap proach across five different datasets which are derived from existing activity recognition benchmarks. Furthermore, we extensively compare ABNet with existing works in person identification and demonstrate its effectiveness for activity based biometrics across all five datasets.",
    "chosen": "ABNet is a framework for person identification from daily activities, disentangling biometric and non-biometric features using a bias-less teacher and biometric distortion. It exploits activity priors through joint biometric and activity learning, evaluated across five datasets.",
    "rejected": "ABNet leverages disentanglement of biometric and non-biometric features to perform person identification from daily activities. It utilizes a bias-less teacher model and exploits activity priors for effective biometric feature learning, demonstrating superior performance across five datasets."
  },
  {
    "prompt": "summary: Despite the growing demand for accurate surface nor mal estimation models, existing methods use general purpose dense prediction models, adopting the same induc tive biases as other tasks. In this paper, we discuss the inductive biases needed for surface normal estimation and propose to (1) utilize the per-pixel ray direction and (2) en code the relationship between neighboring surface normals by learning their relative rotation. The proposed method can generate crisp — yet, piecewise smooth — predictions for challenging in-the-wild images of arbitrary resolution and aspect ratio. Compared to a recent ViT-based state of-the-art model, our method shows a stronger generaliza tion ability, despite being trained on an orders of magni tude smaller dataset.",
    "chosen": "A method rethinking inductive biases for surface normal estimation is proposed, utilizing per-pixel ray direction and encoding relative rotation between neighboring surface normals. It shows stronger generalization ability despite being trained on a smaller dataset.",
    "rejected": "This paper discusses inductive biases necessary for surface normal estimation and proposes DSINE, a method utilizing per-pixel ray direction and relative rotation between neighboring normals. DSINE shows stronger generalization ability despite being trained on a smaller dataset compared to state-of-the-art models."
  },
  {
    "prompt": "summary: Recent breakthroughs in text-to-4D generation rely on pre-trained text-to-image and text-to-video models to gener ate dynamic 3D scenes. However, current text-to-4D meth ods face a three-way tradeoff between the quality of scene appearance, 3D structure, and motion. For example, text to-image models and their 3D-aware variants are trained on internet-scale image datasets and can be used to pro duce scenes with realistic appearance and 3D structure—but no motion. Text-to-video models are trained on relatively smaller video datasets and can produce scenes with motion, but poorer appearance and 3D structure. While these mod els have complementary strengths, they also have opposing weaknesses, making it difficult to combine them in a way that alleviates this three-way tradeoff. Here, we introduce hybrid score distillation sampling, an alternating optimiza tion procedure that blends supervision signals from multiple pre-trained diffusion models and incorporates benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we demonstrate synthesis of 4D scenes with compelling appearance, 3D structure, and motion.",
    "chosen": "4D-fy is a text-to-4D generation method using hybrid score distillation sampling, blending supervision signals from multiple pre-trained diffusion models. It synthesizes 4D scenes with compelling appearance, 3D structure, and motion.",
    "rejected": "Hybrid score distillation sampling combines supervision signals from multiple pre-trained diffusion models for high-fidelity text-to-4D generation. This method synthesizes dynamic scenes with compelling appearance, 3D structure, and motion, addressing tradeoffs faced by current text-to-4D methods."
  },
  {
    "prompt": "summary: Automating visual inspection in industrial production lines is essential for increasing product quality across var ious industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However, existing pub lic datasets primarily consist of images without anomalies, limiting the practical application of AD methods in pro duction settings. To address this challenge, we present (1) the Valeo Anomaly Dataset (VAD), a novel real-world in dustrial dataset comprising 5000 images, including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset, we introduce (2) Segmentation based Anomaly Detector (SegAD). First, SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next, SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier, yielding the final anomaly score. Our SegAD achieves state-of-the-art perfor mance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC). The code and the models are publicly available1.",
    "chosen": "SegAD is introduced for supervised anomaly detection in industrial images, using anomaly and segmentation maps with a Boosted Random Forest classifier. It achieves state-of-the-art performance on the Valeo Anomaly Dataset and VisA dataset.",
    "rejected": "The Valeo Anomaly Dataset (VAD) and Segmentation-based Anomaly Detector (SegAD) are introduced for industrial anomaly detection. SegAD computes local statistics from anomaly and segmentation maps, achieving state-of-the-art performance on VAD and VisA datasets."
  },
  {
    "prompt": "summary: Contrastive Vision-Language Pre-training, known as CLIP, has shown promising effectiveness in addressing downstream image recognition tasks. However, recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks, one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor, existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model, which makes them inapplicable to data-limited scenarios. In this work, motivated by the re cent success of learnable prompts, we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator, such that the trigger can change text features via trigger-aware prompts, result ing in a powerful and generalizable attack. Extensive ex periments conducted on 11 datasets verify that the clean ac curacy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to un seen classes, and shows a strong generalization capability under cross-dataset and cross-domain settings.",
    "chosen": "BadCLIP is a backdoor attack on CLIP models during prompt learning, influencing both image and text encoders with a trigger. It achieves high attack success rates while maintaining clean accuracy and generalizing to unseen classes and domains.",
    "rejected": "Our research focuses on person identification during routine activities, proposing ABNet to disentangle biometric and non-biometric features using RGB videos. This framework enhances identification accuracy beyond traditional gait or facial recognition methods, showing effectiveness across diverse datasets."
  },
  {
    "prompt": "summary: Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and fed erated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of nu merous edge devices and additional cross-client domain an notations in the real world, such restrictions may be im practical and involve potential privacy leaks. In this pa per, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specif ically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tun ing without labor-intensive annotation. Extensive experi ments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels.",
    "chosen": "DiPrompT is a novel federated learning approach for domain generalization that learns adaptive prompts in a distributed manner, eliminating the one-to-one mapping restriction between source domains and local clients. It introduces global and domain prompts to capture general and domain-specific knowledge, and a dynamic query metric to automatically search suitable domain labels. Extensive experiments show DiPrompT outperforms state-of-the-art FL methods without domain labels and even surpasses centralized methods using domain labels.",
    "rejected": "Federated learning (FL) has emerged as a method for decentralized data learning. DiPrompT proposes adaptive prompts for domain generalization in FL without explicit domain labels. It introduces global and domain-specific prompts, enhancing performance on unseen domains. The approach achieves superior results compared to centralized methods."
  },
  {
    "prompt": "summary: 3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of pho torealism. However, the computational cost of these meth ods remains a significant barrier to their widespread adop tion, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to sup port realistic facial expressions, such as in the case of a dy namic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining fine grained controllability and high rendering quality. Our key idea lies in the",
    "chosen": "A novel fast 3D neural implicit head avatar model is proposed, achieving real-time rendering while maintaining fine-grained controllability and high quality. It uses local hash table blendshapes attached to an underlying face parametric model, with per-vertex hash-tables merged via CNN-predicted weights. This enables efficient density and color predictions using a lightweight MLP accelerated by a hierarchical nearest neighbor search. Experiments show real-time performance with comparable rendering quality to state-of-the-arts.",
    "rejected": "A novel 3D neural implicit head avatar model is introduced, achieving real-time rendering while maintaining high quality and controllability. By using local hash table blendshapes and an efficient MLP, it ensures fine-grained expression control and fast performance suitable for dynamic facial performances in VR."
  },
  {
    "prompt": "summary: We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of ﬁxed point solving into the framework of diffusion based generative modeling. Our approach embeds an im plicit ﬁxed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related ﬁxed point problems. Combined with a new stochastic training method, this ap proach signiﬁcantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efﬁciency: reallocating computation across timesteps and reusing ﬁxed point solutions between timesteps. We con duct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demon strating substantial improvements in performance and efﬁ ciency. Compared to the state-of-the-art DiT model [38], FPDM contains 87% fewer parameters, consumes 60% less memory during training, and improves image generation *Equal Contribution. †Corresponding author. quality in situations where sampling computation or time is limited.",
    "chosen": "The Fixed Point Diffusion Model (FPDM) integrates fixed point solving into diffusion-based generative modeling, embedding an implicit fixed point layer in the denoising network. Combined with stochastic training, it reduces model size, memory usage, and accelerates training. It also improves sampling efficiency through computation reallocation and solution reuse. Experiments on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church show FPDM significantly improves performance and efficiency, with 87% fewer parameters and 60% less memory than DiT.",
    "rejected": "The Fixed Point Diffusion Model (FPDM) integrates fixed point solving into diffusion-based generative modeling, reducing model size and memory usage. This approach accelerates training and improves sampling efficiency, offering significant improvements over state-of-the-art models like DiT."
  },
  {
    "prompt": "summary: Out-of-distribution (OOD) detection methods often ex ploit auxiliary outliers to train model identifying OOD sam ples, especially discovering challenging outliers from aux iliary outliers dataset to improve OOD detection. However, they may still face limitations in effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data, i.e., ID-like samples. To this end, we propose a novel OOD detection framework that discovers ID-like outliers using CLIP [32] from the vicin ity space of the ID samples, thus helping to identify these most challenging OOD samples. Then a prompt learning framework is proposed that utilizes the identified ID-like outliers to further leverage the capabilities of CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a small number of ID samples to learn the prompts of the model without exposing other auxiliary outlier datasets. By focusing on the most challenging ID-like OOD samples and elegantly exploiting the capabilities of CLIP, our method achieves superior few-shot learning performance on various real-world image datasets (e.g., in 4-shot OOD detection on the ImageNet-1k dataset, our method reduces the average FPR95 by 12.16% and improves the average AUROC by 2.76%, compared to state-of-the-art methods).",
    "chosen": "A novel OOD detection framework discovers ID-like outliers using CLIP from the vicinity of ID samples, then uses prompt learning to leverage CLIP's capabilities for OOD detection. This method focuses on challenging ID-like OOD samples, requiring few ID samples to learn prompts without auxiliary outlier datasets. Extensive experiments demonstrate superior few-shot learning performance, reducing average FPR95 by 12.16% and improving AUROC by 2.76% on ImageNet-1k in 4-shot OOD detection.",
    "rejected": "A new OOD detection framework leveraging CLIP identifies ID-like outliers to improve detection accuracy. Prompt learning with these outliers enhances CLIP's capabilities, demonstrating superior few-shot learning performance on various datasets by significantly reducing FPR95 and improving AUROC."
  },
  {
    "prompt": "summary: We introduce a novel sequential modeling approach which enables learning a Large Vision Model (LVM) without mak ing use of any linguistic data. To do this, we deﬁne a common format, “visual sentences”, in which we can represent raw images and videos as well as annotated data sources such as semantic segmentations and depth reconstructions with out needing any meta-knowledge beyond the pixels. Once this wide variety of visual data (comprising 420 billion to kens) is represented as sequences, the model can be trained to minimize a cross-entropy loss for next token prediction. By training across various scales of model architecture and data diversity, we provide empirical evidence that our models scale effectively. Many different vision tasks can be solved by designing suitable visual prompts at test time.",
    "chosen": "A sequential modeling approach enables learning a Large Vision Model (LVM) using only visual data, defining \"visual sentences\" to represent diverse visual data as sequences. A 3 billion parameter transformer trained on 1.64 billion images minimizes cross-entropy loss for next token prediction. This model demonstrates effective scaling, solves various vision tasks via test-time prompts, benefits from unsupervised data, and shows potential for general visual reasoning.",
    "rejected": "Introducing a Large Vision Model (LVM) trained without linguistic data, this work defines \"visual sentences\" to represent diverse visual data formats. LVM scales effectively across different architectures and data diversities, solving many vision tasks through prompt design at test time."
  },
  {
    "prompt": "summary: This paper presents DriveTrack, a new benchmark and data generation framework for long-range keypoint tracking in real-world videos. DriveTrack is motivated by the observation that the accuracy of state-of-the-art trackers depends strongly on visual attributes around the selected keypoints, such as texture and lighting. The problem is that these artifacts are especially pronounced in real-world videos, but these trackers are unable to train on such scenes due to a dearth of annotations. DriveTrack bridges this gap by building a framework to auto matically annotate point tracks on autonomous driving datasets. We release a dataset consisting of 1 billion point tracks across 24 hours of video, which is seven orders of magnitude greater than prior real-world benchmarks and on par with the scale of synthetic benchmarks. DriveTrack unlocks new use cases for point tracking in real-world videos. First, we show that fine tuning keypoint trackers on DriveTrack improves accuracy on real-world scenes by up to 7%. Second, we analyze the sensitiv ity of trackers to visual artifacts in real scenes and motivate the idea of running assistive keypoint selectors alongside trackers.",
    "chosen": "DriveTrack is a benchmark and data generation framework for long-range keypoint tracking in real-world videos. It automatically annotates point tracks on autonomous driving datasets, releasing 1 billion point tracks across 24 hours of video. This improves tracker accuracy on real-world scenes by up to 7% and analyzes tracker sensitivity to visual artifacts, motivating assistive keypoint selectors.",
    "rejected": "DriveTrack provides a benchmark for long-range keypoint tracking in real-world videos, addressing limitations of synthetic datasets. Fine-tuning trackers on DriveTrack improves real-world scene accuracy and analyzes tracker sensitivity to visual artifacts, proposing assistive keypoint selectors."
  },
  {
    "prompt": "summary: Sign Languages (SL) serve as the primary mode of com munication for the Deaf and Hard of Hearing communities. Deep learning methods for SL recognition and translation have achieved promising results. However, Sign Language Production (SLP) poses a challenge as the generated mo tions must be realistic and have precise semantic meaning. Most SLP methods rely on 2D data, which hinders their re alism. In this work, a diffusion-based SLP model is trained on a curated large-scale dataset of 4D signing avatars and their corresponding text transcripts. The proposed method can generate dynamic sequences of 3D avatars from an un constrained domain of discourse using a diffusion process formed on a novel and anatomically informed graph neu ral network defined on the SMPL-X body skeleton. Through quantitative and qualitative experiments, we show that the proposed method considerably outperforms previous meth ods of SLP. This work makes an important step towards re alistic neural sign avatars, bridging the communication gap between Deaf and hearing communities.1 1",
    "chosen": "A diffusion-based Sign Language Production (SLP) model is trained on a large-scale 4D signing avatar dataset. It generates dynamic 3D avatar sequences using a diffusion process on a novel graph neural network defined on the SMPL-X body skeleton. Experiments show significant improvement over previous methods, advancing realistic neural sign avatars and bridging the communication gap between Deaf and hearing communities.",
    "rejected": "A diffusion-based Sign Language Production (SLP) model generates realistic 3D avatar motions from text transcripts. Using an anatomically informed graph neural network, it outperforms previous SLP methods, marking a significant step towards bridging communication gaps between Deaf and hearing communities."
  },
  {
    "prompt": "summary: Recent advances in large-scale pretraining have yielded visual foundation models with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and local ize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual foundation models. We posit that 3D awareness implies that representations (1) encode the 3D structure of the scene and (2) consistently represent the surface across views. We conduct a series of experiments using task-speciﬁc probes and zero-shot inference procedures on frozen fea tures. Our experiments reveal several limitations of the current models.",
    "chosen": "This work analyzes the 3D awareness of visual foundation models through task-specific probes and zero-shot inference. It evaluates whether representations encode 3D structure and maintain multiview consistency. Experiments reveal several limitations in current models, indicating further research is needed to enhance their 3D understanding.",
    "rejected": "This paper explores the 3D awareness of visual foundation models. Through experiments on task-specific probes, it reveals that current models lack consistent 3D representation across views, despite their strong 2D capabilities. The study highlights the need for improved 3D understanding in these models."
  },
  {
    "prompt": "summary: We propose SketchINR, to advance the representation of vector sketches with implicit neural models. A vari able length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes. The learned func tion predicts the xy point coordinates in a sketch at each time and stroke. Despite its simplicity, SketchINR outper forms existing representations at multiple tasks: (i) Encod ing an entire sketch dataset into a fixed size latent vec tor, SketchINR gives 60× and 10× data compression over raster and vector sketches, respectively. (ii) SketchINR’s auto-decoder provides a much higher-fidelity representa tion than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO. (iii) SketchINR supports parallelisation that can decode/render ∼100× faster than other learned vector representations such as SketchRNN. (iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complex ity of strokes. As a first look at implicit sketches, Sketch INR’s compact high-fidelity representation will support fu ture work in modelling long and complex sketches.",
    "chosen": "SketchINR advances vector sketch representation with implicit neural models, compressing sketches into a latent space encoding shape as a function of time and strokes. It outperforms existing representations in data compression, fidelity, scalability, and rendering speed, supporting future work in modeling complex sketches.",
    "rejected": "SketchINR advances vector sketch representations using implicit neural models. It compresses variable-length sketches into a fixed latent space, enabling the encoding of shape details over time. This approach facilitates more effective processing and manipulation of vector sketches for various applications."
  },
  {
    "prompt": "summary: Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks, such as classiﬁcation and retrieval. Despite their perfor mance, because improving performance on new tasks re quires task-speciﬁc knowledge, their adaptation is essen tial. While labels are needed for the adaptation, acquiring them is typically expensive. To overcome this challenge, ac tive learning, a method of achieving a high performance by obtaining labels for a small number of samples from ex perts, has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leverag ing them to train models. In this study, we pose the ques tion, “how can the pre-trained VLMs be adapted under the active learning framework?” In response to this inquiry, we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates, and (2) the knowl edge of VLMs can provide hints for achieving the balance before labeling. Based on these observations, we devise a novel active learning framework for VLMs, denoted as PCB. To assess the effectiveness of our approach, we conduct ex periments on seven different real-world datasets, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods.",
    "chosen": "A novel active learning framework for pre-trained Vision Language Models (VLMs) is proposed, addressing class imbalance issues in labeling candidates. This framework, called PCB, leverages VLM knowledge before labeling to achieve better balance. Experiments on seven real-world datasets show PCB surpasses conventional active learning and random sampling methods.",
    "rejected": "SketchINR advances the representation of vector sketches through implicit neural models. It compresses variable-length sketches into a fixed-dimension latent space, encoding the underlying shape as a function of time and structure. This method facilitates efficient processing and manipulation of vector sketches, enhancing their application in various domains."
  },
  {
    "prompt": "summary: Despite being (pre)trained on a massive amount of data, state-of-the-art video-language alignment models are not robust to semantically-plausible contrastive changes in the video captions. Our work addresses this by identifying a broad spectrum of contrast misalignments, such as re placing entities, actions, and flipping event order, which alignment models should be robust against. To this end, we introduce the VideoCon, a video-language alignment dataset constructed by a large language model that gen erates plausible contrast video captions and explanations for differences between original and contrast video cap tions. Then, a generative video-language model is fine tuned with VideoCon to assess video-language entailment and generate explanations. Our VideoCon-based alignment model significantly outperforms current models. It exhibits a 12-point increase in AUC for the video-language align ment task on human-generated contrast captions. Finally, our model sets new state of the art zero-shot performance in temporally-extensive video-language tasks such as text to-video retrieval (SSv2-Temporal) and video question an swering (ATP-Hard). Moreover, our model shows superior performance on novel videos and human-crafted captions and explanations.",
    "chosen": "VideoCon is a video-language alignment dataset constructed by a large language model, generating plausible contrast video captions and explanations for differences. A generative video-language model fine-tuned with VideoCon significantly outperforms current models, achieving a 12-point increase in AUC for the video-language alignment task and setting new state-of-the-art zero-shot performance in temporally-extensive video-language tasks.",
    "rejected": "In addressing video-language alignment robustness, VideoCon dataset is introduced, containing semantically plausible contrastive captions. Fine-tuned models on VideoCon show significant improvement in AUC and zero-shot performance on tasks like text-to-video retrieval and video question answering."
  },
  {
    "prompt": "summary: Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applica tions like 3D head avatar editing across different represen tations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMM driving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effective ness of the generative modification process, we develop sev eral techniques, including an expression-dependent mod ification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model con vergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and con sistent results across multiple expression and viewpoints.",
    "chosen": "GeneAvatar is a generic avatar editing approach for various 3DMM-driven volumetric head avatars. It uses an expression-aware modification generative model to lift 2D editing from a single image to a consistent 3D modification field. The method includes techniques like expression-dependent modification distillation, implicit latent space guidance, and segmentation-based loss reweighting, delivering high-quality and consistent results across multiple expressions and viewpoints.",
    "rejected": "GeneAvatar proposes a novel approach to edit 3D head avatars across various volumetric representations. It employs an expression-aware modification generative model that ensures consistency across expressions and viewpoints, supporting both intensive and delicate editing from single-perspective images."
  },
  {
    "prompt": "summary: Adversarial attacks aim to perturb images such that a pre dictor outputs incorrect results. Due to the limited research in structured attacks, imposing consistency checks on natural multi-object scenes is a practical defense against conven tional adversarial attacks. More desired attacks should be able to fool defenses with such consistency checks. Therefore, we present the first approach GLOW that copes with various attack requests by generating global layout-aware adversar ial attacks, in which both categorical and geometric layout constraints are explicitly established. Specifically, we focus on object detection tasks and given a victim image, GLOW first localizes victim objects according to target labels. And then it generates multiple attack plans, together with their context-consistency scores. GLOW, on the one hand, is ca pable of handling various types of requests, including single or multiple victim objects, with or without specified victim objects. On the other hand, it produces a consistency score for each attack plan, reflecting the overall contextual consis tency that both semantic category and global scene layout are considered. We conduct our experiments on MS COCO and Pascal. Extensive experimental results demonstrate that we can achieve about 30% average relative improvement compared to state-of-the-art methods in conventional single object attack request; Moreover, such superiority is also valid across more generic attack requests, under both white box and zero-query black-box settings. Finally, we conduct comprehensive human analysis, which not only validates our claim further but also provides strong evidence that our evaluation metrics reflect human reviews well.",
    "chosen": "GLOW is the first approach to generate global layout-aware adversarial attacks on object detection, establishing categorical and geometric layout constraints. It localizes victim objects and generates multiple attack plans with context-consistency scores. Experiments on MS COCO and Pascal show GLOW achieves about 30% average relative improvement compared to state-of-the-art methods in conventional single object attack requests and is valid across more generic attack requests under both white-box and zero-query black-box settings.",
    "rejected": "GLOW introduces a global layout-aware adversarial attack method targeting object detection tasks. By generating attacks with categorical and geometric constraints, GLOW achieves about 30% average relative improvement over state-of-the-art methods, proving effective under white-box and black-box settings."
  },
  {
    "prompt": "summary: Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating mo tion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Composi tions (HMC) without any postprocessing or redundant de noising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the ab solute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of the-art results in terms of accuracy, realism, and smooth ness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per mo tion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.",
    "chosen": "FlowMDM is the first diffusion-based model generating seamless Human Motion Compositions without postprocessing. It uses Blended Positional Encodings for global motion coherence and smooth transitions, achieving state-of-the-art results on Babel and HumanML3D datasets. FlowMDM is robust against varying text descriptions at inference time due to its Pose-Centric Cross-ATtention.",
    "rejected": "FlowMDM introduces diffusion-based human motion generation driven by textual descriptions. Utilizing blended positional encodings, it achieves seamless transitions between actions, demonstrating state-of-the-art results in accuracy, realism, and smoothness on benchmarks like Babel and HumanML3D."
  },
  {
    "prompt": "summary: Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called ‘frozen fea tures’, leads to impressive performance on a number of downstream few-shot tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, data augmenta tion is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an ex tensive pilot study on few-shot image classification that ex plores applying data augmentations in the frozen feature space, dubbed ‘frozen feature augmentation (FroFA)’, cov ering twenty augmentations in total. Our study demon strates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve few-shot performance con sistently across three network architectures, three large pre training datasets, and eight transfer datasets.",
    "chosen": "FroFA explores applying data augmentations in the frozen feature space for few-shot image classification. The study shows that simple pointwise augmentations like brightness can consistently improve few-shot performance across different network architectures, pre-training datasets, and transfer datasets.",
    "rejected": "Frozen feature augmentation (FroFA) applies data augmentations to frozen features from pretrained vision models, improving few-shot learning performance. Simple point-wise augmentations such as brightness consistently enhance performance across multiple architectures and datasets."
  },
  {
    "prompt": "summary: Non-isometric shape correspondence remains a funda mental challenge in computer vision. Traditional methods using Laplace-Beltrami operator (LBO) eigenmodes face limitations in characterizing high-frequency extrinsic shape changes like bending and creases. We propose a novel ap proach of combining the non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with the in trinsic ones of the LBO, creating a hybrid spectral space in which we construct functional maps. To this end, we present a theoretical framework to effectively integrate non orthogonal basis functions into descriptor and learning based functional map methods. Our approach can be in corporated easily into existing functional map pipelines across varying applications and can handle complex de formations beyond isometries. We show extensive evalua tions across various supervised and unsupervised settings and demonstrate significant improvements. Notably, our ap proach achieves up to 15% better mean geodesic error for non-isometric correspondence settings and up to 45% im provement in scenarios with topological noise.",
    "chosen": "This novel approach combines non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with intrinsic Laplace-Beltrami operator eigenmodes to create a hybrid spectral space for functional maps. It effectively integrates non-orthogonal basis functions into descriptor and learning-based methods, handling complex deformations beyond isometries and showing significant improvements in non-isometric correspondence settings and scenarios with topological noise.",
    "rejected": "A novel shape correspondence approach integrates non-orthogonal extrinsic basis functions with intrinsic Laplace-Beltrami operator eigenmodes. This hybrid spectral space method shows up to 15% better mean geodesic error for non-isometric shapes and handles topological noise effectively."
  },
  {
    "prompt": "summary: Point cloud matching, a crucial technique in computer vision, medical and robotics ﬁelds, is primarily concerned with ﬁnding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing lo cal differences is crucial for accurately identifying a cor rect match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to pro vide meaningful local insights about the paired geome tries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account ﬁne local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we in troduce a new operator, called Coupled Laplacian1, that allows to easily generate aligned eigenspaces for multiple registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We ﬁrstly evaluate the performance of the proposed technique in a point-wise man ner, focusing on the task of object anomaly localization on the MVTec 3D-AD dataset. Additionally, we deﬁne a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Cou pled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks.",
    "chosen": "A new technique based on graph Laplacian eigenmaps is proposed for point cloud matching, introducing a Coupled Laplacian operator to generate aligned eigenspaces for multiple registered geometries. The similarity between aligned high-dimensional spaces provides a locally meaningful score for shape matching, outperforming other methods in object anomaly localization and automatic Bone Side Estimation tasks.",
    "rejected": "Using graph Laplacian eigenmaps, a new point cloud matching technique addresses local structure differences. The Coupled Laplacian operator aligns eigenspaces, providing meaningful similarity scores. It outperforms existing methods in object anomaly localization and introduces Bone Side Estimation for medical applications."
  },
  {
    "prompt": "summary: In recent years, automated Gallbladder Cancer (GBC) detection has gained the attention of researchers. Cur rent state-of-the-art (SOTA) methodologies relying on ultra sound sonography (US) images exhibit limited generaliza tion, emphasizing the need for transformative approaches. We observe that individual US frames may lack sufficient information to capture disease manifestation. This study advocates for a paradigm shift towards video-based GBC detection, leveraging the inherent advantages of spatiotem poral representations. Employing the Masked Autoencoder (MAE) for representation learning, we address shortcom ings in conventional image-based methods. We propose a novel design called FocusMAE to systematically bias the selection of masking tokens from high-information re gions, fostering a more refined representation of malig nancy. Additionally, we contribute the most extensive US video dataset for GBC detection. We also note that, this is the first study on US video-based GBC detection. We validate the proposed methods on the curated dataset, and report a new SOTA accuracy of 96.4% for the GBC de tection problem, against an accuracy of 84% by current Image-based SOTA – GBCNet and RadFormer, and 94.7% by Video-based SOTA – AdaMAE. We further demonstrate the generality of the proposed FocusMAE on a public CT based Covid detection dataset, reporting an improvement in accuracy by 3.3% over current baselines.",
    "chosen": "FocusMAE is a novel design for video-based Gallbladder Cancer detection, systematically biasing masking token selection from high-information regions to refine malignancy representation. The method achieves a new state-of-the-art accuracy of 96.4% on the curated dataset and demonstrates generality on a public CT-based Covid detection dataset.",
    "rejected": "FocusMAE enhances masked autoencoders for ultrasound-based Gallbladder Cancer detection using a spatiotemporal approach. By biasing masking towards high-information regions, it achieves 96.4% accuracy, surpassing current image-based and video-based SOTA methods. Code and dataset are available."
  },
  {
    "prompt": "summary: We propose a novel method for 3D point cloud ac tion recognition. Understanding human actions in RGB videos has been widely studied in recent years, however, its 3D point cloud counterpart remains under-explored de spite the clear value that 3D information may bring. This is mostly due to the inherent limitation of the point cloud data modality—lack of structure, permutation invariance, and varying number of points—which makes it difﬁcult to learn a spatio-temporal representation. To address this lim itation, we propose the 3DinAction pipeline that ﬁrst esti mates patches moving in time (t-patches) as a key build ing block, alongside a hierarchical architecture that learns an informative spatio-temporal representation. We show that our method achieves improved performance on existing datasets, including DFAUST and IKEA ASM.",
    "chosen": "3DinAction is a novel method for 3D point cloud action recognition, proposing a pipeline that estimates patches moving in time (t-patches) and uses a hierarchical architecture to learn spatio-temporal representations. The method improves performance on existing datasets like DFAUST and IKEA ASM.",
    "rejected": "This study investigates non-isometric shape correspondence using a hybrid spectral space combining intrinsic and extrinsic basis functions. The proposed method significantly improves upon traditional approaches, particularly in handling complex deformations and topological noise."
  },
  {
    "prompt": "summary: Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during in ference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an as sembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates Foundation Mod els of various kinds: (i) CLIP backbone for its robust feature representation, (ii) Diffusion Model to diversify the con tent, thereby covering various modes of the possible tar get distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS ex cels in adapting from synthetic to real DGSS benchmarks and under varying weather conditions, notably outperform ing prior methods by 5.6% and 6.7% on averaged mIoU, re spectively. Our",
    "chosen": "CLOUDS is a framework integrating various foundation models for Domain Generalized Semantic Segmentation, including CLIP for feature representation, Diffusion Model for content diversification, and Segment Anything Model for prediction refinement. It outperforms prior methods by 5.6% and 6.7% on averaged mIoU in adapting from synthetic to real DGSS benchmarks and under varying weather conditions.",
    "rejected": "To improve point cloud matching, a technique based on graph Laplacian eigenmaps is introduced, focusing on fine local structures. The Coupled Laplacian operator aligns eigenspaces, providing a locally meaningful score for shape matching. The method excels in tasks such as object anomaly localization and Bone Side Estimation."
  },
  {
    "prompt": "summary: Precise geospatial vegetation forecasting holds potential across diverse sectors, including agriculture, forestry, hu manitarian aid, and carbon accounting. To leverage the vast availability of satellite imagery for this task, various works have applied deep neural networks for predicting multispectral images in photorealistic quality. However, the important area of vegetation dynamics has not been thoroughly explored. Our study introduces GreenEarthNet, the first dataset specifically designed for high-resolution vegetation forecasting, and Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images with fine resolution across Eu rope. Our multi-modal transformer model Contextformer leverages spatial context through a vision backbone and predicts the temporal dynamics on local context patches incorporating meteorological time series in a parameter efficient manner. The GreenEarthNet dataset features a learned cloud mask and an appropriate evaluation scheme for vegetation modeling. It also maintains compatibil ity with the existing satellite imagery forecasting dataset EarthNet2021, enabling cross-dataset model comparisons. Our extensive qualitative and quantitative analyses reveal that our methods outperform a broad range of baseline techniques. This includes surpassing previous state-of-the art models on EarthNet2021, as well as adapted models from time series forecasting and video prediction. To the best of our knowledge, this work presents the first mod els for continental-scale vegetation modeling at fine reso lution able to capture anomalies beyond the seasonal cycle, thereby paving the way for predicting vegetation health and behaviour in response to climate variability and extremes. ",
    "chosen": "GreenEarthNet is the first dataset specifically designed for high-resolution vegetation forecasting, paired with Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images. This multi-modal transformer model leverages spatial context through a vision backbone and predicts temporal dynamics on local context patches incorporating meteorological time series efficiently. GreenEarthNet includes a learned cloud mask and an appropriate evaluation scheme, maintaining compatibility with EarthNet2021. Extensive analyses show Contextformer outperforms various baselines, including previous SOTA models on EarthNet2021 and adapted models from time series forecasting and video prediction.",
    "rejected": "GreenEarthNet introduces Contextformer, a novel deep learning model for predicting vegetation greenness from Sentinel 2 satellite images across Europe. This study presents the first continental-scale vegetation modeling at fine resolution, capable of capturing anomalies beyond seasonal cycles."
  },
  {
    "prompt": "summary: Astronaut photography, spanning six decades of human spaceflight, presents a unique Earth observations dataset with immense value for both scientific research and disaster response. Despite their significance, accurately localizing the geographical extent of these images, which is crucial for effective utilization, poses substantial challenges. Cur rent, manual localization efforts are time-consuming, mo tivating the need for automated solutions. We propose a novel approach – leveraging image retrieval – to address this challenge efficiently. We introduce innovative training techniques which contribute to the development of a high performance model, EarthLoc. We develop six evaluation datasets and perform a comprehensive benchmark compar ing EarthLoc to existing methods, showcasing its superior efficiency and accuracy. Our approach marks a signifi cant advancement in automating the localization of astro naut photography, which will help bridge a critical gap in Earth observations data.",
    "chosen": "EarthLoc is a novel approach leveraging image retrieval to automate the localization of astronaut photography. This method efficiently addresses the challenge of identifying geographical areas in astronaut photographs, which is crucial for effective utilization. EarthLoc employs innovative training techniques to develop a high-performance model, supported by six evaluation datasets and comprehensive benchmark comparisons. The approach significantly advances automated localization of astronaut photography, bridging a critical gap in Earth observations data.",
    "rejected": "EarthLoc proposes an innovative approach for automating the localization of astronaut photography using image retrieval techniques. The method significantly advances the efficiency and accuracy in localizing these unique Earth observation datasets."
  },
  {
    "prompt": "summary: We describe a novel method, StyLitGAN, for relighting and resurfacing images in the absence of labeled data. StyL itGAN generates images with realistic lighting effects, includ ing cast shadows, soft shadows, inter-reflections, and glossy effects, without the need for paired or CGI data. StyLit GAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained Style GAN to identify a set of directions. By prompting the model to fix one component (e.g., albedo) and vary another (e.g., shading), we generate relighted images by adding the identi fied directions to the latent style codes. Quantitative metrics of change in albedo and lighting diversity allow us to choose effective directions using a forward selection process. Qual itative evaluation confirms the effectiveness of our method.",
    "chosen": "StyLitGAN is a novel method for relighting and resurfacing images without labeled data. It generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify effective directions for relighting. By fixing one component (e.g., albedo) and varying another (e.g., shading), it generates relighted images. Quantitative metrics and qualitative evaluation confirm the method's effectiveness.",
    "rejected": "StyLitGAN generates relighted images without paired or CGI data, utilizing intrinsic image methods and StyleGAN's latent space to manipulate lighting while preserving albedo and geometry, achieving realistic lighting effects."
  },
  {
    "prompt": "summary: Object detection in radar imagery with neural networks shows great potential for improving autonomous driving. However, obtaining annotated datasets from real radar im ages, crucial for training these networks, is challenging, especially in scenarios with long-range detection and ad verse weather and lighting conditions where radar perfor mance excels. To address this challenge, we present Rad SimReal, an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental con ditions, all without the need for real data collection. Re markably, our findings demonstrate that training object de tection models on RadSimReal data and subsequently eval uating them on real-world data produce performance lev els comparable to models trained and tested on real data from the same dataset, and even achieves better perfor mance when testing across different real datasets. Rad SimReal offers advantages over other physical radar sim ulations that it does not necessitate knowledge of the radar design details, which are often not disclosed by radar sup pliers, and has faster run-time. This innovative tool has the potential to advance the development of computer vision al gorithms for radar-based autonomous driving applications. ",
    "chosen": "RadSimReal is an innovative physical radar simulation capable of generating synthetic radar images with annotations for various radar types and environmental conditions without real data collection. This simulation addresses the challenge of obtaining annotated radar datasets, crucial for training neural networks in object detection. RadSimReal demonstrates comparable performance to real-data-trained models when evaluated on real-world data and even outperforms them across different real datasets. The simulation offers advantages over other physical radar simulations, including faster run-time and no need for radar design details.",
    "rejected": "RadSimReal simulates synthetic radar images with annotations, offering an effective solution for training object detection models in radar imagery. It demonstrates comparable performance to real-world data-trained models, even surpassing them in cross-dataset evaluations."
  },
  {
    "prompt": "summary: Recent advancements in large vision-language models enabled visual object detection in open-vocabulary scenar ios, where object classes are defined in free-text formats during inference. In this paper, we aim to probe the state of-the-art methods for open-vocabulary object detection to determine to what extent they understand fine-grained prop erties of objects and their parts. To this end, we intro duce an evaluation protocol based on dynamic vocabulary generation to test whether models detect, discern, and as sign the correct fine-grained description to objects in the presence of hard-negative classes. We contribute with a benchmark suite of increasing difficulty and probing dif ferent properties like color, pattern, and material. We fur ther enhance our investigation by evaluating several state of-the-art open-vocabulary object detectors using the pro posed protocol and find that most existing solutions, which shine in standard open-vocabulary benchmarks, struggle to accurately capture and distinguish finer object details. We conclude the paper by highlighting the limitations of current methodologies and exploring promising research directions to overcome the discovered drawbacks. ",
    "chosen": "This study evaluates state-of-the-art open-vocabulary object detection methods using a dynamic vocabulary generation protocol to test their ability to detect, discern, and assign fine-grained descriptions to objects. The benchmark suite increases in difficulty and probes properties like color, pattern, and material. Results show that most existing solutions struggle to accurately capture and distinguish finer object details, highlighting limitations and suggesting research directions for improvement.",
    "rejected": "This paper evaluates open-vocabulary object detection methods' ability to understand fine-grained properties of objects. Results show limitations in accurately distinguishing finer details, highlighting challenges and suggesting future research directions."
  },
  {
    "prompt": "summary: Learning compatible representations enables the inter changeable use of semantic features as models are updated over time. This is particularly relevant in search and re trieval systems where it is crucial to avoid reprocessing of the gallery images with the updated model. While recent research has shown promising empirical evidence, there is still a lack of comprehensive theoretical understanding about learning compatible representations. In this paper, we demonstrate that the stationary representations learned by the d-Simplex fixed classifier optimally approximate compat ibility representation according to the two inequality con straints of its formal definition. This not only establishes a solid foundation for future works in this line of research but also presents implications that can be exploited in practical learning scenarios. An exemplary application is the now standard practice of downloading and fine-tuning new pre trained models. Specifically, we show the strengths and criti cal issues of stationary representations in the case in which a model undergoing sequential fine-tuning is asynchronously replaced by downloading a better-performing model pre trained elsewhere. Such a representation enables seamless delivery of retrieval service (i.e., no reprocessing of gallery images) and offers improved performance without opera tional disruptions during model replacement.",
    "chosen": "IAM-CL2R demonstrates that stationary representations learned by the d-Simplex fixed classifier optimally approximate compatible representations according to theoretical constraints. This establishes a foundation for learning compatible representations and has practical implications for retrieval systems. Stationary representations enable seamless retrieval services during model replacement without reprocessing gallery images, crucial for systems requiring minimal operational disruptions.",
    "rejected": "The paper explores learning compatible representations for search and retrieval systems, demonstrating that stationary representations optimally approximate compatibility, enabling seamless service delivery during model updates."
  },
  {
    "prompt": "summary: Significant progress has been made in scene text detec tion models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace. Previous works either treated text detection and grouping using separate mod els, or train a model from scratch while using a unified one. All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets. In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to as semble text instance features. To capture broader contex tual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment. Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simul taneously inheriting generalized text detection ability from pre-training. In the case of full parameter fine-tuning, we can further improve layout analysis performance.",
    "chosen": "Text Grouping Adapter (TGA) is a module enabling the use of pre-trained text detectors for layout analysis. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as inputs to assemble text instance features. It predicts text group masks by one-to-many assignment to capture broader contextual information. Experiments show TGA enhances layout analysis performance even with frozen pre-trained models, inheriting generalized text detection ability and improving performance further with full parameter fine-tuning.",
    "rejected": "TGA enables pre-trained text detectors to learn layout analysis, improving scene text grouping without retraining. It uses detected text regions and image features to predict text group masks, enhancing layout analysis performance."
  },
  {
    "prompt": "summary: The increasing use of transformer-based large lan guage models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page set ting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pre trained single-page models to the multi-page setting, with out requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level desig nated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method. For ad ditional computational savings during decoding, we intro duce an optional compression stage using our compression transformer(C-Former ),reducing the encoded sequence length, thereby allowing a tradeoff between quality and latency. Extensive experiments showcase GRAM’s state of-the-art performance on the benchmarks for multi-page DocVQA, demonstrating the effectiveness of our approach.",
    "chosen": "GRAM extends pretrained single-page models to multi-page DocVQA without heavy pretraining. It uses a single-page encoder for local understanding and enhances it with document-level layers and learnable tokens for global reasoning. A compression stage using C-Former reduces sequence length, allowing a quality-latency tradeoff. GRAM achieves state-of-the-art performance on multi-page DocVQA benchmarks.",
    "rejected": "GRAM extends single-page document visual question answering (DocVQA) models to multi-page settings, facilitating global reasoning through document-level designated layers and learnable tokens. An optional compression stage allows quality-latency trade-offs."
  },
  {
    "prompt": "summary: Knowledge of lane topology is a core problem in au tonomous driving. Aerial imagery can provide high res olution, quickly updatable lane source data but detecting lanes from such data has so far been an expensive man ual process or, where automated solutions exist, undriv able and requiring of downstream processing. We pro pose a method for large-scale lane topology extraction from aerial imagery while ensuring that the resulting lanes are realistic and drivable by introducing a novel B´ezier Graph shared parameterisation of B´ezier curves. We develop a transformer-based model to predict these B´ezier Graphs from input aerial images, demonstrating competitive results on the UrbanLaneGraph dataset. We demonstrate that our method generates realistic lane graphs which require both minimal input, and minimal downstream processing. ",
    "chosen": "This method extracts large-scale lane topology from aerial imagery using a novel Bézier Graph parameterization of cubic Bézier curves. A transformer-based model predicts these graphs, generating realistic, drivable lane graphs with minimal input and downstream processing. The approach improves on existing methods by ensuring lane direction continuity at boundaries, demonstrated on the UrbanLaneGraph dataset.",
    "rejected": "This paper presents BGFormer, a transformer-based model for predicting Bezier Graphs from aerial images, generating drivable lane graphs with minimal downstream processing, ideal for HD map creation in autonomous driving."
  },
  {
    "prompt": "summary: Image keypoint descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction. However, descriptions output by learned descriptors are typically not robust to camera rotation. While they can be made more robust by, e.g., data aug mentation, this degrades performance on upright images. Another approach is test-time augmentation, which incurs a significant increase in runtime. Instead, we learn a lin ear transform in description space that encodes rotations of the input image. We call this linear transform a steerer since it allows us to transform the descriptions as if the im age was rotated. From representation theory, we know all possible steerers for the rotation group. Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a de scriptor or (C) we can optimize a descriptor given a fixed steerer. We perform experiments in these three settings and obtain state-of-the-art results on the rotation invariant im age matching benchmarks AIMS and Roto-360. ",
    "chosen": "Steerers are learned linear transforms in description space that encode image rotations for keypoint descriptors. This framework allows rotation-equivariant matching without sacrificing distinctiveness for small rotations or incurring significant runtime costs. Experiments show state-of-the-art results on rotation-invariant benchmarks, with code and model weights published.",
    "rejected": "The study focuses on creating discriminative image keypoint descriptors robust over large viewpoint changes for 3D reconstruction, addressing the lack of robustness in current learned descriptors."
  },
  {
    "prompt": "summary: Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shad ows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly dif ficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It sup ports dynamic motion and is tailored for shadow computa tion, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splat ting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts, and re lighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neu ral characters in novel environments, increasing their ap plicability.",
    "chosen": "Gaussian Shadow Casting for Neural Characters presents a new shadow model using a Gaussian density proxy for efficient shadow computation in neural character models. This method improves reconstructions by better separating albedo, shading, and shadows, enabling realistic relighting and novel pose generation without user input.",
    "rejected": "The paper introduces Gaussian shadows, a new model for accurate lighting and shadow computation in neural avatars reconstructed from video. This method supports dynamic motion and integrates with deferred neural rendering to improve shading and shadow casting. It enables realistic relighting in novel scenes without user input on light direction."
  },
  {
    "prompt": "summary: In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sens ing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, fo cuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the under lying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.",
    "chosen": "This work explores end-to-end training of agents in simulation to minimize the sim2real gap for autonomous robot navigation. The agent predicts velocity commands and is trained with a modified Habitat simulator modeling real robot behavior and noise. Evaluations show significant performance and robustness gains in real navigation scenarios.",
    "rejected": "This work addresses minimizing the sim2real gap for terrestrial robots by end-to-end training in simulations that closely mimic real-world sensing and actuation dynamics. By using a modified Habitat simulator with noise models for odometry and localization, it achieves significant improvements in navigation performance and robustness compared to prior methods."
  },
  {
    "prompt": "summary: Local Interpretable Model-agnostic Explanations (LIME)  a widely used post-ad-hoc model agnostic ex plainable AI (XAI) technique. It works by training a simple transparent (surrogate) model using random samples drawn around the neighborhood of the instance (image) to be explained (IE). Explanations are then extracted for a black-box model and a given IE, using the surrogate model. However, the explanations of LIME suffer from inconsistency across different runs for the same model and the same IE. We identify two main types of inconsistencies: variance in the sign and importance ranks of the segments (superpixels). These factors hinder LIME from obtaining consistent explanations. We analyze these inconsistencies and propose a new method, Stabilized LIME for Consistent Explanations (SLICE). The proposed method handles the stabilization problem in two aspects: using a novel feature selection technique to eliminate spurious superpixels and an adaptive perturbation technique to generate perturbed images in the neighborhood of IE. Our results demonstrate that the explanations from SLICE exhibit signiﬁcantly better consistency and ﬁdelity than LIME (and its variant BayLime).",
    "chosen": "SLICE is a new method enhancing LIME's consistency for image classification explanations. It uses feature selection to eliminate spurious superpixels and adaptive perturbation to generate neighborhood images, yielding more reliable and consistent explanations than LIME and its variants.",
    "rejected": "SLICE is proposed to stabilize explanations generated by LIME, addressing inconsistencies due to variance in segment importance ranks. SLICE uses feature selection and adaptive perturbation techniques to produce more consistent and faithful explanations for black-box models than LIME, enhancing reliability in explainable AI."
  },
  {
    "prompt": "summary: We study the underexplored but fundamental problem of machine understanding of abstract freehand scene sketches. We introduce a sketch encoder that ensures a semantically aware feature space, which we evaluate by testing its per formance on a semantic sketch segmentation task. To train our model, we rely only on bitmap sketches accompanied by brief captions, avoiding the need for pixel-level anno tations. To generalize to a large set of sketches and cat egories, we build upon a vision transformer encoder pre trained with the CLIP model. We freeze the text encoder and perform visual-prompt tuning of the visual encoder branch while introducing a set of critical modifications. First, we augment the classical key-query (k-q) self-attention blocks with value-value (v-v) self-attention blocks. Central to our model is a two-level hierarchical training that enables ef ficient semantic disentanglement: The first level ensures holistic scene sketch encoding, and the second level focuses on individual categories. In the second level of the hierar chy, we introduce cross-attention between the text and vi sion branches. Our method outperforms zero-shot CLIP segmentation results by 37 points, reaching a pixel accu racy of 85.5% on the FS-COCO sketch dataset. Finally, we conduct a user study that allows us to identify further im provements needed over our method to reconcile machine and human understanding of freehand scene sketches.",
    "chosen": "This paper introduces a sketch encoder for understanding abstract freehand scene sketches, evaluated on semantic sketch segmentation. The model, built on a CLIP-pretrained ViT, uses two-level hierarchical training and cross-attention to achieve high pixel accuracy on the FS-COCO dataset.",
    "rejected": "A sketch encoder is introduced that leverages a vision transformer pre-trained with CLIP for semantic segmentation of freehand scene sketches. The hierarchical training approach allows efficient disentanglement at both holistic and category levels, achieving superior zero-shot segmentation accuracy compared to CLIP on the FS-COCO dataset."
  },
  {
    "prompt": "summary: Vision-language foundation models have shown remark able performance in various zero-shot settings such as im age retrieval, classification, or captioning. But so far, those models seem to fall behind when it comes to zero-shot lo calization of referential expressions and objects in images. As a result, they need to be fine-tuned for this task. In this paper, we show that pretrained vision-language (VL) models allow for zero-shot open-vocabulary object local ization without any fine-tuning. To leverage those capabil ities, we propose a Grounding Everything Module (GEM) that generalizes the idea of value-value attention introduced by CLIPSurgery [17] to a self-self attention path. We show that the concept of self-self attention corresponds to cluster ing, thus enforcing groups of tokens arising from the same object to be similar while preserving the alignment with the language space. To further guide the group formation, we propose a set of regularizations that allows the model to finally generalize across datasets and backbones. We evaluate the proposed GEM framework on various bench mark tasks and datasets for semantic segmentation. GEM not only outperforms other training-free open-vocabulary localization methods, but also achieves state-of-the-art re sults on the recently proposed OpenImagesV7 large-scale segmentation benchmark. 1",
    "chosen": "GEM enables zero-shot open-vocabulary object localization using pretrained VL models. The proposed Grounding Everything Module uses self-self attention and regularizations to improve token grouping and alignment, achieving state-of-the-art results on segmentation benchmarks without fine-tuning.",
    "rejected": "GEM enhances zero-shot object localization capabilities of pretrained vision-language models through self-self attention paths, promoting clustering of tokens representing the same object. GEM outperforms existing methods on various benchmarks and achieves state-of-the-art results on OpenImagesV7 large-scale segmentation."
  },
  {
    "prompt": "summary: Text-to-image diffusion models have recently received in creasing interest for their astonishing ability to produce high-ﬁdelity images from solely text inputs. Subsequent research efforts aim to exploit and apply their capabili ties to real image editing. However, existing image-to image methods are often inefﬁcient, imprecise, and of lim ited versatility. They either require time-consuming ﬁne tuning, deviate unnecessarily strongly from the input im age, and/or lack support for multiple, simultaneous edits. To address these issues, we introduce LEDITS++, an efﬁ cient yet versatile and precise textual image manipulation technique. LEDITS++’s novel inversion approach requires no tuning nor optimization and produces high-ﬁdelity re sults with a few diffusion steps. Second, our methodology supports multiple simultaneous edits and is architecture agnostic. Third, we use a novel implicit masking technique that limits changes to relevant image regions. We propose the novel TEdBench++ benchmark as part of our exhaus tive evaluation. Our results demonstrate the capabilities of LEDITS++ and its improvements over previous methods.",
    "chosen": "LEDITS++ is an efficient textual image manipulation technique using a novel inversion approach and implicit masking. It requires no tuning, supports multiple edits, and produces high-fidelity results with few diffusion steps, outperforming previous methods.",
    "rejected": "LEDITS++ presents an efficient textual image manipulation technique requiring no fine-tuning, supporting multiple simultaneous edits, and using implicit masking to limit changes to relevant regions. It provides high-fidelity results with fewer diffusion steps, making it suitable for versatile and precise image editing applications."
  },
  {
    "prompt": "summary: Neural approaches have shown a significant progress on camera-based reconstruction. But they require either a fairly dense sampling of the viewing sphere, or pre-training on an existing dataset, thereby limiting their generaliz ability. In contrast, photometric stereo (PS) approaches have shown great potential for achieving high-quality re construction under sparse viewpoints. Yet, they are imprac tical because they typically require tedious laboratory con ditions, are restricted to dark rooms, and often multi-staged, making them subject to accumulated errors. To address these shortcomings, we propose an end-to-end uncalibrated multi-view PS framework for reconstructing high-resolution shapes acquired from sparse viewpoints in a real-world en vironment. We relax the dark room assumption, and allow a combination of static ambient lighting and dynamic near LED lighting, thereby enabling easy data capture outside the lab. Experimental validation confirms that it outper forms existing baseline approaches in the regime of sparse viewpoints by a large margin. This allows to bring high accuracy 3D reconstruction from the dark room to the real world, while maintaining a reasonable data capture com plexity.",
    "chosen": "This paper proposes an end-to-end uncalibrated multi-view photometric stereo framework for high-resolution 3D reconstruction from sparse viewpoints in real-world settings. It combines static ambient and dynamic LED lighting, outperforming baselines and enabling lab-quality reconstruction in practical conditions.",
    "rejected": "An end-to-end uncalibrated multi-view photometric stereo framework is introduced for 3D reconstruction from sparse viewpoints under ambient and near LED lighting conditions. The approach overcomes limitations of laboratory setups, enabling high-accuracy reconstructions in real-world environments without controlled lighting."
  },
  {
    "prompt": "summary: This paper introduces a versatile paradigm for inte grating multi-view reﬂectance (optional) and normal maps acquired through photometric stereo. Our approach em ploys a pixel-wise joint re-parameterization of reﬂectance and normal, considering them as a vector of radiances rendered under simulated, varying illumination. This re parameterization enables the seamless integration of re ﬂectance and normal maps as input data in neural volume rendering-based 3D reconstruction while preserving a sin gle optimization objective. In contrast, recent multi-view photometric stereo (MVPS) methods depend on multiple, potentially conﬂicting objectives. Despite its apparent sim plicity, our proposed approach outperforms state-of-the-art approaches in MVPS benchmarks across F-score, Chamfer distance, and mean angular error metrics. Notably, it sig niﬁcantly improves the detailed 3D reconstruction of areas with high curvature or low visibility.",
    "chosen": "RNb-NeuS integrates multi-view reflectance and normal maps into neural volume rendering via a joint re-parameterization. This approach outperforms state-of-the-art methods in MVPS benchmarks, improving 3D reconstruction of detailed areas.",
    "rejected": "The paper proposes a joint re-parameterization of reflectance and normal maps as radiances under simulated illumination for integration into neural volume rendering-based 3D reconstruction. This method simplifies optimization and improves reconstruction quality, especially in areas with high curvature or low visibility, outperforming state-of-the-art approaches in MVPS benchmarks."
  },
  {
    "prompt": "summary: In this paper we tackle the problem of learning Structure from-Motion (SfM) through the use of graph attention net works. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initial ization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provide an initial solu tion that can then be refined using BA. In this work we re place these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D key point coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the recon struction for new and unseen sequences. The experimen tal results show that the proposed model outperforms com peting learning-based methods, and challenges COLMAP while having lower runtime. Our",
    "chosen": "This work presents GASFM, a graph attention network for learning Structure-from-Motion. The model takes 2D keypoints as input and outputs camera poses and 3D coordinates, offering faster reconstruction than COLMAP with competitive accuracy.",
    "rejected": "Using graph attention networks, this work replaces traditional sub-problems in Structure-from-Motion (SfM) with a learned model for fast inference of camera poses and 3D keypoints. Experimental results demonstrate superior performance compared to competing learning-based methods and challenge COLMAP while reducing runtime."
  },
  {
    "prompt": "summary: Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object con cept under these shifts. In the absence of training data from target geographies, we hypothesize that geographi cally diverse descriptive knowledge of categories can en hance robustness. For this purpose, we explore the feasibil ity of probing a large language model for geography-based object knowledge, and we examine the effects of integrat ing knowledge into zero-shot and learnable soft prompt ing with CLIP. Within this exploration, we propose geog raphy knowledge regularization to ensure that soft prompts trained on a source set of geographies generalize to an un seen target set. Accuracy gains over prompting baselines on DollarStreet while training only on Europe data are up to +2.8/1.2/1.6 on target data from Africa/Asia/Americas, and +4.6 overall on the hardest classes. Competitive perfor mance is shown vs. few-shot target training, and analysis is provided to direct future study of geographical robustness.",
    "chosen": "The paper explores incorporating geo-diverse knowledge into prompting for geographical robustness in object recognition. Using a large language model for geography-based knowledge and integrating it with CLIP via soft prompting and regularization improves accuracy on target data from diverse regions.",
    "rejected": "The research develops an advanced framework combining photometric stereo and neural rendering for reconstructing textureless objects from sparse viewpoints. By integrating ambient and point-light illumination, it achieves detailed 3D reconstructions outside controlled environments, overcoming limitations associated with conventional photometric stereo techniques."
  },
  {
    "prompt": "summary: Despite noise and caption quality having been acknowl edged as important factors impacting vision-language con trastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be realized. Specifically, we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs, and low caption quality and diversity. Then, we devise effective solutions for address ing both problems, which essentially require training with multiple true positive pairs. Finally, we propose training with sigmoid loss to address such a requirement. We show very large gains over the current state-of-the-art for both image recognition (∼+6% on average over 11 datasets) and image retrieval (∼+19% on Flickr30k and ∼+15% on MSCOCO).",
    "chosen": "FFF addresses noise and caption quality in vision-language contrastive pre-training by studying incorrect negative pair assignments and low caption quality/diversity. It proposes training with multiple true positive pairs using a sigmoid loss, achieving large gains over the state-of-the-art in image recognition and retrieval.",
    "rejected": "This study addresses two issues affecting vision-language pre-training: incorrect assignment of negative pairs and low caption quality. By introducing an algorithm that mines new positive pairs and proposes training with sigmoid loss, significant improvements in image recognition and retrieval are achieved. The approach enhances model accuracy by addressing false negatives and improving caption quality."
  },
  {
    "prompt": "summary: We consider a critical issue of false negatives in Vision Language Pre-training (VLP), a challenge that arises from the inherent many-to-many correspondence of image-text pairs in large-scale web-crawled datasets. The presence of false negatives can impede achieving optimal performance and even lead to a significant performance drop. To address this challenge, we propose MAFA (MAnaging FAlse nega tives), which consists of two pivotal components building upon the recently developed GRouped mIni-baTch sampling (GRIT) strategy: 1) an efficient connection mining process that identifies and converts false negatives into positives, and 2) label smoothing for the image-text contrastive (ITC) loss. Our comprehensive experiments verify the effectiveness of MAFA across multiple downstream tasks, emphasizing the crucial role of addressing false negatives in VLP, potentially even surpassing the importance of addressing false posi tives. In addition, the compatibility of MAFA with the recent BLIP-family model is also demonstrated.",
    "chosen": "MAFA tackles false negatives in Vision-Language Pre-training (VLP) through an efficient connection mining process and label smoothing for the image-text contrastive loss, building on the GRIT strategy. It demonstrates significant improvements across downstream tasks, highlighting the importance of addressing false negatives in VLP.",
    "rejected": "The paper introduces MAFA, designed to tackle false negatives in vision-language pre-training. It utilizes a connection mining process and label smoothing for ITC loss. Experiments show MAFA's effectiveness across multiple tasks, demonstrating its compatibility with recent models like BLIP, achieving superior performance by managing false negatives."
  },
  {
    "prompt": "summary: Nonlinearities are decisive in neural representation learning. Traditional Activation (Act) functions im pose fixed inductive biases on neural networks with ori ented biological intuitions. Recent methods leverage self gated curves to compensate for the rigid traditional Act paradigms in fitting flexibility. However, substantial im provements are still impeded by the norm-induced mis matched feature re-calibrations (see Section 1), i.e., the ac tual importance of a feature can be inconsistent with its explicit intensity such that violates the basic intention of a direct self-gated feature re-weighting. To address this problem, we propose to learn discriminative neural feature Act with a novel prototype, namely, AdaShift, which en hances typical self-gated Act by incorporating an adaptive shift factor into the re-weighting function of Act. AdaShift casts dynamic translations on the inputs of a re-weighting function by exploiting comprehensive feature-filter context cues of different ranges in a simple yet effective manner. We obtain the new intuitions of AdaShift by rethinking the feature-filter relationships from a common Softmax-based classification and by generalizing the new observations to a common learning layer that encodes features with updat able filters. Our practical AdaShifts, built upon the new Act prototype, demonstrate significant improvements to the popular/SOTA Act functions on different vision benchmarks. By simply replacing ReLU with AdaShifts, ResNets can match advanced Transformer counterparts (e.g., ResNet-50 vs. Swin-T) with lower cost and fewer parameters.",
    "chosen": "AdaShift enhances traditional self-gated activation functions by incorporating an adaptive shift factor into the re-weighting function. This approach, which exploits comprehensive feature-filter context cues, demonstrates significant improvements over popular activation functions on various vision benchmarks when integrated with ResNet backbones.",
    "rejected": "To address feature recalibration mismatches, AdaShift is proposed, incorporating an adaptive shift factor into activation functions. This method enhances neural networks' discriminative power without significantly increasing parameters or costs, leading to improved performance on various benchmarks."
  },
  {
    "prompt": "summary: Representing signals using coordinate networks domi nates the area of inverse problems recently, and is widely applied in various scientific computing tasks. Still, there exists an issue of spectral bias in coordinate networks, lim iting the capacity to learn high-frequency components. This problem is caused by the pathological distribution of the neural tangent kernel’s (NTK’s) eigenvalues of coordinate networks. We find that, this pathological distribution could be improved using the classical batch normalization (BN), which is a common deep learning technique but rarely used in coordinate networks. BN greatly reduces the maximum and variance of NTK’s eigenvalues while slightly modifies the mean value, considering the max eigenvalue is much larger than the most, this variance change results in a shift of eigenvalues’ distribution from a lower one to a higher one, therefore the spectral bias could be alleviated (see Fig. 1). This observation is substantiated by the significant improvements of applying BN-based coordinate networks to various tasks, including the image compression, computed tomography reconstruction, shape representation, magnetic resonance imaging and novel view synthesis.",
    "chosen": "This paper shows that batch normalization can alleviate the spectral bias in coordinate networks by shifting the distribution of the neural tangent kernel's eigenvalues. This results in significant improvements in tasks like image compression and tomography reconstruction, where coordinate networks are widely applied.",
    "rejected": "Batch normalization (BN) is explored to alleviate spectral bias in coordinate networks, enhancing learning of high-frequency components. BN modifies the distribution of NTK eigenvalues, improving performance in tasks such as image compression and computed tomography reconstruction."
  },
  {
    "prompt": "summary: In this work, we present Digital Life Project, a frame work utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) So cioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorpo rates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character’s digital body. It integrates motion matching, a proven industry technique to ensure mo tion quality, with cutting-edge advancements in motion gen eration for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concur rently, these characters can perform contextually relevant This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 582",
    "chosen": "The Digital Life Project presents a framework for autonomous 3D characters with social intelligence, featuring SocioMind (a digital brain modeling personalities) and MoMat-MoGen (a text-driven motion synthesis paradigm). This enables characters to engage in autonomous social interactions and express with articulated body motions.",
    "rejected": "The Digital Life Project framework uses language to build autonomous 3D characters capable of social interaction and articulated body motions. SocioMind models personalities and initiates dialogues, while MoMat-MoGen synthesizes motion. Together, they enable virtual characters to autonomously engage in dialogue and evolve socio-psychological states, demonstrating state-of-the-art performance in their domains."
  },
  {
    "prompt": "summary: Traditional 3D content creation tools empower users to bring their imagination to life by giving them direct control over a scene’s geometry, appearance, motion, and camera path. Creating computer-generated videos, however, is a tedious manual process, which can be automated by emerg ing text-to-video diffusion models. Despite great promise, video diffusion models are difficult to control, hindering a user to apply their own creativity rather than amplifying it. To address this challenge, we present a novel approach that combines the controllability of dynamic 3D meshes with the expressivity and editability of emerging diffusion models. For this purpose, our approach takes an animated, low-fidelity rendered mesh as input and injects the ground truth cor respondence information obtained from the dynamic mesh into various stages of a pre-trained text-to-image genera tion model to output high-quality and temporally consistent frames. We demonstrate our approach on various examples where motion can be obtained by animating rigged assets or changing the camera path.",
    "chosen": "This novel approach combines the controllability of dynamic 3D meshes with the expressivity of text-to-image diffusion models. By injecting ground truth correspondence information from dynamic meshes into pre-trained text-to-image models, it generates high-quality, temporally consistent frames, enabling contextually relevant character motions.",
    "rejected": "A novel approach combining dynamic 3D meshes with text-to-image generation models is presented. This method injects ground truth correspondence information into stages of a pre-trained model, producing high-quality, temporally consistent frames from animated low-fidelity rendered meshes, enhancing control and creativity in video generation."
  },
  {
    "prompt": "summary: In the current era of generative AI breakthroughs, gener ating panoramic scenes from a single input image remains a key challenge. Most existing methods use diffusion-based iterative or simultaneous multi-view inpainting. However, the lack of global scene layout priors leads to subpar out puts with duplicated objects (e.g., multiple beds in a bed room) or requires time-consuming human text inputs for each view. We propose L-MAGIC, a novel method leverag ing large language models for guidance while diffusing mul tiple coherent views of 360◦panoramic scenes. L-MAGIC harnesses pre-trained diffusion and language models with out ﬁne-tuning, ensuring zero-shot performance. The output quality is further enhanced by super-resolution and multi view fusion techniques. Extensive experiments demonstrate that the resulting panoramic scenes feature better scene lay outs and perspective view rendering quality compared to re lated works, with >70% preference in human evaluations. Combined with conditional diffusion models, L-MAGIC can accept various input modalities, including but not limited to text, depth maps, sketches, and colored scripts. Apply ing depth estimation further enables 3D point cloud gen eration and dynamic scene exploration with ﬂuid camera motion.",
    "chosen": "L-MAGIC leverages large language models to guide the diffusion of multiple coherent views for generating 360° panoramic scenes. It ensures zero-shot performance by utilizing pre-trained models without fine-tuning and enhances output quality with super-resolution and multi-view fusion techniques.",
    "rejected": "L-MAGIC leverages large language models to guide diffusion models for generating coherent views of 360-degree panoramic scenes. It achieves enhanced scene layouts and perspective rendering quality compared to related methods. L-MAGIC supports various input modalities and enables 3D point cloud generation, offering fluid camera motion and immersive experiences."
  },
  {
    "prompt": "summary: Camera-parameter-free multi-view pose estimation is an emerging technique for 3D human pose estimation (HPE). They can infer the camera settings implicitly or explicitly to mitigate the depth uncertainty impact, showcasing sig nificant potential in real applications. However, due to the limited camera setting diversity in the available datasets, the inferred camera parameters are always simply hard coded into the model during training and not adaptable to the input in inference, making the learned models cannot generalize well under unseen camera settings. A natural solution is to artificially synthesize some samples, i.e., 2D 3D pose pairs, under massive new camera settings. Un fortunately, to prevent over-fitting the existing camera set ting, the number of synthesized samples for each new cam era setting should be comparable with that for the existing one, which multiplies the scale of training and even makes it computationally prohibitive. In this paper, we propose a novel HPE approach under the invariant risk minimization (IRM) paradigm. Precisely, we first synthesize 2D poses from myriad camera settings. We then train our model un der the IRM paradigm, which targets at learning a common optimal model across all camera settings and thus enforces the model to automatically learn the camera parameters based on the input data. This allows the model to accurately infer 3D poses on unseen data by training on only a hand ful of samples from each synthesized setting and thus avoid the unbearable training cost increment. Another appealing feature of our method is that benefited from the capability of IRM in identifying the invariant features, its performance on the seen camera settings is enhanced as well. Compre hensive experiments verify the superiority of our approach.",
    "chosen": "PoseIRM enhances 3D human pose estimation on unseen camera settings using invariant risk minimization. It synthesizes 2D poses from various camera settings and trains a model to learn a common optimal solution, allowing accurate 3D pose inference on unseen data with minimal samples per setting.",
    "rejected": "A novel HPE approach under the invariant risk minimization (IRM) paradigm is introduced. It synthesizes 2D poses from myriad camera settings and trains the model to learn common optimal parameters across all settings. This allows accurate inference of 3D poses on unseen data, enhancing generalization capabilities and performance on both seen and unseen camera settings."
  },
  {
    "prompt": "summary: Synthesizing realistic videos of talking faces under cus tom lighting conditions and viewing angles benefits various downstream applications like video conferencing. However, most existing relighting methods are either time-consuming or unable to adjust the viewpoints. In this paper, we present the first real-time 3D-aware method for relighting in-the wild videos of talking faces based on Neural Radiance Fields (NeRF). Given an input portrait video, our method can synthesize talking faces under both novel views and novel lighting conditions with a photo-realistic and disen tangled 3D representation. Specifically, we infer an albedo tri-plane, as well as a shading tri-plane based on a de sired lighting condition for each video frame with fast dual encoders. We also leverage a temporal consistency network to ensure smooth transitions and reduce flickering artifacts. Our method runs at 32.98 fps on consumer-level hardware and achieves state-of-the-art results in terms of reconstruc tion quality, lighting error, lighting instability, temporal consistency and inference speed. We demonstrate the effec tiveness and interactivity of our method on various portrait videos with diverse lighting and viewing conditions.",
    "chosen": "This real-time 3D-aware method for portrait video relighting uses Neural Radiance Fields (NeRF) to synthesize talking faces under novel views and lighting conditions. It infers albedo and shading tri-planes with fast dual-encoders and ensures temporal consistency, achieving state-of-the-art results in reconstruction quality and inference speed.",
    "rejected": "A real-time 3D-aware method for relighting talking face videos based on Neural Radiance Fields (NeRF) is proposed. It synthesizes realistic talking faces under novel views and lighting conditions, ensuring smooth transitions and reducing flickering artifacts. Running at 32.98 fps, it achieves state-of-the-art results in reconstruction quality, temporal consistency, and speed."
  },
  {
    "prompt": "summary: While existing large vision-language multimodal mod els focus on whole image understanding, there is a promi nent gap in achieving region-specific comprehension. Cur rent approaches that use textual coordinates or spatial en codings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we intro duce a novel multimodal model capable of decoding arbi trary (free-form) visual prompts. This allows users to intu itively mark images and interact with the model using nat ural cues like a “red bounding box” or “pointed arrow”. Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encod ings, yet achieves state-of-the-art performance on region understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. ",
    "chosen": "ViP-LLaVA introduces a multimodal model capable of decoding arbitrary visual prompts, allowing users to interact with the model using natural cues like arrows or bounding boxes. This approach achieves state-of-the-art performance on region-understanding tasks and includes ViP-Bench, a benchmark for assessing visual prompt understanding.",
    "rejected": "This paper introduces a novel multimodal model capable of decoding free-form visual prompts, enabling intuitive image interaction using natural cues like \"red bounding boxes\" or \"pointed arrows.\" By overlaying visual markers directly onto RGB images, the method simplifies user interaction without complex region encodings. It achieves state-of-the-art performance on region-understanding tasks such as Visual7W, PointQA, and Visual Commonsense Reasoning benchmarks. Additionally, the authors present ViP-Bench, a benchmark for evaluating models' capabilities in understanding visual prompts across multiple dimensions. This facilitates future research and advancements in user-friendly visual prompting techniques."
  },
  {
    "prompt": "summary: Neural Radiance Fields (NeRFs) have shown great po tential in novel view synthesis. However, they struggle to render sharp images when the data used for training is af fected by motion blur. On the other hand, event cameras ex cel in dynamic scenes as they measure brightness changes with microsecond resolution and are thus only marginally affected by blur. Recent methods attempt to enhance NeRF reconstructions under camera motion by fusing frames and events. However, they face challenges in recovering accu rate color content or constrain the NeRF to a set of pre defined camera poses, harming reconstruction quality in challenging conditions. This paper proposes a novel for mulation addressing these issues by leveraging both model and learning-based modules. We explicitly model the blur formation process, exploiting the event double integral as an additional model-based prior. Additionally, we model the event-pixel response using an end-to-end learnable re sponse function, allowing our method to adapt to non idealities in the real event-camera sensor. We show, on synthetic and real data, that the proposed approach outper forms existing deblur NeRFs that use only frames as well as those that combine frames and events by +6.13dB and +2.48dB, respectively. ",
    "chosen": "Ev-DeblurNeRF addresses motion blur in NeRF reconstructions by fusing frames and events. It models the blur formation process using event double integral as a prior and employs a learnable event-pixel response function. This method outperforms existing deblur NeRFs, achieving significant improvements on synthetic and real data.",
    "rejected": "This paper addresses the issue of motion blur in Neural Radiance Fields (NeRFs) by leveraging both model and learning-based modules, explicitly modeling the blur formation process using event cameras. The proposed method outperforms existing techniques by significant margins."
  },
  {
    "prompt": "summary: Radiologists highly desire fully automated versatile AI for medical imaging interpretation. However, the lack of ex tensively annotated large-scale multi-disease datasets has hindered the achievement of this goal. In this paper, we explore the feasibility of leveraging language as a natu rally high-quality supervision for chest CT imaging. In light of the limited availability of image-report pairs, we boot strap the understanding of 3D chest CT images by distill ing chest-related diagnostic knowledge from an extensively pre-trained 2D X-ray expert model. Speciﬁcally, we pro pose a language-guided retrieval method to match each 3D CT image with its semantically closest 2D X-ray image, and perform pair-wise and semantic relation knowledge distil lation. Subsequently, we use contrastive learning to align images and reports within the same patient while distin guishing them from the other patients. However, the chal lenge arises when patients have similar semantic diagnoses, such as healthy patients, potentially confusing if treated as negatives. We introduce a robust contrastive learning that identiﬁes and corrects these false negatives. We train our model with over 12K pairs of chest CT images and radi ology reports. Extensive experiments across multiple sce narios, including zero-shot learning, report generation, and ﬁne-tuning processes, demonstrate the model’s feasibility in interpreting chest CT images.",
    "chosen": "This paper explores using language as supervision for chest CT imaging by leveraging knowledge from a 2D X-ray expert model. A language-guided retrieval method matches CT images with semantically closest X-ray images, performing knowledge distillation and contrastive learning. The model is trained on over 12K pairs of chest CT images and radiology reports, demonstrating feasibility in interpreting chest CT images across multiple scenarios.",
    "rejected": "This work explores using language as supervision for interpreting chest CT images, distilling knowledge from 2D X-ray images via contrastive learning to understand 3D chest CT scans without extensive annotations. It demonstrates feasibility across various scenarios."
  },
  {
    "prompt": "summary: Diffusion model-based image restoration (IR) aims to use diffusion models to recover high-quality (HQ) images from degraded images, achieving promising performance. Due to the inherent property of diffusion models, most existing methods need long serial sampling chains to restore HQ images step-by-step, resulting in expensive sampling time and high computation costs. Moreover, such long sampling chains hinder understanding the relationship between inputs and restoration results since it is hard to compute the gra dients in the whole chains. In this work, we aim to rethink the diffusion model-based IR models through a different per spective, i.e., a deep equilibrium (DEQ) fixed point system, called DeqIR. Specifically, we derive an analytical solution by modeling the entire sampling chain in these IR models as a joint multivariate fixed point system. Based on the analyti cal solution, we can conduct parallel sampling and restore HQ images without training. Furthermore, we compute fast gradients via DEQ inversion and found that initialization optimization can boost image quality and control the gen eration direction. Extensive experiments on benchmarks demonstrate the effectiveness of our method on typical IR tasks and real-world settings.",
    "chosen": "DeqIR rethinks diffusion model-based image restoration through a deep equilibrium fixed point system. It models the sampling chain as a joint multivariate fixed point system, enabling parallel sampling and restoring high-quality images without training. The method computes fast gradients via DEQ inversion and optimizes initialization, showing effectiveness on various IR tasks.",
    "rejected": "DeqIR is introduced as a deep equilibrium fixed point system for diffusion model-based image restoration. It enables parallel sampling and restores high-quality images efficiently, showing better performance than traditional methods on multiple image restoration tasks."
  },
  {
    "prompt": "summary: We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body’s shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a train able NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the gener ation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deforma tion field. This facilitates the generation of more com plete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face “Janus” problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing meth ods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation.",
    "chosen": "DreamAvatar is a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. It uses a trainable NeRF and pretrained text-to-image diffusion models, leveraging the SMPL model for shape and pose guidance. The dual-observation-space design and joint optimization of losses improve texture completeness and facial details, outperforming existing methods.",
    "rejected": "DreamAvatar generates high-quality 3D human avatars guided by text and shape priors, utilizing trainable NeRF and pretrained text-to-image diffusion models. It achieves complete textures and geometry faithful to target poses, significantly improving facial details."
  },
  {
    "prompt": "summary: We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid ob jects through an iterative denoising process of compressed latent representations. The diffusion-based priors enable more plausible and probabilistic reconstructions when han dling ambiguous inputs. We parameterize 4D dynamics with latent sets instead of using global latent codes. This novel 4D representation allows us to learn local shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizabil ity to unseen motions and identities. For more temporally coherent object tracking, we synchronously denoise defor mation latent sets and exchange information across multi ple frames. To avoid computational overhead, we designed *Equal Contribution. †Corresponding author. ‡Work done during master’s thesis. an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations.",
    "chosen": "Motion2VecSets is a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. It learns shape and motion distribution through an iterative denoising process of compressed latent representations, parameterized with latent sets for local shape and deformation patterns. This approach improves generalizability to unseen motions and identities, demonstrating superiority in 4D reconstruction from imperfect observations.",
    "rejected": "Motion2VecSets, a 4D diffusion model, learns shape and motion distribution through iterative denoising processes. By parameterizing dynamics with latent sets, it leads to accurate reconstructions and improved generalizability to unseen motions and identities."
  },
  {
    "prompt": "summary: Generalized Category Discovery (GCD) aims to identify a mix of known and novel categories within unlabeled data sets, providing a more realistic setting for image recogni tion. Essentially, GCD needs to remember existing pat terns thoroughly to recognize novel categories. Recent state-of-the-art method SimGCD transfers the knowledge from known-class data to the learning of novel classes through debiased learning. However, some patterns are catastrophically forgot during adaptation and thus lead to poor performance in novel categories classiﬁcation. To ad dress this issue, we propose a novel learning approach, Le goGCD, which is seamlessly integrated into previous meth ods to enhance the discrimination of novel classes while maintaining performance on previously encountered known classes. Speciﬁcally, we design two types of techniques termed as Local Entropy Regularization (LER) and Dual views Kullback–Leibler divergence constraint (DKL). The LER optimizes the distribution of potential known class samples in unlabeled data, thus ensuring the preservation of knowledge related to known categories while learning novel classes. Meanwhile, DKL introduces Kullback–Leibler di vergence to encourage the model to produce a similar pre diction distribution of two view samples from the same im age. In this way, it successfully avoids mismatched predic tion and generates more reliable potential known class sam ples simultaneously. Extensive experiments validate that the proposed LegoGCD effectively addresses the known cate gory forgetting issue across all datasets, e.g., delivering a 7.74% and 2.51% accuracy boost on known and novel classes in CUB, respectively. Our",
    "chosen": "LegoGCD addresses catastrophic forgetting in Generalized Category Discovery by integrating Local Entropy Regularization and Dual-views Kullback-Leibler divergence constraint. These techniques optimize the distribution of known class samples and encourage similar prediction distributions from different views, effectively solving the forgetting issue and improving accuracy on both known and novel classes.",
    "rejected": "LegoGCD introduces Local Entropy Regularization (LER) and Dual-views Kullback–Leibler divergence constraint (DKL) to address catastrophic forgetting in generalized category discovery. These techniques enhance novel class discrimination while preserving known class performance, boosting accuracy."
  },
  {
    "prompt": "summary: We present SuperNormal, a fast, high-ﬁdelity approach to multi-view 3D reconstruction using surface normal maps. With a few minutes, SuperNormal produces detailed sur faces on par with 3D scanners. We harness volume ren dering to optimize a neural signed distance function (SDF) powered by multi-resolution hash encoding. To accelerate training, we propose directional ﬁnite difference and patch based ray marching to approximate the SDF gradients nu merically. While not compromising reconstruction quality, this strategy is nearly twice as efﬁcient as analytical gra dients and about three times faster than axis-aligned ﬁnite difference. Experiments on the benchmark dataset demon strate the superiority of SuperNormal in efﬁciency and ac curacy compared to existing multi-view photometric stereo methods. On our captured objects, SuperNormal produces more ﬁne-grained geometry than recent neural 3D recon struction methods. Our",
    "chosen": "SuperNormal is a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. It optimizes a neural signed distance function powered by multi-resolution hash encoding, using directional finite difference and patch-based ray marching. This method is more efficient and accurate than existing MVPS methods, producing detailed surfaces comparable to 3D scanners.",
    "rejected": "SuperNormal presents a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. It optimizes a neural signed distance function with multi-resolution hash encoding, producing detailed surfaces comparable to 3D scanners efficiently."
  },
  {
    "prompt": "summary: High-resolution wide-angle fisheye images are becom ing more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distor tion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN trans former, which combines the highly uniform Hierarchi cal Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patch ing and windowing operations of the SWIN transformer, en abling the network to process spherical representations with minimal computational overhead. We demonstrate the su perior performance of our model on both synthetic and real automotive datasets, as well as a selection of other image datasets, for semantic segmentation, depth regression and classification tasks. Our",
    "chosen": "HEAL-SWIN is a transformer model combining the Hierarchical Equal Area iso-Latitude Pixelation grid with the Hierarchical Shifted-Window transformer. This enables efficient processing of high-resolution, distortion-free spherical data, demonstrating superior performance on automotive datasets for semantic segmentation, depth regression, and classification tasks.",
    "rejected": "HEAL-SWIN transformer combines the HEALPix grid used in astrophysics with SWIN transformer to handle high-resolution fisheye images effectively. It processes spherical representations with minimal computational overhead, showing superior performance in various computer vision tasks."
  },
  {
    "prompt": "summary: Advances in camera-based physiological monitoring have enabled the robust, non-contact measurement of res piration and the cardiac pulse, which are known to be in dicative of the sleep stage. This has led to research into camera-based sleep monitoring as a promising alterna tive to “gold-standard” polysomnography, which is cum bersome, expensive to administer, and hence unsuitable for longer-term clinical studies. In this paper, we introduce SleepVST, a transformer model which enables state-of-the art performance in camera-based sleep stage classifica tion (sleep staging). After pre-training on contact sensor data, SleepVST outperforms existing methods for cardio respiratory sleep staging on the SHHS and MESA datasets, achieving total Cohen’s kappa scores of 0.75 and 0.77 re spectively. We then show that SleepVST can be successfully transferred to cardio-respiratory waveforms extracted from video, enabling fully contact-free sleep staging. Using a video dataset of 50 nights, we achieve a total accuracy of 78.8% and a Cohen’s κ of 0.71 in four-class video-based sleep staging, setting a new state-of-the-art in the domain.",
    "chosen": "SleepVST is a transformer model for camera-based sleep stage classification. Pre-trained on contact sensor data, it outperforms existing methods for cardiorespiratory sleep staging and can be transferred to video-based staging. Using a video dataset of 50 nights, it achieves state-of-the-art performance in contact-free sleep staging.",
    "rejected": "This paper proposes Ev-DeblurNeRF, which combines blurry images and events to recover sharp NeRFs. It leverages event double integral as an additional prior and models the event-pixel response, enabling high-quality reconstructions under challenging conditions."
  },
  {
    "prompt": "summary: The landscape of deep learning research is moving to wards innovative strategies to harness the true potential of data. Traditionally, emphasis has been on scaling model architectures, resulting in large and complex neural net works, which can be difficult to train with limited compu tational resources. However, independently of the model size, data quality (i.e. amount and variability) is still a ma jor factor that affects model generalization. In this work, we propose a novel technique to exploit available data through the use of automatic data augmentation for the tasks of im age classification and semantic segmentation. We intro duce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be pro cessed as videos. Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day. Our intuition is that the increased receptive field in the temporal dimen sion provided by DAS could lead to benefits also to the spa tial receptive field. More specifically, we leverage DAS to guide the reshaping of the spatial receptive field by selecting task-dependant transformations. As a result, compared to standard augmentation alternatives, we improve in terms of accuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when plugging in our DAS over different light-weight video backbones.",
    "chosen": "DAS is a differentiable augmentation search method for image classification and semantic segmentation. It generates image variations processed as videos, guiding the reshaping of the spatial receptive field with task-dependent transformations. This improves accuracy on multiple datasets when integrated with light-weight video backbones.",
    "rejected": "Radiologists desire fully automated versatile AI for medical imaging interpretation. This paper leverages language as supervision by distilling diagnostic knowledge from pre-trained 2D X-ray models to interpret 3D chest CT images. It uses robust contrastive learning to align images and reports within patients, demonstrating feasibility in real-world applications."
  },
  {
    "prompt": "summary: Due to the high potential for abuse of GenAI systems, the task of detecting synthetic images has recently become of great interest to the research community. Unfortunately, ex isting image-space detectors quickly become obsolete as new high-fidelity text-to-image models are developed at blinding speed. In this work, we propose a new synthetic image detec tor that uses features obtained by inverting an open-source pre-trained Stable Diffusion model. We show that these inver sion features enable our detector to generalize well to unseen generators of high visual fidelity (e.g., DALL·E 3) even when the detector is trained only on lower fidelity fake images generated via Stable Diffusion. This detector achieves new state-of-the-art across multiple training and evaluation se tups. Moreover, we introduce a new challenging evaluation protocol that uses reverse image search to mitigate stylistic and thematic biases in the detector evaluation. We show that the resulting evaluation scores align well with detectors’ in the-wild performance, and release these datasets as public benchmarks for future research. *Work done during an internship at Google Research.",
    "chosen": "FakeInversion is a synthetic image detector that uses features from an inverted Stable Diffusion model. This method generalizes well to unseen high-fidelity generators, even when trained on lower-fidelity images. It achieves state-of-the-art results across multiple training and evaluation setups and introduces a new evaluation protocol using reverse image search.",
    "rejected": "Due to the rapid development of high-fidelity text-to-image models, there is an increasing need for synthetic image detection. This research introduces a detector using inversion features from a Stable Diffusion model, proving effective against unseen generators. The evaluation protocol includes reverse image search to mitigate biases, aligning with real-world performance."
  },
  {
    "prompt": "summary: Detecting edges in images suffers from the problems of (P1) heavy imbalance between positive and negative classes as well as (P2) label uncertainty owing to disagreement be tween different annotators. Existing solutions address P1 using class-balanced cross-entropy loss and dice loss and P2 by only predicting edges agreed upon by most annota tors. In this paper, we propose RankED, a uniﬁed ranking based approach that addresses both the imbalance problem (P1) and the uncertainty problem (P2). RankED tackles these two problems with two components: One component which ranks positive pixels over negative pixels, and the second which promotes high conﬁdence edge pixels to have more label certainty. We show that RankED outperforms previous studies and sets a new state-of-the-art on NYUD v2, BSDS500 and Multi-cue datasets.",
    "chosen": "RankED is a unified ranking-based approach for edge detection that addresses both class imbalance and label uncertainty. It ranks positive pixels over negatives and promotes high-confidence edge pixels. RankED outperforms previous methods and sets a new state-of-the-art on NYUD-v2, BSDS500, and Multi-cue datasets.",
    "rejected": "This paper presents RankED, a ranking-based method addressing class imbalance and label uncertainty in edge detection. It ranks positive pixels over negatives and promotes high-confidence edge pixels, outperforming previous studies on datasets like NYUD-v2 and BSDS500."
  },
  {
    "prompt": "summary: Detecting objects in 3D under various (normal and ad verse) weather conditions is essential for safe autonomous driving systems. Recent approaches have focused on em ploying weather-insensitive 4D radar sensors and lever aging them with other modalities, such as LiDAR. How ever, they fuse multi-modal information without considering the sensor characteristics and weather conditions, and lose some height information which could be useful for localiz ing 3D objects. In this paper, we propose a novel frame work for robust LiDAR and 4D radar-based 3D object de tection. Specifically, we propose a 3D-LRF module that considers the distinct patterns they exhibit in 3D space (e.g., precise 3D mapping of LiDAR and wide-range, weather insensitive measurement of 4D radar) and extract fusion features based on their 3D spatial relationship. Then, our weather-conditional radar-flow gating network mod ulates the information flow of fusion features depending on weather conditions, and obtains enhanced feature that effectively incorporates the strength of two domains un der various weather conditions. The extensive experiments demonstrate that our model achieves SoTA performance for 3D object detection under various weather conditions.",
    "chosen": "This paper proposes a novel framework for robust LiDAR and 4D radar-based 3D object detection. It introduces a 3D-LRF module to extract fusion features based on the 3D spatial relationship of LiDAR and radar, and a weather-conditional radar-flow gating network to modulate information flow. The model achieves state-of-the-art performance under various weather conditions.",
    "rejected": "A novel framework for 3D object detection under diverse weather conditions is proposed, utilizing LiDAR and 4D radar. The approach, featuring a 3D-LRF module and weather-conditional radar-flow gating network, demonstrates state-of-the-art performance by effectively fusing sensor data based on their spatial relationships and weather conditions."
  },
  {
    "prompt": "summary: Group robustness strategies aim to mitigate learned bi ases in deep learning models that arise from spurious cor relations present in their training datasets. However, most existing methods rely on the access to the label distribution of the groups, which is time-consuming and expensive to obtain. As a result, unsupervised group robustness strate gies are sought. Based on the insight that a trained model’s classiﬁcation strategies can be inferred accurately based on explainability heatmaps, we introduce ExMap, an unsuper vised two stage mechanism designed to enhance group ro bustness in traditional classiﬁers. ExMap utilizes a cluster ing module to infer pseudo-labels based on a model’s ex plainability heatmaps, which are then used during training in lieu of actual labels. Our empirical studies validate the efﬁcacy of ExMap  We demonstrate that it bridges the per formance gap with its supervised counterparts and outper forms existing partially supervised and unsupervised meth ods. Additionally, ExMap can be seamlessly integrated with existing group robustness learning strategies. Finally, we demonstrate its potential in tackling the emerging issue of multiple shortcut mitigation1.",
    "chosen": "ExMap is an unsupervised two-stage mechanism enhancing group robustness in classifiers using explainability heatmaps for pseudo-label inference. It bridges the performance gap with supervised methods and outperforms existing unsupervised approaches, demonstrating effectiveness in mitigating spurious correlations.",
    "rejected": "ExMap, an unsupervised mechanism, enhances group robustness in classifiers without relying on labeled groups. By clustering explainability heatmaps, it infers pseudo-labels used during training, bridging the performance gap with supervised methods and outperforming existing partially supervised and unsupervised techniques."
  },
  {
    "prompt": "summary: Bird’s-eye View (BeV) representations have emerged as the de-facto shared space in driving applications, offer ing a unified space for sensor data fusion and supporting various downstream tasks. However, conventional models use grids with fixed resolution and range and face compu tational inefficiencies due to the uniform allocation of re sources across all cells. To address this, we propose Point BeV, a novel sparse BeV segmentation model operating on sparse BeV cells instead of dense grids. This approach of fers precise control over memory usage, enabling the use of long temporal contexts and accommodating memory constrained platforms. PointBeV employs an efficient two pass strategy for training, enabling focused computation on regions of interest. At inference time, it can be used with various memory/performance trade-offs and flexibly adjusts to new specific use cases. PointBeV achieves state of-the-art results on the nuScenes dataset for vehicle, pedes trian, and lane segmentation, showcasing superior perfor mance in static and temporal settings despite being trained solely with sparse signals. We release our code with two new efficient modules used in the architecture: Sparse Fea ture Pulling, designed for the effective extraction of features from images to BeV, and Submanifold Attention, which en ables efficient temporal modeling. The",
    "chosen": "PointBeV is a sparse Bird’s-eye View (BeV) segmentation model operating on sparse BeV cells. It offers memory usage control, enabling long temporal contexts and efficient computation. PointBeV achieves state-of-the-art results on nuScenes for vehicle, pedestrian, and lane segmentation with sparse signals.",
    "rejected": "PointBeV is introduced as a sparse Bird’s-eye View (BeV) segmentation model that operates efficiently on sparse BeV cells instead of dense grids. This approach offers precise memory control and achieves state-of-the-art results on the nuScenes dataset, particularly in static and temporal settings, through its efficient two-pass training strategy."
  },
  {
    "prompt": "summary: Coordinate based implicit neural representations have gained rapid popularity in recent years as they have been successfully used in image, geometry and scene modeling tasks. In this work, we present a novel use case for such implicit representations in the context of learning anatomi cally constrained face models. Actor speciﬁc anatomically constrained face models are the state of the art in both facial performance capture and performance retargeting. Despite their practical success, these anatomical models are slow to evaluate and often require extensive data capture to be built. We propose the anatomical implicit face model; an ensem ble of implicit neural networks that jointly learn to model the facial anatomy and the skin surface with high-ﬁdelity, and can readily be used as a drop in replacement to con ventional blendshape models. Given an arbitrary set of skin surface meshes of an actor and only a neutral shape with estimated skull and jaw bones, our method can recover a dense anatomical substructure which constrains every point on the facial surface. We demonstrate the usefulness of our approach in several tasks ranging from shape ﬁtting, shape editing, and performance retargeting.",
    "chosen": "AIM is an ensemble of implicit neural networks modeling facial anatomy and skin surface with high-fidelity. It recovers dense anatomical substructures from skin surface meshes and neutral shapes, providing advantages in shape fitting, editing, and performance retargeting.",
    "rejected": "The anatomical implicit face model uses implicit neural networks to jointly learn facial anatomy and skin surface with high fidelity. Given skin surface meshes and neutral shapes with estimated bones, it recovers dense anatomical substructures, enabling applications ranging from shape fitting to performance retargeting."
  },
  {
    "prompt": "summary: Superpixels play a crucial role in image processing by partitioning an image into clusters of pixels with similar vi sual attributes. This facilitates subsequent image process ing tasks, offering computational advantages over the ma nipulation of individual pixels. While numerous overseg mentation techniques have emerged in recent years, many rely on predefined initialization and termination criteria. In this paper, a novel top-down superpixel segmentation algo rithm called Hierarchical Histogram Threshold Segmenta tion (HHTS) is introduced. It eliminates the need for ini tialization and implements auto-termination, outperform ing state-of-the-art methods w.r.t. boundary recall. This is achieved by iteratively partitioning individual pixel seg ments into foreground and background and applying inten sity thresholding across multiple color channels. The un derlying iterative process constructs a superpixel hierarchy that adapts to local detail distributions until color infor mation exhaustion. Experimental results demonstrate the superiority of the proposed approach in terms of bound ary adherence, while maintaining competitive runtime per formance on the BSDS500 and NYUV2 datasets. Further more, an application of HHTS in refining machine learning based semantic segmentation masks produced by the Seg ment Anything Foundation Model (SAM) is presented.",
    "chosen": "HHTS is a novel top-down superpixel segmentation algorithm eliminating initialization and implementing auto-termination. It outperforms state-of-the-art methods in boundary recall by iteratively partitioning pixel segments and applying intensity thresholding, demonstrating superior boundary adherence on BSDS500 and NYUV2 datasets.",
    "rejected": "HHTS, a top-down superpixel segmentation algorithm, automatically terminates and outperforms state-of-the-art methods in boundary recall. It iteratively partitions pixel segments into foreground and background, applying intensity thresholding across multiple channels, demonstrating superior performance on BSDS500 and NYUV2 datasets."
  },
  {
    "prompt": "summary: The study of complex human interactions and group ac tivities has become a focal point in human-centric computer vision. However, progress in related tasks is often hindered by the challenges of obtaining large-scale labeled datasets from real-world scenarios. To address the limitation, we in troduce M3Act, a synthetic data generator for multi-view multi-group multi-person human atomic actions and group activities. Powered by Unity Engine, M3Act features mul tiple semantic groups, highly diverse and photorealistic im ages, and a comprehensive set of annotations, which facil itates the learning of human-centered tasks across single person, multi-person, and multi-group conditions. We demonstrate the advantages of M3Act across three core ex periments. The results suggest our synthetic dataset can sig nificantly improve the performance of several downstream methods and replace real-world datasets to reduce cost. Notably, M3Act improves the state-of-the-art MOTRv2 on DanceTrack dataset, leading to a hop on the leaderboard from 10th to 2nd place. Moreover, M3Act opens new re search for controllable 3D group activity generation. We define multiple metrics and propose a competitive baseline for the novel task. Our code and data are available at our",
    "chosen": "M3Act is a synthetic data generator for multi-view, multi-group, multi-person human actions and activities. Powered by Unity Engine, it offers diverse, photorealistic images and comprehensive annotations, improving downstream methods and enabling controllable 3D group activity generation.",
    "rejected": "M3Act, a synthetic data generator for multi-view multi-group human activities, is presented. Utilizing Unity Engine, M3Act provides photorealistic images and comprehensive annotations, enhancing learning for human-centric tasks. It improves state-of-the-art MOTRv2 on DanceTrack, opening new research avenues for controllable 3D group activity generation."
  },
  {
    "prompt": "summary: 3D visual grounding involves matching natural language descriptions with their corresponding objects in 3D spaces. Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent. In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model integrates a self-attention-based scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships. Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis. Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions.",
    "chosen": "MiKASA is a transformer model for 3D visual grounding integrating a scene-aware object encoder and multi-key-anchor technique. It enhances object recognition accuracy and spatial relationship understanding, achieving the highest accuracy in the Referit3D challenge.",
    "rejected": "The paper discusses the challenges in detecting synthetic images due to advancements in generative AI systems. A new detector leveraging inversion features from a pre-trained model shows robust generalization to unseen generators. Additionally, a novel evaluation protocol using reverse image search helps mitigate biases in detector evaluations."
  },
  {
    "prompt": "summary: The spiking cameras offer the benefits of high dynamic range (HDR), high temporal resolution, and low data redun dancy. However, reconstructing HDR videos in high-speed conditions using single-bit spikings presents challenges due to the limited bit depth. Increasing the bit depth of the spik ings is advantageous for boosting HDR performance, but the readout efficiency will be decreased, which is unfavorable for achieving a high frame rate (HFR) video. To address these challenges, we propose a readout mechanism to ob tain rolling-mixed-bit (RMB) spikings, which involves inter leaving multi-bit spikings within the single-bit spikings in a rolling manner, thereby combining the characteristics of high bit depth and efficient readout. Furthermore, we introduce RMB-Net for reconstructing HDR and HFR videos. RMB Net comprises a cross-bit attention block for fusing mixed-bit spikings and a cross-time attention block for achieving tem poral fusion. Extensive experiments conducted on synthetic and real-synthetic data demonstrate the superiority of our method. For instance, pure 3-bit spikings result in 3 times of data volume, whereas our method achieves comparable performance with less than 2% increase in data volume.",
    "chosen": "RMB-Net reconstructs HDR and HFR videos from rolling-mixed-bit spikings. It uses a cross-bit attention block for fusing mixed-bit spikings and a cross-time attention block for temporal fusion, achieving superior performance with minimal data volume increase.",
    "rejected": "A novel approach for robust 3D object detection under various weather conditions combines LiDAR and 4D radar data. By considering distinct patterns in 3D space and modulating information flow based on weather conditions, this method enhances fusion features, achieving state-of-the-art performance in adverse weather scenarios."
  },
  {
    "prompt": "summary: In subject-driven text-to-image synthesis, the synthesis process tends to be heavily inﬂuenced by the reference im ages provided by users, often overlooking crucial attributes detailed in the text prompt. In this work, we propose Subject-Agnostic Guidance (SAG), a simple yet effective solution to remedy the problem. We show that through constructing a subject-agnostic condition and applying our proposed dual classiﬁer-free guidance, one could obtain outputs consistent with both the given subject and input text prompts. We validate the efﬁcacy of our approach through both optimization-based and encoder-based methods. Ad ditionally, we demonstrate its applicability in second-order customization methods, where an encoder-based model is ﬁne-tuned with DreamBooth. Our approach is conceptually simple and requires only minimal code modiﬁcations, but leads to substantial quality improvements, as evidenced by our evaluations and user studies.",
    "chosen": "Subject-Agnostic Guidance (SAG) is introduced to improve subject-driven text-to-image synthesis. SAG constructs a subject-agnostic condition and applies dual classifier-free guidance, ensuring outputs align with both the subject and text prompts. This method enhances quality in optimization-based, encoder-based, and second-order customization approaches.",
    "rejected": "In subject-driven text-to-image synthesis, existing methods often emphasize subject-specific details over attributes described in text prompts. This paper introduces Subject-Agnostic Guidance (SAG), which ensures outputs align with both the subject and input text descriptions. SAG uses dual classifier-free guidance to improve image synthesis quality significantly."
  },
  {
    "prompt": "summary: Single-photon Light Detection and Ranging (LiDAR) systems are often equipped with an array of detectors for improved spatial resolution and sensing speed. However, given a fixed amount of flux produced by the laser trans mitter across the scene, the per-pixel Signal-to-Noise Ra tio (SNR) will decrease when more pixels are packed in a unit space. This presents a fundamental trade-off between the spatial resolution of the sensor array and the SNR re ceived at each pixel. Theoretical characterization of this fundamental limit is explored. By deriving the photon ar rival statistics and introducing a series of new approxi mation techniques, the Mean Squared Error (MSE) of the maximum-likelihood estimator of the time delay is derived. The theoretical predictions align well with simulations and real data.",
    "chosen": "This paper explores the fundamental trade-off in single-photon LiDAR between spatial resolution and Signal-to-Noise Ratio (SNR). It derives the Mean Squared Error (MSE) of the maximum-likelihood estimator for time delay, providing theoretical insights into the sensor's performance limits.",
    "rejected": "Single-photon LiDAR systems face a trade-off between spatial resolution and per-pixel Signal-to-Noise Ratio (SNR). By analyzing photon arrival statistics, this paper derives the Mean Squared Error (MSE) of time delay estimations, providing theoretical insights into optimizing SNR versus spatial resolution in single-photon detectors."
  },
  {
    "prompt": "summary: We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaus sian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable train ing as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally sup ported representations, we predict a dense probability dis tribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and ed itable 3D radiance field. Additional materials can be found on the project website. 1",
    "chosen": "pixelSplat is a feed-forward model reconstructing 3D radiance fields from image pairs using 3D Gaussian primitives. It offers real-time rendering and memory-efficient training, outperforming state-of-the-art methods in novel view synthesis with faster rendering and explicit 3D representations.",
    "rejected": "PixelSplat is introduced as a feed-forward model for reconstructing 3D radiance fields from pairs of images using 3D Gaussian primitives. It offers real-time rendering and efficient memory usage, making it suitable for scalable training and fast 3D reconstruction. Experiments show superior performance compared to state-of-the-art methods."
  },
  {
    "prompt": "summary: This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text prompts. To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects’ category, and thus, it is applicable to general objects. For motion generation, a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically This research was conducted when Jihyeon Kim was a graduate student (Master candidate) at UNIST†. Co-last authors∗. plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the object hand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. ",
    "chosen": "A method for generating 3D hand-object interaction sequences from text prompts is proposed. It decomposes the task into contact generation and motion generation, using a VAE-based network and a Transformer-based diffusion model. This approach generates diverse interactions applicable to unseen objects.",
    "rejected": "This paper presents a novel method for generating sequences of hand-object interactions in 3D guided by text. It decomposes the task into contact generation and motion generation subtasks, utilizing a VAE-based network and Transformer-based diffusion model respectively. The approach generates realistic interactions applicable to unseen objects, enhancing the potential for diverse applications."
  },
  {
    "prompt": "summary: We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to pro duce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hier archical correlation clustering. For this purpose, we first in vestigate embedding the respective hierarchy to be used for tree preserving embedding and feature extraction. There after, we study the extension of minimax distance measures to correlation clustering, as another representation learn ing paradigm. Finally, we demonstrate the performance of our methods on several datasets.",
    "chosen": "Hierarchical correlation clustering is extended to handle positive and negative pairwise dissimilarities. The paper studies unsupervised representation learning using hierarchical correlation clustering, proposing methods for tree preserving embedding and feature extraction.",
    "rejected": "A hierarchical correlation clustering method is proposed that extends traditional correlation clustering to handle positive and negative pairwise dissimilarities. It explores unsupervised representation learning through embedding hierarchies and minimax distance measures. This approach provides an effective solution for clustering complex data structures."
  },
  {
    "prompt": "summary: Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evalu ation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person per spective. However, the capability of VLMs to “think” from a first-person perspective, a crucial attribute for advanc ing autonomous agents and robotics, remains largely unex plored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encom passes six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from ego centric videos, with manually annotated question-answer pairs containing first-person information. To comprehen sively assess VLMs, we evaluate twenty-one popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evalu ated VLMs still possess considerable potential for improve ment in first-person perspective tasks. Meanwhile, enlarg ing the number of trainable parameters has the most signif icant impact on model performance on EgoThink. In con clusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispens able resource for future research in the realm of embodied artificial intelligence and robotics. *Equal contribution, ‡ Project lead, B Corresponding author",
    "chosen": "EgoThink is a visual question-answering benchmark assessing VLMs' ability to think from a first-person perspective. It includes six core capabilities with twelve detailed dimensions, evaluated using manually annotated ego-centric video clips and GPT-4 for answer grading.",
    "rejected": "EgoThink benchmarks vision-language models on their ability to understand first-person perspective tasks. With six core capabilities and twelve detailed dimensions, it evaluates twenty-one popular models using GPT-4 for grading. Results indicate significant room for improvement in first-person perspective understanding, highlighting the benchmark's importance for future research."
  },
  {
    "prompt": "summary: This paper proposes a novel task named ”3D part group ing”. Suppose there is a mixed set containing scattered parts from various shapes. This task requires algorithms to find out every possible combination among all the parts. To address this challenge, we propose the so called Gradi ent Field-based Auto-Regressive Sampling framework (G FARS) tailored specifically for the 3D part grouping task. In our framework, we design a gradient-field-based selection graph neural network (GNN) to learn the gradients of a log conditional probability density in terms of part selection, where the condition is the given mixed part set. This in novative approach, implemented through the gradient-field based selection GNN, effectively captures complex relation ships among all the parts in the input. Upon completion of the training process, our framework becomes capable of autonomously grouping 3D parts by iteratively selecting them from the mixed part set, leveraging the knowledge ac quired by the trained gradient-field-based selection GNN. Our",
    "chosen": "G-FARS is a framework for 3D part grouping using a gradient-field-based selection graph neural network. It learns part relationships and groups 3D parts autonomously, applicable to mixed part sets with unknown group numbers.",
    "rejected": "Introducing \"3D part grouping,\" this paper proposes a Gradient Field-based Auto-Regressive Sampling framework (G-FARS) designed to identify all possible combinations among scattered parts. Using a gradient-field-based selection graph neural network, G-FARS autonomously groups parts from mixed sets, demonstrating its capability in handling complex part relationships."
  },
  {
    "prompt": "summary: Backdoor attack poses a significant security threat to Deep Learning applications. Existing attacks are often not evasive to established backdoor detection techniques. This susceptibility primarily stems from the fact that these attacks typically leverage a universal trigger pattern or transfor mation function, such that the trigger can cause misclas sification for any input. In response to this, recent papers have introduced attacks using sample-specific invisible trig gers crafted through special transformation functions. While these approaches manage to evade detection to some extent, they reveal vulnerability to existing backdoor mitigation techniques. To address and enhance both evasiveness and resilience, we introduce a novel backdoor attack LOTUS. Specifically, it leverages a secret function to separate sam ples in the victim class into a set of partitions and applies unique triggers to different partitions. Furthermore, LOTUS incorporates an effective trigger focusing mechanism, en suring only the trigger corresponding to the partition can induce the backdoor behavior. Extensive experimental re sults show that LOTUS can achieve high attack success rate across 4 datasets and 7 model structures, and effectively evading 13 backdoor detection and mitigation techniques. The",
    "chosen": "LOTUS is a novel backdoor attack leveraging secret functions to partition victim-class samples and apply unique triggers. It incorporates a trigger focusing mechanism, achieving high attack success rates across datasets and model structures while evading detection and mitigation techniques.",
    "rejected": "LOTUS is a novel backdoor attack leveraging a secret function to partition victim class samples and apply unique triggers to different partitions. It includes a trigger focusing mechanism ensuring only designated triggers can induce backdoor behavior. LOTUS achieves high attack success rates across various datasets and models while evading numerous detection and mitigation techniques."
  },
  {
    "prompt": "summary: We present Cutie, a video object segmentation (VOS) net work with object-level memory reading, which puts the object representation from memory back into the video object seg mentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In con trast, Cutie performs top-down object-level memory reading by adapting a small set of object queries. Via those, it in teracts with the bottom-up pixel features iteratively with a query-based object transformer (qt, hence Cutie). The ob ject queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accu rate segmentation. Together with foreground-background masked attention, Cutie cleanly separates the semantics of the foreground object from the background. On the challeng ing MOSE dataset, Cutie improves by 8.7 J &F over XMem with a similar running time and improves by 4.2 J &F over DeAOT while being three times faster.",
    "chosen": "Cutie is a video object segmentation network with object-level memory reading using object queries and a query-based transformer. It improves segmentation accuracy by separating foreground and background semantics, outperforming existing methods on challenging datasets.",
    "rejected": "We present Cutie, a video object segmentation method that leverages spatiotemporal context for accurate and efficient segmentation. Cutie integrates deep learning techniques with dynamic programming to refine object boundaries over time, achieving state-of-the-art performance on several benchmark datasets."
  },
  {
    "prompt": "summary: The problem of calibrating deep neural networks (DNNs) for multi-label learning is considered. It is well known that DNNs trained by cross-entropy for single-label, or one-hot, classification are poorly calibrated. Many cali bration techniques have been proposed to address the prob lem. However, little attention has been paid to the cali bration of multi-label DNNs. In this literature, the focus has been on improving labeling accuracy in the face of se vere dataset unbalance. This is addressed by the introduc tion of asymmetric losses, which have became very popular. However, these losses do not induce well calibrated clas sifiers. In this work, we first provide a theoretical expla nation for this poor calibration performance, by showing that these loses losses lack the strictly proper property, a necessary condition for accurate probability estimation. To overcome this problem, we propose a new Strictly Proper Asymmetric (SPA) loss. This is complemented by a Label Pair Regularizer (LPR) that increases the number of cali bration constraints introduced per training example. The effectiveness of both contributions is validated by extensive experiments on various multi-label datasets. The resulting training method is shown to significantly decrease the cal ibration error while maintaining state-of-the-art accuracy.",
    "chosen": "A new Strictly Proper Asymmetric (SPA) loss and Label Pair Regularizer (LPR) are proposed to calibrate multi-label DNNs. These methods address poor calibration in multi-label learning, significantly reducing calibration error while maintaining accuracy.",
    "rejected": "This paper addresses the calibration of deep neural networks (DNNs) for multi-label learning. While asymmetric losses improve labeling accuracy under dataset imbalance, they fail to produce well-calibrated classifiers. The authors show that these losses lack strictly proper properties, essential for accurate probability estimation. To solve this, they propose a Strictly Proper Asymmetric (SPA) loss and introduce a Label Pair Regularizer (LPR) to increase calibration constraints per training example. Extensive experiments validate the effectiveness of both contributions, significantly reducing calibration error while maintaining state-of-the-art accuracy. This work highlights the importance of calibration in safety-critical applications such as medical diagnosis and cost-sensitive domains like fraud detection."
  },
  {
    "prompt": "summary: Image denoising is a fundamental task in computer vi sion. While prevailing deep learning-based supervised and self-supervised methods have excelled in eliminating in distribution noise, their susceptibility to out-of-distribution (OOD) noise remains a significant challenge. The re cent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation. Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored. This paper un covers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising. Leveraging these properties, we devise an asymmetrical encoder-decoder denoising net work, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising. The progressive feature augmen tation strategy is further proposed to mitigate feature over fitting and improve the robustness of the learnable decoder. Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method.",
    "chosen": "This paper proposes a method to enhance the generalization of image denoising models using features from the frozen ResNet encoder of CLIP. The authors devise an asymmetrical encoder-decoder network and introduce a progressive feature augmentation strategy. Experiments show superior generalization across diverse out-of-distribution noises.",
    "rejected": "Image denoising methods struggle with out-of-distribution (OOD) noise. This paper explores using dense features from CLIP's frozen ResNet encoder for generalizable denoising, proposing an asymmetrical encoder-decoder network. It incorporates a progressive feature augmentation strategy to mitigate overfitting, demonstrating superior performance across various OOD noises."
  },
  {
    "prompt": "summary: The Segment Anything Model (SAM) has garnered sig nificant attention for its versatile segmentation abilities and intuitive prompt-based interface. However, its application in medical imaging presents challenges, requiring either substantial training costs and extensive medical datasets for full model fine-tuning or high-quality prompts for optimal performance. This paper introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient fine-tuning of med ical images via a two-stage hierarchical decoding proce dure. In the initial stage, H-SAM employs SAM’s original decoder to generate a prior probabilistic mask, guiding a more intricate decoding process in the second stage. Specif ically, we propose two key designs: 1) A class-balanced, mask-guided self-attention mechanism addressing the un balanced label distribution, enhancing image embedding; 2) A learnable mask cross-attention mechanism spatially modulating the interplay among different image regions based on the prior mask. Moreover, the inclusion of a hier archical pixel decoder in H-SAM enhances its proficiency in capturing fine-grained and localized details. This approach enables SAM to effectively integrate learned medical priors, facilitating enhanced adaptation for medical image seg mentation with limited samples. Our H-SAM demonstrates a 4.78% improvement in average Dice compared to existing prompt-free SAM variants for multi-organ segmentation us ing only 10% of 2D slices. Notably, without using any unla beled data, H-SAM even outperforms state-of-the-art semi supervised models relying on extensive unlabeled training data across various medical datasets. Our",
    "chosen": "This paper introduces H-SAM, a prompt-free adaptation of SAM for efficient fine-tuning in medical image segmentation. It employs a two-stage hierarchical decoding procedure with novel attention mechanisms and a hierarchical pixel decoder. H-SAM achieves significant improvements in average Dice scores with limited training data and outperforms semi-supervised models without using unlabeled data.",
    "rejected": "H-SAM enhances Segment Anything Model (SAM) for medical image segmentation through a prompt-free adaptation using a two-stage hierarchical decoding procedure. By addressing unbalanced label distribution and spatial modulation among regions, H-SAM shows a 4.78% improvement in Dice score compared to existing SAM variants, even outperforming state-of-the-art semi-supervised models."
  },
  {
    "prompt": "summary: Driver’s eye gaze holds a wealth of cognitive and inten tional cues crucial for intelligent vehicles. Despite its sig niﬁcance, research on in-vehicle gaze estimation remains limited due to the scarcity of comprehensive and well annotated datasets in real driving scenarios. In this pa per, we present three novel elements to advance in-vehicle gaze research. Firstly, we introduce IVGaze, a pioneering dataset capturing in-vehicle gaze, collected from 125 sub jects and covering a large range of gaze and head poses within vehicles. In this dataset, we propose a new vision based solution for in-vehicle gaze collection, introducing a reﬁned gaze target calibration method to tackle annota tion challenges. Second, our research focuses on in-vehicle gaze estimation leveraging the IVGaze. In-vehicle face im ages often suffer from low resolution, prompting our in troduction of a gaze pyramid transformer that leverages transformer-based multilevel features integration. Expand ing upon this, we introduce the dual-stream gaze pyramid transformer (GazeDPTR). Employing perspective transfor mation, we rotate virtual cameras to normalize images, uti lizing camera pose to merge normalized and original im ages for accurate gaze estimation. GazeDPTR shows state of-the-art performance on the IVGaze dataset. Thirdly, we explore a novel strategy for gaze zone classiﬁcation by extending the GazeDPTR. A foundational tri-plane and project gaze onto these planes are newly deﬁned. Leverag ing both positional features from the projection points and visual attributes from images, we achieve superior perfor mance compared to relying solely on visual features, sub stantiating the advantage of gaze estimation. ",
    "chosen": "This paper presents IVGaze, a novel dataset for in-vehicle gaze estimation, along with a vision-based collection solution. The authors propose gaze pyramid transformers and a dual-stream gaze pyramid transformer (GazeDPTR) for accurate gaze estimation. They also explore gaze zone classification by projecting gaze onto tri-planes and leveraging positional features and visual attributes.",
    "rejected": "This paper introduces IVGaze, a dataset for in-vehicle gaze estimation, and proposes GazeDPTR, which utilizes transformer-based multilevel features integration and dual-stream processing. GazeDPTR achieves state-of-the-art performance on the IVGaze dataset, enhancing accuracy in gaze zone classification by defining tri-plane projections and leveraging positional features from these projections."
  },
  {
    "prompt": "summary: Large-scale visual-language pre-trained models have achieved significant success in various video tasks. How ever, most existing methods follow an “adapt then align” paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static im ages to complicated activity concepts. In this paper, we pro pose a novel “Align before Adapt” (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned enti ties, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This paradigm reuses the visual-language alignment of VLP during adaptation and tries to explain an action by the underlying entities. This helps understand actions by bridging the gap with complex activity semantics, partic ularly when facing unfamiliar or unseen categories. ALT demonstrates competitive performance while maintaining remarkably low computational costs. In fully supervised ex periments, it achieves 88.1% top-1 accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the previous state-of-the-art methods in both zero-shot and few shot experiments, emphasizing its superior generalizability across various learning scenarios.",
    "chosen": "This paper proposes \"Align before Adapt\" (ALT), a novel paradigm for video action recognition that establishes entity-to-region alignments before adapting to video representation learning. ALT reuses visual-language alignments during adaptation and demonstrates competitive performance with low computational costs across fully supervised, zero-shot, and few-shot experiments.",
    "rejected": "ALT paradigm advances video action recognition by aligning entity-to-region before adapting to video-level representations. By grounding text embeddings of relevant entities into image regions, ALT bridges the gap between static images and complex activity semantics. It achieves competitive performance while maintaining low computational costs, outperforming previous methods in both zero-shot and few-shot scenarios."
  },
  {
    "prompt": "summary: We present the pioneering Large Visual Motion Model (LVMM), meticulously engineered to analyze the intrinsic dynamics encapsulated within real-world imagery. Our model, fortified with a wealth of prior knowledge extracted from billions of image pairs, demonstrates promising results in predicting a diverse spectrum of scene dynamics. As a result, it can infuse any generic image with authentic dynamic effects, enhancing its visual allure.",
    "chosen": "This paper presents LVMM, a large visual motion model designed to predict local motion within scenes. The model comprises a motion rendering model and a motion diffusion model, and it can infuse static images with dynamic effects. The authors demonstrate promising results in predicting diverse scene dynamics.",
    "rejected": "LVMM predicts scene dynamics to animate static images, enhancing visual allure. The model extracts latent motion vectors and generates motion trajectories using a diffusion model. LVMM accurately predicts local motion trajectory, producing realistic dynamic effects. Its contributions include pioneering the prediction of diverse scene dynamics and introducing mechanisms for motion rendering and diffusion."
  },
  {
    "prompt": "summary: Recent advancements in text-to-image technology have significantly advanced the field of image customization. Among various applications, the task of customizing diverse scenes for user-specified composited elements holds great application value but has not been extensively explored. Addressing this gap, we propose AnyScene, a specialized framework designed to create varied scenes from compos ited foreground using textual prompts. AnyScene addresses the primary challenges inherent in existing methods, partic ularly scene disharmony due to a lack of foreground seman tic understanding and distortion of foreground elements. Specifically, we develop a foreground injection module that guides a pre-trained diffusion model to generate cohesive scenes in visual harmony with the provided foreground. To enhance robust generation, we implement a layout control strategy that prevents distortions of foreground elements. Furthermore, an efficient image blending mechanism seam lessly reintegrates foreground details into the generated scenes, producing outputs with overall visual harmony and precise foreground details. In addition, we propose a new benchmark and a series of quantitative metrics to evaluate this proposed image customization task. Extensive exper imental results demonstrate the effectiveness of AnyScene, which confirms its potential in various applications.",
    "chosen": "This paper introduces AnyScene, a framework for creating customized scenes from composited foregrounds using textual prompts. It includes a foreground injection module, layout control strategy, and image blending mechanism. The framework addresses scene disharmony and foreground distortion issues and shows effectiveness in various applications.",
    "rejected": "AnyScene customizes scenes based on composited foreground elements using textual prompts. It addresses disharmony and distortion issues by developing a foreground injection module, layout control strategy, and efficient image blending mechanism. AnyScene ensures visual harmony and precise foreground details, demonstrating its effectiveness in various applications through extensive experiments."
  },
  {
    "prompt": "summary: This work introduces ArtAdapter, a transformative text to-image (T2I) style transfer framework that transcends tra ditional limitations of color, brushstrokes, and object shape, capturing high-level style elements such as composition and distinctive artistic expression. The integration of a multi level style encoder with our proposed explicit adaptation mechanism enables ArtAdapter to achieve unprecedented fidelity in style transfer, ensuring close alignment with tex tual descriptions. Additionally, the incorporation of an Aux iliary Content Adapter (ACA) effectively separates content from style, alleviating the borrowing of content from style references. Moreover, our novel fast finetuning approach could further enhance zero-shot style representation while mitigating the risk of overfitting. Comprehensive evalua tions confirm that ArtAdapter surpasses current state-of the-art methods.",
    "chosen": "This paper presents ArtAdapter, a text-to-image style transfer framework that captures high-level style elements. It integrates a multi-level style encoder with an explicit adaptation mechanism and an Auxiliary Content Adapter. The framework achieves unprecedented fidelity in style transfer and surpasses current state-of-the-art methods.",
    "rejected": "ArtAdapter transforms text descriptions into stylistically rich images, capturing high-level style elements beyond color and brushstrokes. Featuring a multi-level style encoder and explicit adaptation mechanism, it separates content from style and employs fast finetuning to enhance zero-shot style representation. Comprehensive evaluations confirm ArtAdapter's superiority over current state-of-the-art methods."
  },
  {
    "prompt": "summary: Understanding how attention varies across individuals has significant scientific and societal impacts. However, exist ing visual scanpath models treat attention uniformly, ne glecting individual differences. To bridge this gap, this pa per focuses on individualized scanpath prediction (ISP), a new attention modeling task that aims to accurately predict how different individuals shift their attention in diverse vi sual tasks. It proposes an ISP method featuring three novel technical components: (1) an observer encoder to charac terize and integrate an observer’s unique attention traits, (2) an observer-centric feature integration approach that holistically combines visual features, task guidance, and observer-specific characteristics, and (3) an adaptive fix ation prioritization mechanism that refines scanpath pre dictions by dynamically prioritizing semantic feature maps based on individual observers’ attention traits. These novel components allow scanpath models to effectively address the attention variations across different observers. Our method is generally applicable to different datasets, model architectures, and visual tasks, offering a comprehensive tool for transforming general scanpath models into indi vidualized ones. Comprehensive evaluations using value based and ranking-based metrics verify the method’s effec tiveness and generalizability.",
    "chosen": "This paper focuses on individualized scanpath prediction (ISP) to model individual variations in eye movements. The proposed method includes an observer encoder, observer-centric feature integration, and adaptive fixation prioritization. The approach effectively transforms general scanpath models into individualized ones and shows strong generalizability.",
    "rejected": "ISP method focuses on predicting individual scanpaths during visual tasks. It integrates observer-specific characteristics, combines task guidance with visual features, and refines predictions using an adaptive fixation prioritization mechanism. The approach effectively addresses attention variations across individuals, verified through comprehensive evaluations on various datasets and visual tasks."
  },
  {
    "prompt": "summary: How to effectively explore multi-scale representations of rain streaks is important for image deraining. In contrast to existing Transformer-based methods that depend mostly on single-scale rain appearance, we develop an end-to-end multi-scale Transformer that leverages the potentially use ful features in various scales to facilitate high-quality im age reconstruction. To better explore the common degra dation representations from spatially-varying rain streaks, we incorporate intra-scale implicit neural representations based on pixel coordinates with the degraded inputs in a closed-loop design, enabling the learned features to facili tate rain removal and improve the robustness of the model in complex scenarios. To ensure richer collaborative rep resentation from different scales, we embed a simple yet ef fective inter-scale bidirectional feedback operation into our multi-scale Transformer by performing coarse-to-ﬁne and ﬁne-to-coarse information communication. Extensive ex periments demonstrate that our approach, named as NeRD Rain, performs favorably against the state-of-the-art ones on both synthetic and real-world benchmark datasets. ",
    "chosen": "This paper develops an end-to-end multi-scale Transformer for image deraining, incorporating intra-scale implicit neural representations and inter-scale bidirectional feedback. The method, named NeRD-Rain, explores multi-scale representations and demonstrates favorable performance against state-of-the-art approaches on various benchmark datasets.",
    "rejected": "This paper proposes an efficient data-driven solution for self-localization within a floorplan, which does not require retraining per map or location and avoids needing a large image database of the area. The method introduces a novel probabilistic model with an observation module (comprising single and multiview components) and a temporal filtering module. It operates using a ray-based representation internally and is designed to work on conventional consumer hardware, overcoming common limitations such as the need for upright images. The system meets real-time requirements and outperforms state-of-the-art methods significantly."
  },
  {
    "prompt": "summary: An effective pre-training framework with universal 3D representations is extremely desired in perceiving large scale dynamic scenes. However, establishing such an ideal framework that is both task-generic and label-efficient poses a challenge in unifying the representation of the same primitive across diverse scenes. The current con trastive 3D pre-training methods typically follow a frame level consistency, which focuses on the 2D-3D relationships in each detached image. Such inconsiderate consistency greatly hampers the promising path of reaching an uni versal pre-training framework: (1) The cross-scene seman tic self-conflict, i.e., the intense collision between primitive segments of the same semantics from different scenes; (2) Lacking a globally unified bond that pushes the cross-scene semantic consistency into 3D representation learning. To This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 19925",
    "chosen": "This paper addresses the challenge of establishing a universal pre-training framework for large-scale dynamic scene perception. The authors propose a method to unify cross-scene semantic consistency in 3D representation learning, tackling issues like cross-scene semantic self-conflict and the lack of a globally unified bond in current contrastive 3D pre-training methods.",
    "rejected": "This research explores Handwritten Mathematical Expression Generation (HMEG) from symbolic sequences, framing it as a graph-to-image (G2I) generation problem. To address the lack of real masks for supervision, the authors propose an end-to-end G2I pipeline consisting of graph to layout, layout to mask, and mask to image stages. They introduce a Less-is-More (LiM) learning strategy to enhance prediction accuracy and a differentiable layout refinement module to clarify ambiguous areas. Their model, optimized in an end-to-end manner, generates high-quality HME images, surpassing previous generative methods. Additionally, the generated images improve HME recognition models through data augmentation."
  },
  {
    "prompt": "summary: The rebroadcasting of screen-recaptured document im ages introduces a signiﬁcant risk to the conﬁdential docu ments processed in government departments and commer cial companies. However, detecting recaptured document images subjected to distortions from online social networks (OSNs) is challenging since the common forensics cues, such as moir´e pattern, are weakened during transmission. In this work, we ﬁrst devise a pixel-level distortion model of the screen-recaptured document image to identify the robust features of color artifacts. Then, we extract a chromaticity map from the recaptured image to highlight the presence of color artifacts even under low-quality samples. Based on the prior understanding, we design a chromaticity map adapter (CMA) to efﬁciently extract the chromaticity map, and feed it into the transformer backbone as multi-modal prompt tokens. To evaluate the performance of the pro posed method, we collect a recaptured ofﬁce document im age dataset with over 10K diverse samples. Experimental results demonstrate that the proposed CMA method outper forms a SOTA approach (with RGB modality only), reducing the average EER from 26.82% to 16.78%. Robustness eval uation shows that our method achieves 0.8688 and 0.7554 AUCs under samples with JPEG compression (QF=70) and resolution as low as 534×503 pixels.",
    "chosen": "This paper presents a method to detect screen-recaptured document images using a chromaticity map adapter (CMA). The CMA extracts chromaticity maps highlighting color artifacts, which are fed into a transformer backbone. Evaluated on a dataset with over 10K samples, the method reduces average EER from 26.82% to 16.78% and shows robustness under JPEG compression and low resolution.",
    "rejected": "This paper introduces a method for detecting screen-recaptured document images by analyzing color artifacts using a chromaticity map adapter (CMA) and transformer backbone. The proposed approach improves upon existing state-of-the-art methods, reducing the average Equal Error Rate (EER) from 26.82% to 16.78%, even under JPEG compression and low-resolution conditions."
  },
  {
    "prompt": "summary: Passive depth estimation based on stereo or defocus re lies on the presence of the texture on an object to resolve its depth. Hence, recovering the depth of a textureless object— for example, a large white wall—is not just hard but perhaps even impossible. Or is it? We show that spatial coherence, a property of natural light sources, can be used to resolve the depth of a scene point even when it is textureless. Our approach relies on the idea that natural light scattered off a scene point is locally coherent with itself, while incoher ent with the light scattered from other surface points; we use this insight to design an optical setup that uses self interference as a texture feature for estimating depth. Our lab prototype is capable of resolving depths of textureless objects in sunlight as well as indoor lights.",
    "chosen": "This paper introduces a novel approach for passive 3D reconstruction of textureless objects using spatial coherence of natural light. The method leverages self-interference as a texture feature, enabling depth estimation of textureless surfaces. A lab prototype demonstrates successful depth resolution of textureless objects in various lighting conditions.",
    "rejected": "The research presents an innovative technique for depth estimation of textureless objects through spatial coherence in natural light sources. By utilizing self-interference as a texture feature, the optical setup can estimate depths accurately under both sunlight and indoor lighting conditions, overcoming traditional limitations of passive depth estimation methods."
  },
  {
    "prompt": "summary: This paper proposes ConsistDreamer – a novel frame work that lifts 2D diffusion models with 3D awareness and 3D consistency, thus enabling high-fidelity instruction guided scene editing. To overcome the fundamental limi tation of missing 3D consistency in 2D diffusion models, our key insight is to introduce three synergistic strategies that augment the input of the 2D diffusion model to be come 3D-aware and to explicitly enforce 3D consistency during the training process. Specifically, we design sur rounding views as context-rich input for the 2D diffusion model, and generate 3D-consistent structured noise instead of image-independent noise. Moreover, we introduce self supervised consistency-enforcing training within the per scene editing procedure. Extensive evaluation shows that our ConsistDreamer achieves state-of-the-art performance for instruction-guided scene editing across various scenes and editing instructions, particularly in complicated large scale indoor scenes from ScanNet++, with significantly im proved sharpness and fine-grained textures. Notably, Con sistDreamer stands as the first work capable of success fully editing complex (e.g., plaid/checkered) patterns. Our",
    "chosen": "This paper proposes ConsistDreamer, a framework enhancing 2D diffusion models with 3D awareness for high-fidelity scene editing. The approach introduces three strategies: using surrounding views as context, generating 3D-consistent noise, and self-supervised consistency training. ConsistDreamer achieves state-of-the-art performance in instruction-guided scene editing, particularly in complex indoor scenes.",
    "rejected": "ConsistDreamer is introduced as a novel framework that enhances 2D diffusion models with 3D awareness and consistency, enabling high-fidelity instruction-guided scene editing. It achieves state-of-the-art performance across various scenes, particularly improving sharpness and fine-grained textures in large-scale indoor scenes."
  },
  {
    "prompt": "summary: In this paper, we democratise caricature generation, em powering individuals to effortlessly craft personalised car icatures with just a photo and a conceptual sketch. Our objective is to strike a delicate balance between abstraction and identity, while preserving the creativity and subjectiv ity inherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing alongside single-image personalisa tion, selectively applying nuanced edits to cross-attention layers for a seamless merge of identity and style. Addition ally, we propose Random Mask Reconstruction to enhance robustness, directing the model to focus on distinctive iden tity and style features. Crucially, our aim is not to replace artists but to eliminate accessibility barriers, allowing en thusiasts to engage in the artistry.",
    "chosen": "This paper presents a method for generating personalized caricatures from a photo and a conceptual sketch. The approach combines Explicit Rank-1 Model Editing and single-image personalization, focusing on cross-attention layers. Random Mask Reconstruction is used to enhance robustness, ensuring a balance between identity and style.",
    "rejected": "A new method for generating personalized caricatures from photos and conceptual sketches is proposed, balancing abstraction and identity while preserving artistic creativity. Techniques like Explicit Rank-1 Model Editing and Random Mask Reconstruction are employed to enhance robustness and ensure seamless integration of identity and style features."
  },
  {
    "prompt": "summary: We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture genera tion with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting based sampling strategy for arbitrary long sequence gen eration in diffusion models, offering flexibility and compu tational efficiency. Our method provides a practical so lution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art per formance both quantitatively and qualitatively. Addition ally, a user study confirms the superiority of DiffSHEG over prior approaches. By enabling the real-time generation of expressive and synchronized motions, DiffSHEG showcases its potential for various applications in the development of digital humans and embodied agents.",
    "chosen": "This paper proposes DiffSHEG, a diffusion-based approach for speech-driven holistic 3D expression and gesture generation. The method enables uni-directional information flow from expression to gesture and introduces an outpainting-based sampling strategy for generating sequences of arbitrary length. Evaluated on public datasets, DiffSHEG achieves state-of-the-art performance.",
    "rejected": "DiffSHEG is a diffusion-based model designed for speech-driven holistic 3D expression and gesture generation, capable of producing synchronized expressions and gestures from arbitrary length audio inputs. Evaluation on public datasets shows superior qualitative and quantitative performance, offering potential applications in digital human development and embodied agents."
  },
  {
    "prompt": "summary: We present a method for large-mask pluralistic image in painting based on the generative framework of discrete la tent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible lo cations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis net work that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public bench marks validate our design choices as the proposed method outperforms strong baselines in both visual quality and di versity metrics.",
    "chosen": "This paper presents a method for large-mask pluralistic image inpainting using discrete latent codes. The approach includes a restrictive partial encoder, a bidirectional transformer, and a synthesis network. Experiments show the method outperforms baselines in visual quality and diversity metrics.",
    "rejected": "This paper describes a pluralistic image inpainting method based on discrete latent codes, which generates coherent and diverse complete images even under extreme mask settings. The approach outperforms strong baselines in visual quality and diversity metrics, demonstrating its effectiveness in handling challenging inpainting tasks."
  },
  {
    "prompt": "summary: We present DRESS , a large vision language model (LVLM) that innovatively exploits Natural Language feed back (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs gener ally rely only on the instruction ﬁnetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This re duces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and reﬁnement. The critique NLF identiﬁes the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The reﬁnement NLF offers concrete suggestions for improve ment and is adopted to improve the interaction ability of the LVLMs– which focuses on LVLMs’ ability to reﬁne responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experi mental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVLMs.",
    "chosen": "This paper introduces DRESS, a large vision-language model that uses natural language feedback (NLF) for enhanced alignment and interaction. The model categorizes NLF into critique and refinement types, improving response quality and multi-turn interaction ability. Experimental results demonstrate DRESS's superiority over existing LVLMs.",
    "rejected": "DRESS, a large vision language model, leverages Natural Language Feedback (NLF) to improve alignment and multi-turn interaction capabilities. It categorizes NLF into critique and refinement types to better align responses with human preferences and improve interaction abilities. Experimental results show significant improvements in helpfulness, honesty, and harmlessness compared to state-of-the-art LVLMs."
  },
  {
    "prompt": "summary: Self-supervised denoising has attracted widespread at tention due to its ability to train without clean images. How ever, noise in real-world scenarios is often spatially cor related, which causes many self-supervised algorithms that assume pixel-wise independent noise to perform poorly. Re cent works have attempted to break noise correlation with downsampling or neighborhood masking. However, denois ing on downsampled subgraphs can lead to aliasing effects and loss of details due to a lower sampling rate. Further more, the neighborhood masking methods either come with high computational complexity or do not consider local spatial preservation during inference. Through the analy sis of existing methods, we point out that the key to obtain ing high-quality and texture-rich results in real-world self supervised denoising tasks is to train at the original input resolution structure and use asymmetric operations during training and inference. Based on this, we propose Asymmet ric Tunable Blind-Spot Network (AT-BSN), where the blind spot size can be freely adjusted, thus better balancing noise correlation suppression and image local spatial destruction during training and inference. In addition, we regard the pre-trained AT-BSN as a meta-teacher network capable of generating various teacher networks by sampling different blind-spots. We propose a blind-spot based multi-teacher distillation strategy to distill a lightweight network, signif icantly improving performance. Experimental results on multiple datasets prove that our method achieves state-of the-art, and is superior to other self-supervised algorithms in terms of computational overhead and visual effects.",
    "chosen": "This paper proposes an asymmetric tunable blind-spot network (AT-BSN) for self-supervised denoising in real-world scenarios. The method balances noise correlation suppression and image local spatial preservation, using asymmetric operations during training and inference. The approach achieves state-of-the-art performance with lower computational overhead.",
    "rejected": "AT-BSN is introduced as a solution for real-world self-supervised denoising, addressing the challenge of spatially correlated noise. This asymmetric tunable blind-spot network balances noise correlation suppression and local spatial preservation, achieving state-of-the-art performance in terms of computational overhead and visual effects."
  },
  {
    "prompt": "summary: In this paper we propose an efﬁcient data-driven solu tion to self-localization within a ﬂoorplan. Floorplan data is readily available, long-term persistent and inherently ro bust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal ﬁltering module. Operating internally with an efﬁcient ray-based representation, the observation module consists of a single and a multiview module to pre dict horizontal depth from images and fuses their results to beneﬁt from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing meth ods [16, 17, 20, 28] that often demand upright images. Our full system meets real-time requirements, while outperform ing the state-of-the-art [20, 28] by a signiﬁcant margin.",
    "chosen": "This paper presents F3Loc, a method for self-localization within a floorplan using a probabilistic model with observation and temporal filtering modules. The method operates on consumer hardware and meets real-time requirements, outperforming existing approaches.",
    "rejected": "An efficient data-driven solution for camera localization within floorplans is proposed, utilizing a probabilistic model with observation and temporal filtering modules. The method operates on conventional consumer hardware and meets real-time requirements, significantly outperforming current state-of-the-art methods."
  },
  {
    "prompt": "summary: In this paper, we explore a novel challenging genera tion task, i.e. Handwritten Mathematical Expression Gen eration (HMEG) from symbolic sequences. Since symbolic sequences are naturally graph-structured data, we formu late HMEG as a graph-to-image (G2I) generation problem. Unlike the generation of natural images, HMEG requires critic layout clarity for synthesizing correct and recogniz able formulas, but has no real masks available to super vise the learning process. To alleviate this challenge, we propose a novel end-to-end G2I generation pipeline (i.e. graph →layout →mask →image), which requires no real masks or nondifferentiable alignment between layouts and masks. Technically, to boost the capacity of predicting detailed relations among adjacent symbols, we propose a Less-is-More (LiM) learning strategy. In addition, we de sign a differentiable layout refinement module, which maps bounding boxes to pixel-level soft masks, so as to further al leviate ambiguous layout areas. Our whole model, includ ing layout prediction, mask refinement, and image genera tion, can be jointly optimized in an end-to-end manner. Ex perimental results show that, our model can generate high quality HME images, and outperforms previous generative methods. Besides, a series of ablations study demonstrate effectiveness of the proposed techniques. Finally, we vali date that our generated images promisingly boosts the per formance of HME recognition models, through data aug mentation. ",
    "chosen": "This paper explores generating handwritten mathematical expressions from symbolic sequences, formulating it as a graph-to-image generation problem. The proposed end-to-end pipeline includes layout prediction, mask refinement, and image generation. Experimental results show the model generates high-quality images and boosts recognition performance through data augmentation.",
    "rejected": "The paper explores Handwritten Mathematical Expression Generation (HMEG) as a graph-to-image generation problem, proposing an end-to-end pipeline with layout prediction, mask refinement, and image generation stages. The Less-is-More (LiM) learning strategy and differentiable layout refinement module improve detailed relation prediction among symbols, resulting in high-quality HME images."
  },
  {
    "prompt": "summary: In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable capabilities in representing 3D scenes. To expedite the rendering process, learnable explicit representations have been introduced for combination with implicit NeRF representation, which however results in a large storage space requirement. In this paper, we introduce the Context-based NeRF Compression (CNC) framework, which leverages highly efﬁcient context models to provide a storage-friendly NeRF representation. Speciﬁcally, we excavate both level-wise and dimension-wise context dependencies to enable probability prediction for information entropy reduction. Additionally, we exploit hash collision and occupancy grids as strong prior knowledge for better context modeling. To the best of our knowledge, we are the ﬁrst to construct and exploit context models for NeRF compression. We achieve a size reduction of 100⇥and 70⇥with improved ﬁdelity against the baseline Instant-NGP on Synthesic-NeRF and Tanks and Temples datasets, respectively. Additionally, we attain 86.7% and 82.3% storage size reduction against the SOTA NeRF compression method BiRF. Our",
    "chosen": "This paper presents CNC, a framework for compressing NeRF representations using context models to reduce storage requirements. The approach leverages level-wise and dimension-wise context dependencies and utilizes hash collisions and occupancy grids for better context modeling. Experiments show significant size reductions and improved fidelity compared to baseline methods.",
    "rejected": "In recent years, Neural Radiance Field (NeRF) has shown remarkable 3D scene representation abilities but faced challenges with storage space due to explicit representations. This paper presents Context-based NeRF Compression (CNC), which utilizes context models for efficient compression while improving fidelity. CNC achieves significant size reduction compared to Instant-NGP and BiRF methods."
  },
  {
    "prompt": "summary: Diffusion models have shown an impressive ability to model complex data distributions, with several key advan tages over GANs, such as stable training, better coverage of the training distribution’s modes, and the ability to solve inverse problems without extra training. However, most dif fusion models learn the distribution of fixed-resolution im ages. We propose to learn the distribution of continuous images by training diffusion models on image neural fields, which can be rendered at any resolution, and show its ad vantages over fixed-resolution models. To achieve this, a key challenge is to obtain a latent space that represents pho torealistic image neural fields. We propose a simple and effective method, inspired by several recent techniques but with key changes to make the image neural fields photo realistic. Our method can be used to convert existing la tent diffusion autoencoders into image neural field autoen coders. We show that image neural field diffusion mod els can be trained using mixed-resolution image datasets, outperform fixed-resolution diffusion models followed by super-resolution models, and can solve inverse problems with conditions applied at different scales efficiently.",
    "chosen": "This paper introduces Image Neural Field Diffusion models (INFD), which learn the distribution of continuous images by training diffusion models on image neural fields. The method converts existing latent diffusion autoencoders into image neural field autoencoders and demonstrates advantages over fixed-resolution models, including better performance on mixed-resolution datasets and efficient solving of inverse problems.",
    "rejected": "Diffusion models have demonstrated advantages over GANs in modeling complex data distributions. This work proposes Image Neural Field Diffusion models (INFD), capable of rendering images at any resolution and solving inverse problems efficiently without extra training. INFD outperforms fixed-resolution diffusion models followed by super-resolution models."
  },
  {
    "prompt": "summary: The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun dation model (InternVL), which scales up the vision foun dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the develop ment of multi-modal large models. † This work is done when they are interns at Shanghai AI Laboratory; B corresponding author (daijifeng@tsinghua.edu.cn)",
    "chosen": "This paper presents InternVL, a large-scale vision-language foundation model with 6 billion parameters. The model is progressively aligned with large language models (LLMs) using web-scale image-text data and achieves state-of-the-art performance on 32 visual-linguistic benchmarks. It offers powerful visual capabilities and can serve as an alternative to ViT-22B.",
    "rejected": "This paper introduces a large-scale vision-language foundation model named InternVL, aligning with large language models using web-scale image-text data. It performs well on various visual-linguistic benchmarks, offering a promising alternative to ViT-22B. The model bridges the gap between vision models and language models."
  },
  {
    "prompt": "summary: Speech-preserving facial expression manipulation (SPFEM) aims to modify facial emotions while meticu lously maintaining the mouth animation associated with spoken content. Current works depend on inaccessible paired training samples for the person, where two aligned frames exhibit the same speech content yet differ in emo tional expression, limiting the SPFEM applications in real-world scenarios. In this work, we discover that speak ers who convey the same content with different emotions exhibit highly correlated local facial animations, providing valuable supervision for SPFEM. To capitalize on this insight, we propose a novel adaptive spatial coherent correlation learning (ASCCL) algorithm, which models the aforementioned correlation as an explicit metric and integrates the metric to supervise manipulating facial expression and meanwhile better preserving the facial animation of spoken contents. To this end, it first learns a spatial coherent correlation metric, ensuring the visual disparities of adjacent local regions of the image belonging to one emotion are similar to those of the corresponding counterpart of the image belonging to another emotion. Recognizing that visual disparities are not uniform across all regions, we have also crafted a disparity-aware adaptive strategy that prioritizes regions that present greater challenges. During SPFEM model training, we construct the adaptive spatial coherent correlation metric between corresponding local regions of the input and output images as addition loss to supervise the generation *Zhijing Yang is the corresponding author. Tianshui Chen, Jian man Lin, and Zhijing Yang are with Guangdong University of Tech nology. Chunmei Qing is with South China University of Technol ogy. Liang Lin is with Sun Yat-Sen University. This work was supported in part by National Natural Science Foundation of China (NSFC) under Grant No. 62206060, in Part by Natural Science Foun dation of Guangdong Province (2022A1515011555, 2023A1515012568, 2023A1515012561), Guangdong Provincial Key Laboratory of Human Digital Twin (2022B1212010004), and in part by Guangzhou Basic and Applied Basic Research Foundation under Grant No. SL2022A04J01626. Reference Source NED ASCCL Figure 1. Several examples are generated by the current advanced NED with and without the proposed ASCCL algorithm. Incor porating the ASCCL can better manipulate the expressions and meanwhile preserve mouth shapes. process. We conduct extensive experiments on variant datasets, and the results demonstrate the effectiveness of the proposed ASCCL algorithm.",
    "chosen": "This paper proposes a novel adaptive spatial coherent correlation learning (ASCCL) algorithm for speech-preserving facial expression manipulation (SPFEM). The algorithm models the correlation of local facial animations and integrates it as a metric to supervise the manipulation process, ensuring better preservation of mouth animations associated with spoken content.",
    "rejected": "Speech-preserving facial expression manipulation aims to modify facial emotions while preserving mouth animations. This paper proposes an adaptive spatial coherent correlation learning (ASCCL) algorithm that improves expression manipulation while better preserving speech-related facial animations. Extensive experiments validate the effectiveness of ASCCL."
  },
  {
    "prompt": "summary: Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability to perceive and under stand multi-modal signals. However, most of the exist ing MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text pairs, leading to insufficient ex traction and reasoning of visual knowledge. To address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model (LION), which empow ers the MLLM by injecting visual knowledge in two lev els. 1) Progressive incorporation of fine-grained spatial aware visual knowledge. We design a vision aggregator cooperated with region-level vision-language (VL) tasks to incorporate fine-grained spatial-aware visual knowledge into the MLLM. To alleviate the conflict between image level and region-level VL tasks during incorporation, we devise a dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This progressive incorporation scheme contributes to the mutual promotion between these two kinds of VL tasks. 2) Soft prompting of high-level se mantic visual evidence. We facilitate the MLLM with high level semantic visual evidence by leveraging diverse image tags. To mitigate the potential influence caused by imper fect predicted tags, we propose a soft prompting method by embedding a learnable token into the tailored text instruc tion. Comprehensive experiments on several multi-modal benchmarks demonstrate the superiority of our model (e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over InstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).",
    "chosen": "This paper introduces LION, a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model. LION incorporates fine-grained spatial-aware visual knowledge and soft prompts of high-level semantic visual evidence into the multimodal large language model (MLLM), improving performance on various multi-modal benchmarks.",
    "rejected": "Multimodal Large Language Models (MLLMs) face challenges in extracting and reasoning visual knowledge. This work presents LION, enhancing MLLMs by injecting visual knowledge at two levels: progressive incorporation of fine-grained spatial-aware knowledge and soft prompting of high-level semantic visual evidence. LION shows superior performance on several multi-modal benchmarks."
  },
  {
    "prompt": "summary: Recent progress in Large Multimodal Models (LMM) has opened up great possibilities for various applications in the ﬁeld of human-machine interactions. However, develop ing LMMs that can comprehend, reason, and plan in com plex and diverse 3D environments remains a challenging topic, especially considering the demand for understanding permutation-invariant point cloud representations of the 3D scene. Existing works seek help from multi-view images by projecting 2D features to 3D space, which inevitably leads to huge computational overhead and performance degrada tion. In this paper, we present LL3DA, a Large Language 3D Assistant that takes point cloud as the direct input and responds to both text instructions and visual interactions. The additional visual interaction enables LMMs to better comprehend human interactions with the 3D environment and further remove the ambiguities within plain texts. Ex periments show that LL3DA achieves remarkable results and surpasses various 3D vision-language models on both 3D Dense Captioning and 3D Question Answering.",
    "chosen": "This paper presents LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and responds to both text instructions and visual interactions. The model enables better comprehension of human interactions with 3D environments and removes ambiguities in plain texts, achieving remarkable results on 3D Dense Captioning and 3D Question Answering.",
    "rejected": "Large Multimodal Models (LMM) struggle with understanding permutation-invariant point cloud representations of 3D scenes. LL3DA is introduced as a solution, taking point clouds directly as input and responding to text instructions and visual interactions. Experiments demonstrate LL3DA's superior performance in 3D Dense Captioning and 3D Question Answering tasks."
  },
  {
    "prompt": "summary: For image super-resolution (SR), bridging the gap be tween the performance on synthetic datasets and real-world degradation scenarios remains a challenge. This work in troduces a novel ”Low-Res Leads the Way” (LWay) train ing framework, merging Supervised Pre-training with Self supervised Learning to enhance the adaptability of SR mod els to real-world images. Our approach utilizes a low resolution (LR) reconstruction network to extract degrada tion embeddings from LR images, merging them with super resolved outputs for LR reconstruction. Leveraging unseen LR images for self-supervised learning guides the model to adapt its modeling space to the target domain, facili tating fine-tuning of SR models without requiring paired high-resolution (HR) images. The integration of Discrete Wavelet Transform (DWT) further refines the focus on high frequency details. Extensive evaluations show that our method significantly improves the generalization and de tail restoration capabilities of SR models on unseen real world datasets, outperforming existing methods. Our train ing regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications.",
    "chosen": "This paper introduces a novel \"Low-Res Leads the Way\" (LWay) training framework for image super-resolution (SR). The framework combines Supervised Pre-training with Self-supervised Learning, utilizing a low-resolution reconstruction network to extract degradation embeddings and guide model adaptation to real-world images without requiring paired high-resolution images.",
    "rejected": "Image super-resolution (SR) faces challenges bridging synthetic datasets' performance and real-world degradation scenarios. A novel \"Low-Res Leads the Way\" (LWay) training framework merges supervised pre-training with self-supervised learning, significantly improving SR models' generalization and detail restoration capabilities on unseen real-world datasets."
  },
  {
    "prompt": "summary: Pose regression networks predict the camera pose of a query image relative to a known environment. Within this family of methods, absolute pose regression (APR) has re cently shown promising accuracy in the range of a few cen timeters in position error. APR networks encode the scene geometry implicitly in their weights. To achieve high ac curacy, they require vast amounts of training data that, re alistically, can only be created using novel view synthesis in a days-long process. This process has to be repeated for each new scene again and again. We present a new approach to pose regression, map-relative pose regression (marepo), that satisfies the data hunger of the pose re gression network in a scene-agnostic fashion. We condi tion the pose regressor on a scene-specific map representa tion such that its pose predictions are relative to the scene map. This allows us to train the pose regressor across hundreds of scenes to learn the generic relation between a scene-specific map representation and the camera pose. Our map-relative pose regressor can be applied to new map representations immediately or after mere minutes of fine-tuning for the highest accuracy. Our approach out performs previous pose regression methods by far on two public datasets, indoor and outdoor.",
    "chosen": "This paper presents map-relative pose regression (marepo), a new approach to pose regression for visual re-localization. The method conditions the pose regressor on a scene-specific map representation, allowing training across hundreds of scenes and immediate application to new map representations. The approach outperforms previous methods on public datasets.",
    "rejected": "Pose regression networks predict camera poses from query images but require vast amounts of training data. This paper introduces map-relative pose regression (marepo), conditioning pose regressors on scene-specific maps, enabling training across multiple scenes. Marepo outperforms previous methods on public indoor and outdoor datasets."
  },
  {
    "prompt": "summary: We introduce Mind Artist (MindArt), a novel and efficient neural decoding architecture to snap artistic photographs from our mind in a controllable manner. Recently, progress has been made in image reconstruction with non-invasive brain recordings, but it’s still difficult to generate realis tic images with high semantic fidelity due to the scarcity of data annotations. Unlike previous methods, this work casts the neural decoding into optimal transport (OT) and rep resentation decoupling problems. Specifically, under dis crete OT theory, we design a graph matching-guided neu ral representation learning framework to seek the under lying correspondences between conceptual semantics and neural signals, which yields a natural and meaningful self supervisory task. Moreover, the proposed MindArt, struc tured with multiple stand-alone modal branches, enables the seamless incorporation of semantic representation into any visual style information, thus leaving it to have multi modal reconstruction and training-free semantic editing ca †Corresponding author: Yu Qi. pabilities. By doing so, the reconstructed images of Min dArt have phenomenal realism both in terms of semantics and appearance. We compare our MindArt with leading al ternatives, and achieve SOTA performance in different de coding tasks. Importantly, our approach can directly gen erate a series of stylized “mind snapshots” w/o extra opti mizations, which may open up more potential applications.",
    "chosen": "This paper introduces Mind Artist (MindArt), a neural decoding architecture for generating artistic photographs from human thoughts. The method casts neural decoding into optimal transport and representation decoupling problems, enabling multimodal reconstruction and training-free semantic editing capabilities. MindArt achieves state-of-the-art performance in different decoding tasks.",
    "rejected": "Mind Artist (MindArt) proposes a novel neural decoding architecture for generating artistic photographs from brain recordings. Utilizing discrete optimal transport theory, MindArt designs a graph matching-guided framework, achieving phenomenal realism in reconstructed images. It supports multimodal reconstruction and training-free semantic editing capabilities."
  },
  {
    "prompt": "summary: Learning-based stereo matching techniques have made significant progress. However, existing methods inevitably lose geometrical structure information during the feature channel generation process, resulting in edge detail mis matches. In this paper, the Motif Channel Attention Stereo Matching Network (MoCha-Stereo) is designed to address this problem. We provide the Motif Channel Correlation Volume (MCCV) to determine more accurate edge match ing costs. MCCV is achieved by projecting motif chan nels, which capture common geometric structures in fea ture channels, onto feature maps and cost volumes. In ad dition, edge variations in the reconstruction error map also affect details matching, we propose the Reconstruction Er ror Motif Penalty (REMP) module to further refine the full resolution disparity estimation. REMP integrates the fre quency information of typical channel features from the re construction error. MoCha-Stereo ranks 1st on the KITTI 2015 and KITTI-2012 Reflective leaderboards. Our struc ture also shows excellent performance in Multi-View Stereo. *Co-first author. †Corresponding author.",
    "chosen": "This paper proposes MoCha-Stereo, a motif channel attention stereo matching network. The approach introduces the Motif Channel Correlation Volume (MCCV) and Reconstruction Error Motif Penalty (REMP) module to improve edge matching and full-resolution disparity estimation. MoCha-Stereo ranks 1st on KITTI-2015 and KITTI-2012 Reflective leaderboards and shows excellent performance in Multi-View Stereo.",
    "rejected": "For image super-resolution, this study develops a new training framework combining supervised learning on synthetic data with self-supervised learning on real test data, termed \"Low-Res Leads the Way\". This approach enhances adaptability and fine-tuning of SR models without paired HR images, showing improved generalization on real-world datasets."
  },
  {
    "prompt": "summary: Recent advances in generative diffusion models have en abled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photo realistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multi-view consistent diffusion approach. We demonstrate that accu rate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More im portantly, this integration facilitates a seamless and accu rate incorporation of facial expression and body pose con trol into the generation process. To the best of our knowl edge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an un seen subject; extensive quantitative and qualitative evalua tions demonstrate the advantages of our approach over ex isting state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks. The code for our project is publicly available.",
    "chosen": "This paper enhances the quality and functionality of generative diffusion models for creating controllable, photorealistic human avatars from a single image. By integrating a 3D morphable model into a multi-view-consistent diffusion approach, the framework improves novel view synthesis and enables accurate facial expression and body pose control. Extensive evaluations show advantages over existing avatar creation models.",
    "rejected": "Recent advances have enabled the generation of 3D human avatars from single images using generative diffusion models. This work integrates a 3D morphable model into a multi-view-consistent diffusion approach, enhancing photorealism and controllability of facial expressions and body poses. The proposed method creates fully 3D-consistent, animatable avatars with superior performance compared to existing avatar creation models."
  },
  {
    "prompt": "summary: Neural Radiance Field (NeRF) has been widely recog nized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is in herently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shad ows. In this work, we propose a novel paradigm, namely “Heuristics-Guided Segmentation” (HuGS), which signifi cantly enhances the separation of static scenes from tran sient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the metic ulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient dis tractors for NeRFs trained in non-static scenes.",
    "chosen": "This paper proposes NeRF-HuGS, a novel paradigm combining heuristics and segmentation models to improve NeRF's performance in non-static scenes. The method fuses Structure-from-Motion (SfM)-based heuristics and color residual heuristics, effectively separating static scenes from transient distractors. Experiments demonstrate the method's superiority in mitigating artifacts caused by moving objects or shadows.",
    "rejected": "NeRF has shown excellence in novel view synthesis but struggles with transient distractors. A new paradigm named Heuristics-Guided Segmentation (HuGS) combines hand-crafted heuristics and state-of-the-art segmentation models to improve separation between static scenes and transient distractors, significantly mitigating artifacts in NeRFs trained in non-static scenes."
  },
  {
    "prompt": "summary: Absolute Pose Regression (APR) methods use deep neu ral networks to directly regress camera poses from RGB images. However, the predominant APR architectures only rely on 2D operations during inference, resulting in limited accuracy of pose estimation due to the lack of 3D geometry constraints or priors. In this work, we propose a test-time refinement pipeline that leverages implicit geometric con straints using a robust feature field to enhance the ability of APR methods to use 3D information during inference. We also introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D geometric features during train ing and directly renders dense novel view features at test time to refine APR methods. To enhance the robustness of our model, we introduce a feature fusion module and a pro gressive training strategy. Our proposed method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets. ",
    "chosen": "This paper presents a test-time refinement pipeline for Absolute Pose Regression (APR) methods, enhancing their ability to use 3D information during inference. The proposed Neural Feature Synthesizer (NeFeS) model encodes 3D geometric features during training and renders dense novel view features at test time. The method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets.",
    "rejected": "APR methods for camera pose estimation face limitations due to lack of 3D geometry constraints during inference. This work proposes a test-time refinement pipeline that leverages implicit geometric constraints through a robust feature field, introducing a Neural Feature Synthesizer (NeFeS) to encode 3D features and refine APR accuracy."
  },
  {
    "prompt": "summary: We explore the boundaries of scaling up a multilingual vision and language model, both in terms of size of the com ponents and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based doc ument understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. Our model advances the state-of-the-art on most vision-and-language benchmarks considered (20+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.",
    "chosen": "This paper explores scaling up a multilingual vision and language model, achieving new performance levels on various complex tasks. The model advances the state-of-the-art on most vision-and-language benchmarks and shows emerging capabilities like complex counting and multilingual object detection.",
    "rejected": "Scaling up multilingual vision and language models achieves improved performance across various tasks, including image captioning, question answering, and object detection. PaLI-X demonstrates enhanced capabilities like complex counting and multilingual object detection by scaling both visual and language components, resulting in balanced parameter allocation and state-of-the-art results on multiple benchmarks."
  },
  {
    "prompt": "summary: Due to the resource-intensive nature of training vision language models on expansive video data, a majority of studies have centered on adapting pre-trained image language models to the video domain. Dominant pipelines propose to tackle the visual discrepancies with additional temporal learners while overlooking the substantial dis crepancy for web-scaled descriptive narratives and con cise action category names, leading to less distinct semantic space and potential performance limitations. In this work, we prioritize the refinement of text knowledge to facilitate generalizable video recognition. To address the limitations of the less distinct semantic space of category names, we prompt a large language model (LLM) to augment action class names into Spatio-Temporal Descriptors thus bridg ing the textual discrepancy and serving as a knowledge base for general recognition. Moreover, to assign the best de scriptors with different video instances, we propose Optimal Descriptor Solver, forming the video recognition problem as solving the optimal matching flow across frame-level repre sentations and descriptors. Comprehensive evaluations in zero-shot, few-shot, and fully supervised video recognition highlight the effectiveness of our approach. Our best model achieves a state-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.",
    "chosen": "This paper proposes refining text knowledge with optimal spatio-temporal descriptors for generalizable video recognition. The method prompts a large language model to augment action class names and proposes an Optimal Descriptor Solver. Evaluations show effectiveness in zero-shot, few-shot, and fully supervised video recognition.",
    "rejected": "To address discrepancies in adapting pre-trained image-language models to video recognition, this paper proposes augmenting action class names into Spatio-Temporal Descriptors using a large language model (LLM). An Optimal Descriptor Solver is introduced to match frame-level representations with descriptors, leading to significant improvements in zero-shot, few-shot, and fully supervised video recognition."
  },
  {
    "prompt": "summary: Recently, some large kernel convnets strike back with appealing performance and efﬁciency. However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem. Due to these issues, current CNNs compromise to scale up to 51 × 51 in the form of stripe convolution (i.e., 51 × 5 + 5 × 51) and start to saturate as the ker nel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human vision, we propose a human-like periph eral convolution that efﬁciently reduces over 90% parame ter count of dense grid convolution through parameter shar ing, and manage to scale up kernel size to extremely large. Our peripheral convolution behaves highly similar to hu man, reducing the complexity of convolution from O(K2) to O(log K) without backﬁring performance. Built on this, we propose Parameter-efﬁcient Large Kernel Network (PeLK). Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classiﬁca tion, semantic segmentation on ADE20K and object detec tion on MS COCO. For the ﬁrst time, we successfully scale up the kernel size of CNNs to an unprecedented 101 × 101 and demonstrate consistent improvements.",
    "chosen": "This paper introduces Parameter-efficient Large Kernel Networks (PeLK) with peripheral convolution, inspired by human vision. The network efficiently reduces parameters, scales up kernel size, and outperforms modern vision Transformers and ConvNet architectures on various vision tasks.",
    "rejected": "Large kernel convnets can achieve high performance but suffer from optimization issues as kernel sizes increase. Inspired by human vision, a peripheral convolution method reduces parameter counts by over 90% while scaling up kernel size to extremely large dimensions. The Parameter-efficient Large Kernel Network (PeLK) outperforms modern vision transformers and ConvNet architectures on various vision tasks, successfully scaling kernels to an unprecedented 101x101 size."
  },
  {
    "prompt": "summary: Domain Generalization (DG) aims to resolve distribu tion shifts between source and target domains, and cur rent DG methods are default to the setting that data from source and target domains share identical categories. Nev ertheless, there exists unseen classes from target domains in practical scenarios. To address this issue, Open Set Domain Generalization (OSDG) has emerged and several methods have been exclusively proposed. However, most ex isting methods adopt complex architectures with slight im provement compared with DG methods. Recently, vision language models (VLMs) have been introduced in DG fol lowing the fine-tuning paradigm, but consume huge train ing overhead with large vision models. Therefore, in this paper, we innovate to transfer knowledge from VLMs to lightweight vision models and improve the robustness by introducing Perturbation Distillation (PD) from three per spectives, including Score, Class and Instance (SCI), named SCI-PD. Moreover, previous methods are oriented by the benchmarks with identical and fixed splits, ignoring the di vergence between source domains. These methods are re vealed to suffer from sharp performance decay with our proposed new benchmark Hybrid Domain Generalization (HDG) and a novel metric H2-CV, which construct var ious splits to comprehensively assess the robustness of algorithms. Extensive experiments demonstrate that our method outperforms state-of-the-art algorithms on multi ple datasets, especially improving the robustness when con fronting data scarcity.",
    "chosen": "This paper addresses Open Set Domain Generalization (OSDG) by transferring knowledge from vision-language models to lightweight vision models and introducing Perturbation Distillation (PD) from three perspectives. The method improves robustness and outperforms state-of-the-art algorithms on multiple datasets.",
    "rejected": "Domain Generalization (DG) faces challenges when dealing with unseen classes in target domains. Open Set Domain Generalization (OSDG) addresses this issue, but existing methods often require complex architectures. This paper introduces Perturbation Distillation (PD) from three perspectives (Score, Class, Instance), named SCI-PD, which transfers knowledge from vision-language models to lightweight vision models, improving robustness and performance on multiple datasets, especially under data scarcity."
  },
  {
    "prompt": "summary: Weakly-supervised Video Anomaly Detection (wVAD) aims to detect frame-level anomalies using only video level labels in training. Due to the limitation of coarse grained labels, Multi-Instance Learning (MIL) is prevail ing in wVAD. However, MIL suffers from insufficiency of binary supervision to model diverse abnormal patterns. Be sides, the coupling between abnormality and its context hin ders the learning of clear abnormal event boundary. In this paper, we propose prompt-enhanced MIL to detect var ious abnormal events while ensuring clear event bound aries. Concretely, we design the abnormal-aware prompts by using abnormal class annotations together with learn able prompt, which can incorporate semantic priors into video features dynamically. The detector can utilize the semantic-rich features to capture diverse abnormal pat terns. In addition, normal context prompt is introduced to amplify the distinction between abnormality and its context, facilitating the generation of clear boundary. With the mu tual enhancement of abnormal-aware and normal context prompt, the model can construct discriminative represen tations to detect divergent anomalies without ambiguous event boundaries. Extensive experiments demonstrate our method achieves SOTA performance on three public bench marks. The",
    "chosen": "This paper proposes prompt-enhanced Multi-Instance Learning (MIL) for weakly supervised video anomaly detection. The method uses abnormal-aware prompts and normal context prompts to capture diverse abnormal patterns and generate clear event boundaries. Experiments show state-of-the-art performance on public benchmarks.",
    "rejected": "Weakly-supervised Video Anomaly Detection (wVAD) aims to detect anomalies using only video-level labels. This paper proposes prompt-enhanced MIL, incorporating abnormal-aware prompts and normal context prompts to generate clear event boundaries and capture diverse abnormal patterns. Extensive experiments show that this method achieves state-of-the-art performance on three public benchmarks."
  },
  {
    "prompt": "summary: Segment Anything Model (SAM) has emerged as a trans formative approach in image segmentation, acclaimed for its robust zero-shot segmentation capabilities and flexible prompting system. Nonetheless, its performance is chal lenged by images with degraded quality. Addressing this limitation, we propose the Robust Segment Anything Model (RobustSAM), which enhances SAM’s performance on low quality images while preserving its promptability and zero shot generalization. Our method leverages the pre-trained SAM model with only marginal parameter increments and computational requirements. The additional parameters of RobustSAM can be optimized within 30 hours on eight GPUs, demonstrating its feasibility and practicality for typ ical research laboratories. We also introduce the Robust Seg dataset, a collection of 688K image-mask pairs with different degradations designed to train and evaluate our model optimally. Extensive experiments across various seg mentation tasks and datasets confirm RobustSAM’s supe rior performance, especially under zero-shot conditions, underscoring its potential for extensive real-world applica tion. Additionally, our method has been shown to effectively improve the performance of SAM-based downstream tasks such as single image dehazing and deblurring.",
    "chosen": "This paper introduces RobustSAM, enhancing Segment Anything Model (SAM) for better performance on low-quality images. The method preserves SAM's promptability and zero-shot generalization with minimal parameter increments. Experiments confirm superior performance across various segmentation tasks.",
    "rejected": "The Segment Anything Model (SAM) excels in zero-shot segmentation but struggles with low-quality images. RobustSAM enhances SAM’s performance on such images with minimal additional parameters, preserving its promptability and zero-shot capabilities. It uses a pre-trained SAM model and can be optimized within 30 hours on eight GPUs. The RobustSeg dataset, comprising 688K image-mask pairs with various degradations, supports training and evaluation. Experiments confirm RobustSAM's superior performance, especially under zero-shot conditions, making it ideal for real-world applications like image dehazing and deblurring."
  },
  {
    "prompt": "summary: We propose SceneTex, a novel method for effectively gen erating high-quality and style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike pre vious methods that either iteratively warp 2D views onto a mesh surface or distillate diffusion latent features with out accurate geometric and style cues, SceneTex formulates the texture synthesis task as an optimization problem in the RGB space where style and geometry consistency are prop erly reflected. At its core, SceneTex proposes a multires olution texture field to implicitly encode the mesh appear ance. We optimize the target texture via a score-distillation based objective function in respective RGB renderings. To further secure the style consistency across views, we intro duce a cross-attention decoder to predict the RGB values by cross-attending to the pre-sampled reference locations in each instance. SceneTex enables various and accurate texture synthesis for 3D-FRONT scenes, demonstrating sig nificant improvements in visual quality and prompt fidelity over the prior texture generation methods.",
    "chosen": "This paper proposes SceneTex, a method for generating high-quality, style-consistent textures for indoor scenes using depth-to-image diffusion priors. The method formulates texture synthesis as an optimization problem and introduces a cross-attention decoder for style consistency. Results show significant improvements in visual quality and prompt fidelity.",
    "rejected": "SceneTex is a novel method for generating high-quality, style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike previous methods, SceneTex formulates texture synthesis as an RGB-space optimization problem, ensuring geometry and style consistency. It employs a multi-resolution texture field and score-distillation-based objective function for optimization, and a cross-attention decoder for style consistency across views. SceneTex shows significant improvements in visual quality and prompt fidelity over prior methods, enabling accurate texture synthesis for 3D-FRONT scenes."
  },
  {
    "prompt": "summary: Recent works on text-to-3d generation show that using only 2D diffusion supervision for 3D generation tends to produce results with inconsistent appearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals with extra legs). Existing methods mainly address this is sue by retraining diffusion models with images rendered from 3D data to ensure multi-view consistency while strug gling to balance 2D generation quality with 3D consistency. In this paper, we present a new framework Sculpt3D that equips the current pipeline with explicit injection of 3D priors from retrieved reference objects without re-training the 2D diffusion model. Specifically, we demonstrate that high-quality and diverse 3D geometry can be guaran teed by keypoints supervision through a sparse ray sam pling approach. Moreover, to ensure accurate appear ances of different views, we further modulate the output of the 2D diffusion model to the correct patterns of the tem plate views without altering the generated object’s style. These two decoupled designs effectively harness 3D in formation from reference objects to generate 3D objects while preserving the generation quality of the 2D diffu sion model. Extensive experiments show our method can largely improve the multi-view consistency while retaining fidelity and diversity. Our",
    "chosen": "This paper presents Sculpt3D, a framework for text-to-3D generation that improves multi-view consistency by injecting 3D priors from reference objects without retraining the 2D diffusion model. The method uses keypoints supervision and modulates the 2D diffusion model's output to ensure accurate appearances across different views. Experiments show significant improvements in multi-view consistency while maintaining fidelity and diversity.",
    "rejected": "Recent research in text-to-3D generation has struggled with multi-view consistency and shape accuracy. Sculpt3D proposes a novel framework that integrates 3D priors from reference objects into the 2D diffusion model without retraining, enhancing the quality and diversity of generated 3D objects."
  },
  {
    "prompt": "summary: Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose esti mation(HPE). Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing plat forms. Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SD Pose, a new self-distillation method for improving the per formance of small transformer-based models. To miti gate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the poten tial of small model parameters. Further, in order to pre vent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M pa rameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods.",
    "chosen": "This paper introduces SD-Pose, a self-distillation method for improving the performance of small transformer-based models in human pose estimation. The method features a Multi-Cycled Transformer module and a self-distillation scheme, achieving state-of-the-art performance among small models on the MSCOCO validation dataset.",
    "rejected": "SDPose introduces a self-distillation method to improve small transformer-based models' performance on human pose estimation tasks. By employing a Multi-Cycled Transformer module and self-distillation scheme, SDPose significantly boosts model efficiency and accuracy, achieving state-of-the-art results among tiny neural network methods."
  },
  {
    "prompt": "summary: Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of cap turing this variation. To address this issue, we present Sec ondPose, a novel approach integrating object-specific ge ometric features with semantic category priors from DI NOv2. Leveraging the advantage of DINOv2 in provid ing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific informa tion. These geometric features are then point-aligned with DINOv2 features to establish a consistent object represen tation under SE(3) transformations, facilitating the map ping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive exper iments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin.",
    "chosen": "This paper presents SecondPose, a novel approach for category-level object pose estimation that integrates object-specific geometric features with semantic category priors from DINOv2. The method extracts SE(3)-invariant geometric features and aligns them with DINOv2 features, achieving significant performance improvements on multiple datasets.",
    "rejected": "SecondPose addresses intra-class shape variation in category-level object pose estimation by integrating geometric features with semantic category priors from DINOv2. This approach enhances the consistency and accuracy of pose predictions, demonstrating superior performance on NOCS-REAL275 and HouseCat6D datasets."
  },
  {
    "prompt": "summary: We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associa tions when any one pair does not. We show our approach can successfully discover how the long tail of human ac tions sound from egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks.",
    "chosen": "This paper proposes SoundingActions, a self-supervised embedding method for learning how actions sound from narrated egocentric videos. The method reinforces audio-visual-language associations when all modalities agree, outperforming recent techniques on multiple datasets and tasks.",
    "rejected": "This paper presents MC3 embedding, a self-supervised multimodal learning method for associating actions with sounds in egocentric videos. MC3 outperforms existing techniques in discovering how sounding actions cluster based on visual and audio cues, advancing cross-modal tasks like activity recognition and content generation."
  },
  {
    "prompt": "summary: Learning generalizable visual representations from Inter net data has yielded promising results for robotics. Yet, pre vailing approaches focus on pre-training 2D representations, being sub-optimal to deal with occlusions and accurately localize objects in complex 3D scenes. Meanwhile, 3D rep resentation learning has been limited to single-object under standing. To address these limitations, we introduce a novel 3D pre-training framework for robotics named SUGAR that captures semantic, geometric and affordance properties of objects through 3D point clouds. We underscore the impor tance of cluttered scenes in 3D representation learning, and automatically construct a multi-object dataset benefiting from cost-free supervision in simulation. SUGAR employs a versatile transformer-based model to jointly address five pre-training tasks, namely cross-modal knowledge distilla tion for semantic learning, masked point modeling to un derstand geometry structures, grasping pose synthesis for object affordance, 3D instance segmentation and referring expression grounding to analyze cluttered scenes. We evalu ate our learned representation on three robotic-related tasks, namely, zero-shot 3D object recognition, referring expres sion grounding, and language-driven robotic manipulation. Experimental results show that SUGAR’s 3D representation outperforms state-of-the-art 2D and 3D representations.",
    "chosen": "This paper introduces SUGAR, a 3D pre-training framework for robotics that captures semantic, geometric, and affordance properties of objects through 3D point clouds. The framework includes five pre-training tasks and demonstrates superior performance on robotic-related tasks compared to state-of-the-art 2D and 3D representations.",
    "rejected": "SUGAR is a 3D pre-training framework designed to learn semantic, geometric, and affordance properties of objects through point clouds. It excels in handling occlusions and accurately localizing objects in cluttered scenes, showing improved performance over state-of-the-art 2D and 3D representations in various robotic tasks."
  },
  {
    "prompt": "summary: Automatic text-to-3D generation that combines Score Dis tillation Sampling (SDS) with the optimization of volume rendering has achieved remarkable progress in synthesizing realistic 3D objects. Yet most existing text-to-3D methods by SDS and volume rendering suffer from inaccurate geometry, e.g., the Janus issue, since it is hard to explicitly integrate 3D priors into implicit 3D representations. Besides, it is usually time-consuming for them to generate elaborate 3D models with rich colors. In response, this paper proposes GSGEN, a novel method that adopts Gaussian Splatting, a recent state of-the-art representation, to text-to-3D generation. GSGEN aims at generating high-quality 3D objects and address ing existing shortcomings by exploiting the explicit nature of Gaussian Splatting that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimiza tion strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimiza †Corresponding author tion, a coarse representation is established under 3D point cloud diffusion prior along with the ordinary 2D SDS opti mization, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative appearance refinement to enrich texture details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D assets with del icate details and accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components. Our",
    "chosen": "This paper proposes GSGEN, a text-to-3D generation method using Gaussian Splatting. The method includes geometry optimization and appearance refinement stages, generating high-quality 3D objects with accurate geometry and rich details. Evaluations show its effectiveness, particularly in capturing high-frequency components.",
    "rejected": "GSGEN proposes using Gaussian Splatting for text-to-3D generation to address geometry inaccuracies and time consumption issues. Through progressive optimization stages including geometry establishment and appearance refinement, GSGEN generates high-quality 3D objects with accurate geometries and rich colors."
  },
  {
    "prompt": "summary: Federated learning facilitates the collaborative learning of a global model across multiple distributed medical in stitutions without centralizing data. Nevertheless, the ex pensive cost of annotation on local clients remains an ob stacle to effectively utilizing local data. To mitigate this issue, federated active learning methods suggest leverag ing local and global model predictions to select a rela tively small amount of informative local data for annota tion. However, existing methods mainly focus on all lo cal data sampled from the same domain, making them un reliable in realistic medical scenarios with domain shifts among different clients. In this paper, we make the first at tempt to assess the informativeness of local data derived from diverse domains and propose a novel methodology termed Federated Evidential Active Learning (FEAL) to calibrate the data evaluation under domain shift. Specif ically, we introduce a Dirichlet prior distribution in both local and global models to treat the prediction as a distribu tion over the probability simplex and capture both aleatoric and epistemic uncertainties by using the Dirichlet-based evidential model. Then we employ the epistemic uncer tainty to calibrate the aleatoric uncertainty. Afterward, we design a diversity relaxation strategy to reduce data re dundancy and maintain data diversity. Extensive experi ments and analysis on five real multi-center medical im age datasets demonstrate the superiority of FEAL over the state-of-the-art active learning methods in federated sce narios with domain shifts. ",
    "chosen": "This paper presents FEAL, a federated evidential active learning method for medical imaging that addresses domain shifts among clients. The method uses a Dirichlet prior distribution to capture uncertainties and a diversity relaxation strategy to maintain data diversity. Experiments on multi-center datasets demonstrate its superiority over existing methods.",
    "rejected": "FEAL introduces federated evidential active learning to calibrate data evaluation under domain shifts in medical federated learning scenarios. By leveraging Dirichlet-based evidential models, FEAL effectively selects informative local data for annotation, improving model performance across diverse domains."
  },
  {
    "prompt": "summary: Despite many attempts to leverage pre-trained text-to image models (T2I) like Stable Diffusion (SD) [25] for con trollable image editing, producing good predictable results remains a challenge. Previous approaches have focused on either fine-tuning pre-trained T2I models on specific datasets to generate certain kinds of images (e.g., with a specific object or person), or on optimizing the weights, text prompts, and/or learning features for each input im age in an attempt to coax the image generator to produce the desired result. However, these approaches all have shortcomings and fail to produce good results in a pre dictable and controllable manner. To address this problem, we present TiNO-Edit, an SD-based method that focuses on optimizing the noise patterns and diffusion timesteps dur ing editing, something previously unexplored in the liter ature. With this simple change, we are able to generate results that both better align with the original images and reflect the desired result. Furthermore, we propose a set of new loss functions that operate in the latent domain of SD, greatly speeding up the optimization when compared to prior losses, which operate in the pixel domain. Our method can be easily applied to variations of SD includ ing Textual Inversion [13] and DreamBooth [27] that en code new concepts and incorporate them into the edited re sults. We present a host of image-editing capabilities en abled by our approach. Our",
    "chosen": "This paper introduces TiNO-Edit, a method for robust diffusion-based image editing that optimizes noise patterns and diffusion timesteps. The method also proposes new loss functions in the latent domain, improving optimization speed and enabling various image-editing capabilities.",
    "rejected": "TiNO-Edit focuses on optimizing noise patterns and diffusion timesteps during image editing using Stable Diffusion models. Introducing new latent domain loss functions, this method achieves better alignment with original images and desired edits, facilitating realistic and high-quality image synthesis."
  },
  {
    "prompt": "summary: We present a method to generate full-body selfies from photographs originally taken at arms length. Because self captured photos are typically taken close up, they have lim ited field of view and exaggerated perspective that distorts facial shapes. We instead seek to generate the photo some one else would take of you from a few feet away. Our ap proach takes as input four selfies of your face and body, a background image, and generates a full-body selfie in a de sired target pose. We introduce a novel diffusion-based ap proach to combine all of this information into high-quality, well-composed photos of you with the desired pose and background.",
    "chosen": "This paper presents a method to generate full-body selfies from arm's length photographs using a diffusion-based approach. The method takes four selfies, a background image, and generates a well-composed full-body selfie in a desired pose.",
    "rejected": "This work presents a diffusion-based method to generate full-body selfies from close-up photographs. Utilizing multiple input selfies and a background image, it creates well-composed photos of subjects in desired poses, overcoming limitations of traditional selfies such as distorted perspectives and limited field of view."
  },
  {
    "prompt": "summary: Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tu mor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, further more, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical ob servation: early-stage tumors (< 2cm) tend to have simi lar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that generative AI models, e.g., Dif fusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demo graphics, imaging protocols, and healthcare facilities.",
    "chosen": "This paper discusses generalizable tumor synthesis in medical images using diffusion models, focusing on early-stage tumors with similar imaging characteristics across organs. The method creates realistic synthetic tumors and trains AI models for detecting real tumors across different domains.",
    "rejected": "The paper discusses tumor synthesis for training AI models in detecting and segmenting tumors across different organs and imaging domains. Successful tumor synthesis requires creating visually realistic and generalizable tumors, ensuring AI models can effectively detect real tumors in varied medical images."
  },
  {
    "prompt": "summary: Pretrained diffusion models and their outputs are widely accessible due to their exceptional capacity for synthe sizing high-quality images and their open-source nature. The users, however, may face litigation risks owing to the models’ tendency to memorize and regurgitate train ing data during inference. To address this, we introduce Anti-Memorization Guidance (AMG), a novel framework employing three targeted guidance strategies for the main causes of memorization: image and caption duplication, and highly specific user prompts. Consequently, AMG en sures memorization-free outputs while maintaining high im age quality and text alignment, leveraging the synergy of its guidance methods, each indispensable in its own right. AMG also features an innovative automatic detection sys tem for potential memorization during each step of in ference process, allows selective application of guidance strategies, minimally interfering with the original sampling process to preserve output utility. We applied AMG to pre trained Denoising Diffusion Probabilistic Models (DDPM) and Stable Diffusion across various generation tasks. The results demonstrate that AMG is the first approach to suc cessfully eradicates all instances of memorization with no or marginal impacts on image quality and text-alignment, as evidenced by FID and CLIP scores.",
    "chosen": "This paper introduces Anti-Memorization Guidance (AMG), a framework to eliminate memorization in diffusion models while maintaining image quality and text alignment. AMG uses three guidance strategies targeting image and caption duplication and specific user prompts. It includes an automatic detection system for potential memorization during inference. Experiments show AMG successfully removes memorization with minimal impact on quality and alignment.",
    "rejected": "Towards Memorization-Free Diffusion Models. This research proposes a novel approach to mitigate memorization in diffusion models, focusing on enhancing generalization while maintaining sample quality. By integrating specific regularization techniques, the model can achieve superior performance on diverse datasets without overfitting."
  },
  {
    "prompt": "summary: 3D pose transfer that aims to transfer the desired pose to a target mesh is one of the most challenging 3D gener ation tasks. Previous attempts rely on well-defined para metric human models or skeletal joints as driving pose sources. However, to obtain those clean pose sources, cumbersome but necessary pre-processing pipelines are in evitable, hindering implementations of the real-time appli cations. This work is driven by the intuition that the robust ness of the model can be enhanced by introducing adver sarial samples into the training, leading to a more invulner able model to the noisy inputs, which even can be further extended to directly handling the real-world data like raw point clouds/scans without intermediate processing. Fur thermore, we propose a novel 3D pose Masked Autoencoder (3D-PoseMAE), a customized MAE that effectively learns 3D extrinsic presentations (i.e., pose). 3D-PoseMAE facil itates learning from the aspect of extrinsic attributes by si multaneously generating adversarial samples that perturb the model and learning the arbitrary raw noisy poses via a multi-scale masking strategy. Both qualitative and quanti tative studies show that the transferred meshes given by our network result in much better quality. Besides, we demon strate the strong generalizability of our method on various poses, different domains, and even raw scans. Experimental results also show meaningful insights that the intermediate adversarial samples generated in the training can success fully attack the existing pose transfer models.",
    "chosen": "This paper presents a 3D pose transfer method enhanced by adversarial samples and a 3D pose Masked Autoencoder (3D-PoseMAE). The method improves robustness to noisy inputs and generalizes well to raw point clouds/scans. The multi-scale masking strategy helps learn arbitrary raw noisy poses. Results show better quality transferred meshes and strong generalizability.",
    "rejected": "Towards Robust 3D Pose Transfer with Adversarial Learning. The study introduces an adversarial learning framework to improve the robustness of 3D pose transfer across different subjects. Through comprehensive experiments, it demonstrates significant improvements in accuracy and stability under various conditions."
  },
  {
    "prompt": "summary: Audio-visual segmentation (AVS) is a challenging task that involves accurately segmenting sounding objects based on audio-visual cues. The effectiveness of audio-visual learning critically depends on achieving accurate cross modal alignment between sound and visual objects. Suc cessful audio-visual learning requires two essential compo nents: 1) a challenging dataset with high-quality pixel-level multi-class annotated images associated with audio files, and 2) a model that can establish strong links between au dio information and its corresponding visual object. How ever, these requirements are only partially addressed by cur rent methods, with training sets containing biased audio visual data, and models that generalise poorly beyond this biased training set. In this work, we propose a new cost effective strategy to build challenging and relatively unbi ased high-quality audio-visual segmentation benchmarks. We also propose a new informative sample mining method for audio-visual supervised contrastive learning to leverage discriminative contrastive samples to enforce cross-modal understanding. We show empirical results that demonstrate the effectiveness of our benchmark. Furthermore, experi ments conducted on existing AVS datasets and on our new benchmark show that our method achieves state-of-the-art (SOTA) segmentation accuracy1.",
    "chosen": "This paper proposes a cost-effective strategy to build unbiased audio-visual segmentation benchmarks and a supervised contrastive learning method for audio-visual segmentation. The method mines informative samples to better constrain audio-visual embeddings. Experiments demonstrate state-of-the-art segmentation accuracy on existing datasets and the new benchmark.",
    "rejected": "Unraveling Instance Associations A Closer Look for Audio-Visual Segmentation. This work investigates the challenge of associating instances in audio-visual scenes. It presents an innovative method that enhances segmentation accuracy by leveraging multimodal information, achieving state-of-the-art results."
  },
  {
    "prompt": "summary: Significant progress in image deblurring has been achieved by deep learning methods, especially the remark able performance of supervised models on paired synthetic data. However, real-world quality degradation is more com plex than synthetic datasets, and acquiring paired data in real-world scenarios poses significant challenges. To ad dress these challenges, we propose a novel unsupervised image deblurring framework based on self-enhancement. The framework progressively generates improved pseudo sharp and blurry image pairs without the need for real paired datasets, and the generated image pairs with higher qualities can be used to enhance the performance of the reconstruc tor. To ensure the generated blurry images are closer to the real blurry images, we propose a novel re-degradation principal component consistency loss, which enforces the principal components of the generated low-quality images to be similar to those of re-degraded images from the original sharp ones. Furthermore, we introduce the self-enhancement strategy that significantly improves deblurring performance without increasing the computational complexity of network during inference. Through extensive experiments on multiple real-world blurry datasets, we demonstrate the superiority of our approach over other state-of-the-art unsupervised methods.",
    "chosen": "This paper presents an unsupervised image deblurring framework based on self-enhancement. The framework generates improved pseudo-sharp and blurry image pairs progressively. A re-degradation principal component consistency loss ensures generated blurry images resemble real ones. The self-enhancement strategy improves deblurring performance without added inference complexity.",
    "rejected": "Unsupervised Blind Image Deblurring Based on Self-Enhancement. The paper addresses the problem of blind image deblurring using unsupervised learning methods. It introduces a self-enhancement technique that iteratively refines the image, leading to clearer outcomes without prior knowledge of blur kernels."
  },
  {
    "prompt": "summary: Phase unwrapping (PU) is a technique to reconstruct original phase images from their noisy wrapped counter parts, finding many applications in scientific imaging. Al though supervised learning has shown promise in PU, its utility is limited in ground-truth (GT) scarce scenarios. This paper presents an unsupervised learning approach that eliminates the need for GTs during end-to-end training. Our approach leverages the insight that both the gradients and wrapped gradients of wrapped phases serve as noisy labels for GT phase gradients, along with sparse outliers induced by the wrapping operation. A recorruption-based self-reconstruction loss in the gradient domain is proposed to mitigate the adverse effects of label noise, complemented with a self-distillation loss for improved generalization. Ad ditionally, by unfolding a variational model of PU that uti lizes wrapped gradients of wrapped phases for its data fitting term, we develop a deep unrolling network that en codes physics of phase wrapping and incorporates special treatments on outliers. In the experiments on three types of phase data, our approach outperforms existing GT-free methods and competes well against the supervised ones.",
    "chosen": "This paper introduces an unsupervised learning approach for phase unwrapping (PU) that doesn't require ground-truth during training. The approach uses gradients and wrapped gradients of wrapped phases as noisy labels, with a self-reconstruction loss and self-distillation loss. It also develops a deep unrolling network encoding physics of phase wrapping. The method outperforms existing GT-free methods and competes with supervised ones.",
    "rejected": "Unsupervised Deep Unrolling Networks for Phase Unwrapping. This paper develops an unsupervised deep unrolling network designed to solve phase unwrapping problems efficiently. The proposed method combines traditional optimization algorithms with deep learning, offering faster computation times and improved accuracy."
  },
  {
    "prompt": "summary: Recent Large Language Models (LLMs) have been en hanced with vision capabilities, enabling them to compre hend images, videos, and interleaved vision-language con tent. However, the learning methods of these large multi modal models (LMMs) typically treat videos as predeter mined clips, rendering them less effective and efficient at handling streaming video inputs. In this paper, we pro pose a novel Learning-In-Video-Stream (LIVE) framework, which enables temporally aligned, long-context, and real time dialogue within a continuous video stream. Our LIVE framework comprises comprehensive approaches to achieve video streaming dialogue, encompassing: (1) a training ob jective designed to perform language modeling for contin uous streaming inputs, (2) a data generation scheme that converts offline temporal annotations into a streaming di alogue format, and (3) an optimized inference pipeline to speed up interactive chat in real-world video streams. With BCorresponding Author. our LIVE framework, we develop a simplified model called VideoLLM-online and demonstrate its significant advan tages in processing streaming videos. For instance, our VideoLLM-online-7B model can operate at over 10 FPS on an A100 GPU for a 5-minute video clip from Ego4D narration. Moreover, VideoLLM-online also showcases state-of-the-art performance on public offline video bench marks, such as recognition, captioning, and forecasting. The code, model, data, and demo have been made available at showlab.github.io/videollm-online.",
    "chosen": "This paper proposes the LIVE framework for temporally aligned, long-context, and real-time dialogue within continuous video streams. LIVE includes a training objective for streaming inputs, a data generation scheme, and an optimized inference pipeline. The VideoLLM-online model demonstrates significant advantages in processing streaming videos and shows state-of-the-art performance on public offline video benchmarks.",
    "rejected": "VideoLLM-online Online Video Large Language Model for Streaming Video. This contribution presents VideoLLM, a large language model tailored for streaming video analysis. Capable of processing online videos in real-time, it showcases remarkable performance in understanding and generating textual descriptions synchronized with video content."
  },
  {
    "prompt": "summary: Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embed dings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the de fault choice for the image encoder. Although pure trans former proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for im age encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language image pretraining (CLIP) framework. We provide a com prehensive way to benchmark different vision models, cov ering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin L significantly outperforms ViT-L by 2.0% ImageNet zero shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 di verse benchmarks, including classification, retrieval, open vocabulary detection and segmentation, and large multi modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% Im ageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).",
    "chosen": "This paper aims to build an evaluation protocol for vision models in the vision-language era under the CLIP framework. It introduces ViTamin, a new vision model tailored for VLMs. ViTamin-L outperforms ViT-L in ImageNet zero-shot accuracy and presents promising results on diverse benchmarks. ViTamin-XL surpasses models with ten times more parameters.",
    "rejected": "ViTamin Designing Scalable Vision Models in the Vision-Language Era. Focusing on scalable vision models, this paper introduces ViTamin, which integrates advancements in vision-language models. ViTamin is designed to handle large-scale visual recognition tasks more effectively, demonstrating its superiority in multiple benchmarks."
  },
  {
    "prompt": "summary: Recent innovations on text-to-3D generation have fea tured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. How ever, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D dif fusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Mean while, we couple the SDS optimization with additional dif ferentiable reward function that encourages rendering im ages of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual ap pearance of 3D models and thus leads to higher visual fi delity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our",
    "chosen": "This paper introduces VP3D, a Visual Prompt-guided text-to-3D diffusion model. VP3D uses a 2D visual prompt generated from input text to strengthen SDS optimization. It couples SDS with a differentiable reward function for better visual alignment. Experiments show VP3D significantly improves visual fidelity and texture details.",
    "rejected": "VP3D Unleashing 2D Visual Prompt for Text-to-3D Generation. VP3D is introduced as a method to generate 3D models from text prompts using 2D visual cues. This technique enhances the realism and detail of generated 3D objects, making significant contributions to the field of computer graphics."
  },
  {
    "prompt": "summary: Spatio-temporal grounding describes the task of local izing events in space and time, e.g., in video data, based on verbal descriptions only. Models for this task are usu ally trained with human-annotated sentences and bound ing box supervision. This work addresses this task from a multimodal supervision perspective, proposing a frame work for spatio-temporal action grounding trained on loose video and subtitle supervision only, without human annota tion. To this end, we combine local representation learn ing, which focuses on leveraging ﬁne-grained spatial in formation, with a global representation encoding that cap tures higher-level representations and incorporates both in a joint approach. To evaluate this challenging task in a real life setting, a new benchmark dataset is proposed, provid ing dense spatio-temporal grounding annotations in long, untrimmed, multi-action instructional videos for over 5K events. We evaluate the proposed approach and other meth ods on the proposed and standard downstream tasks, show ing that our method improves over current baselines in var ious settings, including spatial, temporal, and untrimmed multi-action spatio-temporal grounding.",
    "chosen": "This paper addresses spatio-temporal grounding from a multimodal supervision perspective, proposing a framework trained on loose video and subtitle supervision. It combines local and global representation learning. A new benchmark dataset with dense annotations in long instructional videos is proposed. The method improves over baselines in various settings.",
    "rejected": "What When and Where Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action. This study explores self-supervised learning for spatio-temporal grounding in long, untrimmed videos containing multiple actions. It proposes a framework that accurately localizes and recognizes actions within complex scenes without manual annotations."
  },
  {
    "prompt": "summary: Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent dif fusion. Our observation is that content (i.e., gestures re lated to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vec tors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vec tors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emo tions and style by combining the content from the driving speech with the emotion and style of another speech se quence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and per ceptual evaluations demonstrate that AMUSE outputs real istic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content, and better represent the emotion expressed by the input speech. Our",
    "chosen": "This paper presents AMUSE, an emotional speech-driven body animation model based on latent diffusion. AMUSE disentangles content, emotion, and personal style in latent vectors. It synthesizes 3D human gestures from speech with control over emotion and style. Evaluations show realistic gesture sequences better synchronized with speech content and emotion.",
    "rejected": "Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion. The paper discusses a method for generating realistic 3D body animations driven by emotional speech. Using disentangled latent diffusion models, it achieves nuanced and contextually appropriate animations, significantly advancing the state of the art in non-verbal communication synthesis."
  },
  {
    "prompt": "summary: We introduce a novel approach to single image denoising based on the Blind Spot Denoising principle, which we call MAsked and SHuffled Blind Spot Denoising (MASH). We focus on the case of correlated noise, which often plagues real images. MASH is the result of a careful analysis to determine the relationships between the level of blindness (masking) of the input and the (unknown) noise correlation. Moreover, we introduce a shuffling technique to weaken the local correlation of noise, which in turn yields an addi tional denoising performance improvement. We evaluate MASH via extensive experiments on real-world noisy image datasets. We demonstrate state-of-the-art results compared to existing self-supervised denoising methods. ",
    "chosen": "This paper introduces MASH, a novel approach to single image denoising based on Blind Spot Denoising principles. MASH focuses on correlated noise in real images and introduces a shuffling technique to weaken local noise correlation. Extensive experiments demonstrate state-of-the-art results compared to existing self-supervised denoising methods.",
    "rejected": "We introduce MASH, a novel Blind Spot Denoising technique that handles correlated noise in images by utilizing masked and shuffled input data. This method improves denoising performance on real-world noisy images compared to existing self-supervised methods."
  },
  {
    "prompt": "summary: Referring Image Segmentation (RIS) is a challenging task that requires an algorithm to segment objects referred by free-form language expressions. Despite significant progress in recent years, most state-of-the-art (SOTA) meth ods still suffer from considerable language-image modality gap at the pixel and word level. These methods generally 1) rely on sentence-level language features for language image alignment and 2) lack explicit training supervision for fine-grained visual grounding. Consequently, they ex hibit weak object-level correspondence between visual and language features. Without well-grounded features, prior methods struggle to understand complex expressions that require strong reasoning over relationships among multiple objects, especially when dealing with rarely used or am biguous clauses. To tackle this challenge, we introduce a novel Mask Grounding auxiliary task that significantly im proves visual grounding within language features, by ex plicitly teaching the model to learn fine-grained correspon dence between masked textual tokens and their matching visual objects. Mask Grounding can be directly used on prior RIS methods and consistently bring improvements. Furthermore, to holistically address the modality gap, we also design a cross-modal alignment loss and an accom panying alignment module. These additions work syner gistically with Mask Grounding. With all these techniques, our comprehensive approach culminates in MagNet (Mask grounded Network), an architecture that significantly out performs prior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating our method’s effec tiveness in addressing current limitations of RIS algorithms. Our code and pre-trained weights will be released.",
    "chosen": "This paper presents MagNet, a mask-grounded network for referring image segmentation (RIS). The method introduces a Mask Grounding auxiliary task to improve visual grounding within language features and designs a cross-modal alignment loss and alignment module. MagNet significantly outperforms prior arts on key benchmarks.",
    "rejected": "This paper presents MagNet, an advanced referring image segmentation network incorporating Mask Grounding and cross-modal alignment loss to address language-image modality gaps, enhancing object-level correspondence between visual and language features for improved segmentation accuracy."
  },
  {
    "prompt": "summary: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer the pixel-wise knowledge from the labeled source domain to the unlabeled target do main. However, current UDA methods typically assume a shared label space between source and target, limiting their applicability in real-world scenarios where novel cat egories may emerge in the target domain. In this paper, we introduce Open-Set Domain Adaptation for Semantic Seg mentation (OSDA-SS) for the first time, where the target domain includes unknown classes. We identify two major problems in the OSDA-SS scenario as follows: 1) the exist ing UDA methods struggle to predict the exact boundary of the unknown classes, and 2) they fail to accurately predict the shape of the unknown classes. To address these issues, we propose Boundary and Unknown Shape-Aware open set domain adaptation, coined BUS. Our BUS can accu rately discern the boundaries between known and unknown classes in a contrastive manner using a novel dilation erosion-based contrastive loss. In addition, we propose OpenReMix, a new domain mixing augmentation method that guides our model to effectively learn domain and size invariant features for improving the shape detection of the known and unknown classes. Through extensive experi ments, we demonstrate that our proposed BUS effectively detects unknown classes in the challenging OSDA-SS sce nario compared to the previous methods by a large margin. The",
    "chosen": "This paper addresses open-set domain adaptation for semantic segmentation, where the target domain includes unknown classes. The proposed BUS method accurately discerns boundaries between known and unknown classes and improves shape detection. The method demonstrates effective unknown class detection in challenging scenarios.",
    "rejected": "The authors propose BUS, an open-set domain adaptation method for semantic segmentation capable of detecting unknown classes by using dilation-erosion-based contrastive loss and OpenReMix augmentation, significantly outperforming previous methods in identifying boundaries and shapes of unknown classes."
  },
  {
    "prompt": "summary: This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework, where the input and output of the system are multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key advantages can be brought: 1) We can perform real-like conversations with individuals worldwide in a virtual meeting by utilizing our own primary languages. In contrast to Speech-to-Speech Translation (A2A), which solely translates between audio modalities, the proposed AV2AV directly translates between audio-visual speech. This capability enhances the dialogue experience by presenting synchronized lip movements along with the translated speech. 2) We can improve the robustness of the spoken language translation system. By employing the complementary information of audio-visual speech, the system can effectively translate spoken language even in the presence of acoustic noise, showcasing robust perfor mance. To mitigate the problem of the absence of a parallel AV2AV translation dataset, we propose to train our spoken language translation system with the audio-only dataset of A2A. This is done by learning unified audio-visual speech representations through self-supervised learning in ad vance to train the translation system. Moreover, we propose an AV-Renderer that can generate raw audio and video in parallel. It is designed with zero-shot speaker modeling, thus the speaker in source audio-visual speech can be maintained at the target translated audio-visual speech. The effectiveness of AV2AV is evaluated with extensive ex periments in a many-to-many language translation setting. Demo page is available on choijeongsoo.github.io/av2av. ∗Equal contribution. †Corresponding author. This work was sup ported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1A2C2005529), Insti tute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities), and BK21 FOUR (Connected AI Education & Research Program for Industry and Society Innovation, KAIST EE, No. 4120200113769). It Multilingual  AV2AV Translation … “J'aimerais t'aider” “Ich würde dir gerne helfen” “Me gustaría ayudarte” “Mi piacerebbe aiutarti” De En Pt Es Fr AV Speech Unit  “I would like to help you” “Eu gostaria de ajudar você” … AV Speech Unit  Figure 1. Conceptual illustration of the proposed multilin gual Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework. The system can directly translate between multilingual AV speech without requiring any text. Note that the proposed AV2AV can generate both audio speech and visual speech in listener-oriented (i.e., translated) languages.",
    "chosen": "This paper proposes a direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework. The framework translates between multimodal audio-visual speech inputs and outputs, enhancing dialogue experiences with synchronized lip movements and improving translation robustness in noisy environments.",
    "rejected": "This work introduces AV2AV, a direct audio-visual speech translation framework that enhances dialogue experience by presenting synchronized lip movements along with translated speech, improving robustness against acoustic noise through self-supervised learning of unified audio-visual speech representations."
  },
  {
    "prompt": "summary: We address the problem of generalized category discov ery (GCD) that aims to partition a partially labeled col lection of images; only a small part of the collection is la beled and the total number of target classes is unknown. To address this generalized image clustering problem, we revisit the mean-shift algorithm, i.e., a classic, powerful technique for mode seeking, and incorporate it into a con trastive learning framework. The proposed method, dubbed Contrastive Mean-Shift (CMS) learning, trains an embed ding network to produce representations with better cluster ing properties by an iterative process of mean shift and con trastive update. Experiments demonstrate that our method, both in settings with and without the total number of clus ters being known, achieves state-of-the-art performance on six public GCD benchmarks without bells and whistles.",
    "chosen": "This paper introduces Contrastive Mean-Shift (CMS) learning for generalized category discovery (GCD). The method integrates mean shift into a contrastive learning framework, achieving state-of-the-art performance on GCD benchmarks.",
    "rejected": "The paper discusses Contrastive Mean-Shift (CMS) learning, a method for generalized category discovery that integrates mean shift into a contrastive learning framework, achieving state-of-the-art performance on multiple benchmarks without requiring total cluster number information."
  },
  {
    "prompt": "summary: Photometric stereo leverages variations in illumination conditions to reconstruct surface normals. Display photo metric stereo, which employs a conventional monitor as an illumination source, has the potential to overcome limita tions often encountered in bulky and difficult-to-use conven tional setups. In this paper, we present differentiable dis play photometric stereo (DDPS), addressing an often over looked challenge in display photometric stereo: the design of display patterns. Departing from using heuristic display patterns, DDPS learns the display patterns that yield accu rate normal reconstruction for a target system in an end to-end manner. To this end, we propose a differentiable framework that couples basis-illumination image formation with analytic photometric-stereo reconstruction. The dif ferentiable framework facilitates the effective learning of display patterns via auto-differentiation. Also, for training supervision, we propose to use 3D printing for creating a real-world training dataset, enabling accurate reconstruc tion on the target real-world setup. Finally, we exploit that conventional LCD monitors emit polarized light, which al lows for the optical separation of diffuse and specular re flections when combined with a polarization camera, lead ing to accurate normal reconstruction. Extensive evalua tion of DDPS shows improved normal-reconstruction accu racy compared to heuristic patterns and demonstrates com pelling properties such as robustness to pattern initializa tion, calibration errors, and simplifications in image for mation and reconstruction.",
    "chosen": "This paper presents differentiable display photometric stereo (DDPS), which learns display patterns for accurate normal reconstruction in an end-to-end manner. The framework couples basis-illumination image formation with analytic photometric-stereo reconstruction, showing improved normal-reconstruction accuracy.",
    "rejected": "DDPS is presented as a differentiable display photometric stereo framework that learns optimal illumination patterns via auto-differentiation for accurate surface normal reconstruction, demonstrating superior reconstruction accuracy and robustness against various errors."
  },
  {
    "prompt": "summary: This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the gener ated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various fa cial expressions and geometric transformations. Our frame work utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with content based features, enabling the detection of visual and tempo ral artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its su periority in cross-dataset and cross-manipulation scenar ios. Through further analysis, we also validate the impor tance of using temporal changes of style latent vectors to improve the generality of deepfake video detection.",
    "chosen": "This paper proposes a new approach for fake video detection by analyzing style latent vectors and their temporal changes. The framework utilizes a StyleGRU module and a style attention module to detect visual and temporal artifacts, demonstrating superiority in cross-dataset and cross-manipulation scenarios.",
    "rejected": "A new approach for fake video detection is introduced, focusing on the analysis of style latent vectors' abnormal behavior in temporal changes. Utilizing StyleGRU and style attention modules, this method shows superiority in cross-dataset and cross-manipulation scenarios."
  },
  {
    "prompt": "summary: Advancements in neural signed distance fields (SDFs) have enabled modeling 3D surface geometry from a set of 2D images of real-world scenes. Baking neural SDFs can extract explicit mesh with appearance baked into texture maps as neural features. The baked meshes still have a large memory footprint and require a powerful GPU for real-time rendering. Neural optimization of such large meshes with differentiable rendering pose significant chal lenges. We propose a method to produce optimized meshes for large unbounded scenes with low triangle budget and high fidelity of geometry and appearance. We achieve this by combining advancements in baking neural SDFs with classical mesh simplification techniques and proposing a joint appearance-geometry refinement step. The visual quality is comparable to or better than state-of-the-art neu ral meshing and baking methods with high geometric ac curacy despite significant reduction in triangle count, mak ing the produced meshes efficient for storage, transmission, and rendering on mobile hardware. We validate the effec tiveness of the proposed method on large unbounded scenes from mip-NeRF 360, Tanks & Temples, and Deep Blend ing datasets, achieving at-par rendering quality with 73× reduced triangles and 11× reduction in memory footprint.",
    "chosen": "This paper presents LTM, a method for producing optimized meshes for large unbounded scenes with low triangle budget and high fidelity. The method combines baking neural SDFs with classical mesh simplification techniques, achieving significant reduction in triangle count and memory footprint while maintaining visual quality.",
    "rejected": "The research proposes a method for optimizing large unbounded scene meshes derived from neural SDFs with classical mesh simplification techniques, achieving high fidelity in geometry and appearance while significantly reducing triangle count for efficient storage and transmission."
  },
  {
    "prompt": "summary: Omnidirectional cameras are extensively used in various applications to provide a wide field of vision. However, they face a challenge in synthesizing novel views due to the inevitable presence of dynamic objects, including the photographer, in their wide field of view. In this paper, we introduce a new approach called Omnidirectional Lo cal Radiance Fields (OmniLocalRF) that can render static only scene views, removing and inpainting dynamic ob jects simultaneously. Our approach combines the princi ples of local radiance fields with the bidirectional optimiza tion of omnidirectional rays. Our input is an omnidirec tional video, and we evaluate the mutual observations of the entire angle between the previous and current frames. To reduce ghosting artifacts of dynamic objects and inpaint occlusions, we devise a multi-resolution motion mask pre diction module. Unlike existing methods that primarily sep arate dynamic components through the temporal domain, our method uses multi-resolution neural feature planes for precise segmentation, which is more suitable for long 360◦ videos. Our experiments validate that OmniLocalRF out performs existing methods in both qualitative and quanti tative metrics, especially in scenarios with complex real world scenes. In particular, our approach eliminates the need for manual interaction, such as drawing motion masks by hand and additional pose estimation, making it a highly effective and efficient solution.",
    "chosen": "This paper introduces OmniLocalRF, a method for rendering static-only scene views from omnidirectional videos by removing and inpainting dynamic objects. The approach combines local radiance fields with bidirectional optimization of omnidirectional rays, outperforming existing methods in complex real-world scenes.",
    "rejected": "This paper introduces a novel framework for spatio-temporal action grounding trained solely on video and subtitle supervision, eliminating the need for human annotations. The method combines local representation learning to capture fine-grained spatial information with global representation encoding for higher-level representations. A new benchmark dataset is proposed, featuring dense spatio-temporal grounding annotations in long, untrimmed multi-action instructional videos. Evaluations demonstrate that the proposed approach outperforms current baselines in various settings, including spatial and temporal localization tasks."
  },
  {
    "prompt": "summary: Video Transformers have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video transformers suffer from heavy computational costs induced by the mas sive number of tokens across the entire video frames, which has been the major barrier to train and deploy the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training-free to ken merging for lightweight video Transformer (vid-TLDR) that aims to enhance the efficiency of video Transformers by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Fur ther, we introduce the saliency-aware token merging strat *: Equal contribution, †: Corresponding author. egy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR sig nificantly mitigates the computational complexity of video Transformers while achieving competitive performance com pared to the base model without vid-TLDR.",
    "chosen": "This paper proposes vid-TLDR, a training-free token merging method for lightweight video Transformers. The method enhances efficiency by merging background tokens using a saliency-aware strategy, significantly mitigating computational complexity while maintaining competitive performance.",
    "rejected": "AMUSE is presented as an emotional speech-driven body animation model based on latent diffusion. It addresses the limitation of existing methods by explicitly modeling the impact of emotions on generated gestures. AMUSE maps audio input to three disentangled latent vectors representing content, emotion, and personal style. These vectors condition a latent diffusion model trained to generate gesture motion sequences. AMUSE synthesizes realistic 3D human gestures from speech while allowing control over expressed emotions and style. Comprehensive evaluations show that AMUSE produces more synchronized and emotionally expressive gestures compared to state-of-the-art methods."
  },
  {
    "prompt": "summary: Utilizing multi-view inputs to synthesize novel-view im ages, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we in troduce a Generalizable Semantic Neural Radiance Fields (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view image and the as sociated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract se mantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved perfor mances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effec tiveness of our sampling strategy for visual rendering is fur ther verified.",
    "chosen": "This paper introduces GSNeRF, a generalizable semantic neural radiance field that produces novel-view images and associated semantic maps for unseen scenes. GSNeRF consists of Semantic Geo-Reasoning and Depth-Guided Visual Rendering stages. Experiments confirm GSNeRF's effectiveness in novel-view synthesis and semantic segmentation.",
    "rejected": "Generalizable Semantic Neural Radiance Fields (GSNeRF) integrates semantic information into novel view synthesis, enabling both image and semantic map generation. It uses a two-stage process: Semantic Geo-Reasoning extracts features from multi-view images, while Depth-Guided Visual rendering generates improved images and semantic renderings. GSNeRF demonstrates superior performance in novel view synthesis and semantic segmentation tasks compared to prior methods."
  },
  {
    "prompt": "summary: Deep Neural Networks (DNNs) are widely used for vi sual classiﬁcation tasks, but their complex computation pro cess and black-box nature hinder decision transparency and interpretability. Class activation maps (CAMs) and recent variants provide ways to visually explain the DNN decision making process by displaying ‘attention’ heatmaps of the DNNs. Nevertheless, the CAM explanation only offers rela tive attention information, that is, on an attention heatmap, we can interpret which image region is more or less im portant than the others. However, these regions cannot be meaningfully compared across classes, and the contri bution of each region to the model’s class prediction is not revealed. To address these challenges that ultimately lead to better DNN Interpretation, in this paper, we pro pose CAPE, a novel reformulation of CAM that provides a uniﬁed and probabilistically meaningful assessment of the contributions of image regions. We quantitatively and qual itatively compare CAPE with state-of-the-art CAM methods on CUB and ImageNet benchmark datasets to demonstrate enhanced interpretability. We also test on a cytology imag ing dataset depicting a challenging Chronic Myelomono cytic Leukemia (CMML) diagnosis problem.",
    "chosen": "This paper proposes CAPE, a reformulation of class activation maps (CAM) that provides a unified and probabilistically meaningful assessment of image region contributions. CAPE enhances DNN interpretation by revealing region contributions to class predictions, demonstrated on benchmark datasets and a medical imaging task.",
    "rejected": "CAPE reformulates Class Activation Maps (CAM) to offer a unified and probabilistically meaningful assessment of image region contributions. Unlike traditional CAMs that provide relative attention values, CAPE enables cross-class comparisons and reveals each region's contribution to class prediction. Experiments on CUB, ImageNet, and a cytology dataset demonstrate enhanced interpretability and diagnostic accuracy, making DNN decisions more transparent."
  },
  {
    "prompt": "summary: Music is a universal language that can communicate emotions and feelings. It forms an essential part of the whole spectrum of creative media, ranging from movies to social media posts. Machine learning models that can syn thesize music are predominantly conditioned on textual de scriptions of it. Inspired by how musicians compose music not just from a movie script, but also through visualizations, we propose MELFUSION, a model that can effectively use cues from a textual description and the corresponding im age to synthesize music. MELFUSION is a text-to-music diffusion model with a novel “visual synapse”, which effec tively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we in troduce a new dataset MeLBench, and propose a new eval uation metric IMSM. Our exhaustive experimental evalua tion suggests that adding visual information to the music synthesis pipeline significantly improves the quality of gen erated music, measured both objectively and subjectively, with a relative gain of up to 67.98% on the FAD score. We hope that our work will gather attention to this pragmatic, yet relatively under-explored research area.",
    "chosen": "This paper presents MELFUSION, a text-to-music diffusion model with a \"visual synapse\" that infuses visual semantics into music synthesis. The model improves music quality using visual and textual cues, validated on a new dataset and evaluation metric.",
    "rejected": "MELFUSION is a text-to-music diffusion model incorporating visual cues for music synthesis. By integrating a \"visual synapse,\" it effectively blends textual descriptions with corresponding images to produce high-quality music. An evaluation on MeLBench shows significant improvements in generated music quality, achieving up to a 67.98% gain on the FAD score. This approach highlights the potential of multimodal inputs in creative applications."
  },
  {
    "prompt": "summary: Open-vocabulary semantic segmentation presents the challenge of labeling each pixel within an image based on a wide range of text descriptions. In this work, we introduce a novel cost-based approach to adapt vision-language foun dation models, notably CLIP, for the intricate task of se mantic segmentation. Through aggregating the cosine sim ilarity score, i.e., the cost volume between image and text embeddings, our method potently adapts CLIP for segment ing seen and unseen classes by fine-tuning its encoders, ad dressing the challenges faced by existing methods in han dling unseen classes. Building upon this, we explore meth ods to effectively aggregate the cost volume considering its multi-modal nature of being established between image and text embeddings. Furthermore, we examine various meth ods for efficiently fine-tuning CLIP.",
    "chosen": "This paper introduces a cost-based framework for open-vocabulary semantic segmentation, adapting CLIP through cost volume aggregation. The method effectively segments seen and unseen classes, with fine-tuning improving performance on unseen classes.",
    "rejected": "A cost-based approach adapts vision-language models like CLIP for open-vocabulary semantic segmentation by aggregating cosine similarity scores between image and text embeddings. Fine-tuning CLIP encoders improves handling of seen and unseen classes, addressing limitations in existing methods. Cost aggregation proves effective in generalizing to unseen classes, significantly enhancing performance metrics over feature aggregation methods."
  },
  {
    "prompt": "summary: We present a new open-vocabulary detection framework. Our framework uses both image-level labels and detailed detection annotations when available. Our framework pro ceeds in three steps. We first train a language-conditioned object detector on fully-supervised detection data. This de tector gets to see the presence or absence of ground truth classes during training, and conditions prediction on the set of present classes. We use this detector to pseudo label images with image-level labels. Our detector pro vides much more accurate pseudo-labels than prior ap proaches with its conditioning mechanism. Finally, we train an unconditioned open-vocabulary detector on the pseudo-annotated images. The resulting detector, named DECOLA, shows strong zero-shot performance in open vocabulary LVIS benchmark as well as direct zero-shot trans fer benchmarks on LVIS, COCO, Object365, and OpenIm ages. DECOLA outperforms the prior arts by 17.1 APrare and 9.4 mAP on zero-shot LVIS benchmark. DECOLA achieves state-of-the-art results in various model sizes, ar chitectures, and datasets by only training on open-sourced data and academic-scale computing.",
    "chosen": "This paper presents DECOLA, an open-vocabulary detection framework using language-conditioned object detectors. The framework generates accurate pseudo-labels and trains an unconditioned detector, showing strong zero-shot performance on multiple benchmarks.",
    "rejected": "DECOLA, a language-conditioned object detector, leverages image-level labels and detection annotations to generate accurate pseudo-labels for training. It outperforms previous methods in zero-shot detection benchmarks on LVIS, COCO, Object365, and OpenImages datasets. DECOLA achieves state-of-the-art results across various model sizes and architectures, demonstrating strong generalization capabilities in open-vocabulary settings."
  },
  {
    "prompt": "summary: While GAN-based models have been successful in image stylization tasks, they often struggle with structure preser vation while stylizing a wide range of input images. Re cently, diffusion models have been adopted for image styl ization but still lack the capability to maintain the original quality of input images. Building on this, we propose OS ASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to ef fectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to var ious experimental settings, including stylization with out of-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models. ",
    "chosen": "This paper proposes OSASIS, a one-shot stylization method using diffusion models. OSASIS disentangles image semantics and structure, offering control over content and style levels. It outperforms other methods, especially on rarely encountered inputs.",
    "rejected": "OSASIS introduces a one-shot stylization method using diffusion models, focusing on robust structure preservation. It disentangles semantics from structure, allowing controlled content and style implementation. OSASIS excels particularly in handling rare input images, offering promising solutions for stylization via diffusion models. The approach surpasses other stylization methods in preserving structural integrity."
  },
  {
    "prompt": "summary: Video Frame Interpolation (VFI), which aims at gener ating high-frame-rate videos from low-frame-rate inputs, is a highly challenging task. The emergence of bio-inspired sensors known as event cameras, which boast microsecond level temporal resolution, has ushered in a transformative era for VFI. Nonetheless, the application of event-based VFI techniques in domains with distinct environments from the training data can be problematic. This is mainly be cause event camera data distribution can undergo substan tial variations based on camera settings and scene condi tions, presenting challenges for effective adaptation. In this paper, we propose a test-time adaptation method for event based VFI to address the gap between the source and target domains. Our approach enables sequential learning in an online manner on the target domain, which only provides low-frame-rate videos. We present an approach that lever ages confident pixels as pseudo ground-truths, enabling stable and accurate online learning from low-frame-rate videos. Furthermore, to prevent overfitting during the con tinuous online process where the same scene is encountered repeatedly, we propose a method of blending historical sam ples with current scenes. Extensive experiments validate the effectiveness of our method, both in cross-domain and con tinuous domain shifting setups. The",
    "chosen": "This paper proposes a test-time adaptation method for event-based video frame interpolation. The approach uses confident pixels as pseudo ground-truths for stable online learning, blending historical samples to prevent overfitting. Experiments validate its effectiveness in cross-domain and continuous domain shifting setups.",
    "rejected": "TTA-EVF proposes a test-time adaptation method for event-based Video Frame Interpolation (VFI). It leverages confident pixels as pseudo ground-truths for online learning from low-frame-rate videos, blending historical samples to prevent overfitting. Extensive experiments validate its effectiveness in cross-domain and continuous domain shifting setups, enhancing VFI performance without offline data supply."
  },
  {
    "prompt": "summary: Each photo in an image burst can be considered a sam ple of a complex 3D scene: the product of parallax, diffuse and specular materials, scene motion, and illuminant vari ation. While decomposing all of these effects from a stack of misaligned images is a highly ill-conditioned task, the conventional align-and-merge burst pipeline takes the other extreme: blending them into a single image. In this work, we propose a versatile intermediate representation: a two layer alpha-composited image plus flow model constructed with neural spline fields – networks trained to map input coordinates to spline control points. Our method is able to, during test-time optimization, jointly fuse a burst image capture into one high-resolution reconstruction and decom pose it into transmission and obstruction layers. Then, by discarding the obstruction layer, we can perform a range of tasks including seeing through occlusions, reflection sup pression, and shadow removal. Tested on complex in-the wild captures we find that, with no post-processing steps or learned priors, our generalizable model is able to out perform existing dedicated single-image and multi-view ob struction removal approaches.",
    "chosen": "This paper introduces neural spline fields for burst image fusion and layer separation. The method constructs a two-layer alpha-composited image plus flow model, enabling tasks like occlusion removal and reflection suppression.",
    "rejected": "This work introduces a neural spline fields-based intermediate representation for burst photography, creating a two-layer alpha-composited image plus flow model. During test-time optimization, it fuses burst captures into a high-resolution reconstruction and decomposes them into transmission and obstruction layers. Discarding the obstruction layer facilitates tasks such as seeing through occlusions, reflection suppression, and shadow removal, outperforming existing single-image and multi-view approaches."
  },
  {
    "prompt": "summary: Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression anal ysis within tissue contexts. However, the high costs and methodological limitations of ST necessitate a more ro bust predictive model. In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs). TRIPLEX uniquely harnesses multi-resolution fea tures, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization. By integrating these features through an ef fective fusion strategy, TRIPLEX achieves accurate gene ex pression prediction. Our comprehensive benchmark study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC). The model’s predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX’s potential in advancing cancer diagnosis and treatment.",
    "chosen": "This paper presents TRIPLEX, a deep learning framework predicting spatial gene expression from whole slide images. TRIPLEX uses multi-resolution features and outperforms state-of-the-art models in prediction accuracy.",
    "rejected": "TRIPLEX introduces a deep learning framework for predicting spatial gene expression from Whole Slide Images (WSIs). By integrating multi-resolution features, it captures cellular morphology at individual spots, local context around these spots, and global tissue organization. TRIPLEX employs an effective fusion strategy to achieve accurate predictions, outperforming state-of-the-art models in MSE, MAE, and PCC on three public ST datasets and Visium data. The model's predictions closely align with ground truth gene expressions and tumor annotations, highlighting its potential for advancing cancer diagnosis and treatment."
  },
  {
    "prompt": "summary: Existing research based on deep learning has extensively explored the problem of daytime image dehazing. However, few studies have considered the characteristics of nighttime hazy scenes. There are two distinctions between nighttime and daytime haze. First, there may be multiple active col ored light sources with lower illumination intensity in night time scenes, which may cause haze, glow and noise with localized, coupled and frequency inconsistent characteris tics. Second, due to the domain discrepancy between simu lated and real-world data, unrealistic brightness may occur when applying a dehazing model trained on simulated data to real-world data. To address the above two issues, we propose a semi-supervised model for real-world nighttime dehazing. First, the spatial attention and frequency spec trum ﬁltering are implemented as a spatial-frequency do main information interaction module to handle the ﬁrst is sue. Second, a pseudo-label-based retraining strategy and a local window-based brightness loss for semi-supervised training process is designed to suppress haze and glow while achieving realistic brightness. Experiments on pub lic benchmarks validate the effectiveness of the proposed method and its superiority over state-of-the-art methods. ",
    "chosen": "This paper proposes a semi-supervised model for real-world nighttime dehazing. The method includes a spatial-frequency domain information interaction module and strategies to handle nighttime haze characteristics and domain discrepancies.",
    "rejected": "A novel semi-supervised model is proposed for real-world nighttime dehazing, addressing the unique challenges of nighttime hazy scenes such as localized glow, noise, and frequency inconsistencies due to multiple active light sources. The approach includes a spatial-frequency domain information interaction module and a pseudo-label-based retraining strategy with a local window-based brightness loss to ensure realistic brightness while suppressing haze and glow. Experiments on public benchmarks demonstrate the method's effectiveness and superiority over existing techniques, emphasizing its capability to handle complex nighttime imaging conditions effectively."
  },
  {
    "prompt": "summary: Recent advancements have shown the potential of lever aging both point clouds and images to localize anomalies. Nevertheless, their applicability in industrial manufactur ing is often constrained by significant drawbacks, such as the use of memory banks, which leads to a substantial in crease in terms of memory footprint and inference times. We propose a novel light and fast framework that learns to map features from one modality to the other on nominal samples and detect anomalies by pinpointing inconsisten cies between observed and mapped features. Extensive ex periments show that our approach achieves state-of-the-art detection and segmentation performance in both the stan dard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. Furthermore, we propose a layer pruning technique to improve memory and time efficiency with a marginal sacrifice in performance.",
    "chosen": "This paper presents a multimodal industrial anomaly detection framework that maps features from one modality to another on nominal samples. The approach detects anomalies by identifying inconsistencies between observed and mapped features. Experiments show state-of-the-art performance on the MVTec 3D-AD dataset with faster inference and lower memory usage than previous methods.",
    "rejected": "Recent research proposes a lightweight framework for anomaly detection in industrial settings that maps features between point clouds and images, detecting inconsistencies to identify anomalies. This method outperforms existing techniques in accuracy and speed on the MVTec 3D-AD dataset while using less memory."
  },
  {
    "prompt": "summary: Lifelong Person Re-identiﬁcation (L-ReID) aims to learn from sequentially collected data to match a person across different scenes. Once an L-ReID model is updated using new data, all historical images in the gallery are required to be re-calculated to obtain new features for testing, known as “re-indexing”. However, it is infeasible when raw images in the gallery are unavailable due to data privacy concerns, resulting in incompatible retrieval between the query and the gallery features calculated by different models, which causes signiﬁcant performance degradation. In this pa per, we focus on a new task called Re-indexing Free Life long Person Re-identiﬁcation (RFL-ReID), which requires achieving effective L-ReID without re-indexing raw images in the gallery. To this end, we propose a Continual Com patible Representation (C2R) method, which facilitates the query feature calculated by the continuously updated model to effectively retrieve the gallery feature calculated by the old model in a compatible manner. Speciﬁcally, we design a Continual Compatible Transfer (CCT) network to con tinuously transfer and consolidate the old gallery feature into the new feature space. Besides, a Balanced Compati ble Distillation module is introduced to achieve compatibil ity by aligning the transferred feature space with the new feature space. Finally, a Balanced Anti-forgetting Distilla tion module is proposed to eliminate the accumulated for getting of old knowledge during the continual compatible transfer. Extensive experiments on several benchmark L ReID datasets demonstrate the effectiveness of our method against state-of-the-art methods for both RFL-ReID and L ReID tasks.",
    "chosen": "This paper introduces the Re-indexing Free Lifelong Person Re-identification (RFL-ReID) task and proposes the Continual Compatible Representation (C2R) method. C2R facilitates effective retrieval without re-indexing raw images in the gallery, using a Continual Compatible Transfer (CCT) network and Balanced Compatible Distillation module. Experiments demonstrate effectiveness on benchmark datasets.",
    "rejected": "This paper introduces Re-indexing Free Lifelong Person Re-identification (RFL-ReID), aiming to eliminate the need for re-indexing gallery images during model updates. It presents a Continual Compatible Representation (C2R) approach to maintain retrieval compatibility across model updates without accessing original images, showing improved performance on benchmark datasets."
  },
  {
    "prompt": "summary: Recent advances in instruction tuning have led to the development of State-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these models, the impact of visual adversarial attacks on LMMs has not been thor oughly examined. We conduct a comprehensive study of the robustness of various LMMs against different adversarial attacks, evaluated across tasks including image classifica tion, image captioning, and Visual Question Answer (VQA). We find that in general LMMs are not robust to visual ad versarial inputs. However, our findings suggest that context provided to the model via prompts—such as questions in a QA pair—helps to mitigate the effects of visual adversarial inputs. Notably, the LMMs evaluated demonstrated remark able resilience to such attacks on the ScienceQA task with only an 8.10% drop in performance compared to their vi sual counterparts which dropped 99.73%. We also propose a new approach to real-world image classification which we term query decomposition. By incorporating existence queries into our input prompt we observe diminished at tack effectiveness and improvements in image classification accuracy. This research highlights a previously under ex plored facet of LMM robustness and sets the stage for future work aimed at strengthening the resilience of multimodal systems in adversarial environments.",
    "chosen": "This paper evaluates the robustness of large multimodal models (LMMs) against visual adversarial attacks across tasks like image classification and VQA. Findings indicate LMMs are not robust, but context via prompts mitigates effects. A query decomposition approach for image classification is proposed.",
    "rejected": "The study examines the robustness of Large Multimodal Models (LMMs) against visual adversarial attacks, finding that LMMs are generally not robust but can be mitigated with contextual prompts. The proposed query decomposition method enhances image classification accuracy under adversarial conditions."
  },
  {
    "prompt": "summary: Acquiring large-scale, well-annotated datasets is essen tial for training robust scene text detectors, yet the pro cess is often resource-intensive and time-consuming. While some efforts have been made to explore the synthesis of scene text images, a notable gap remains between syn thetic and authentic data. In this paper, we introduce a novel method that utilizes Neural Radiance Fields (NeRF) to model real-world scenes and emulate the data collec tion process by rendering images from diverse camera per spectives, enriching the variability and realism of the syn thesized data. A semi-supervised learning framework is proposed to categorize semantic regions within 3D scenes, ensuring consistent labeling of text regions across vari ous viewpoints. Our method also models the pose, and view-dependent appearance of text regions, thereby offer ing precise control over camera poses and significantly *Corresponding author. E-mail: lianzhouhui@pku.edu.cn This work was supported by National Natural Science Foundation of China (Grant No.: 62372015), Center For Chinese Font Design and Research, and Key Laboratory of Intelligent Press Media Technology. improving the realism of text insertion and editing within scenes. Employing our technique on real-world scenes has led to the creation of a novel scene text image dataset. Compared to other existing benchmarks, the proposed dataset is distinctive in providing not only standard annotations such as bounding boxes and transcriptions but also the information of 3D pose attributes for text regions, enabling a more detailed evaluation of the robustness of text detection algorithms. Through extensive experiments, we demonstrate the effec tiveness of our proposed method in enhancing the perfor mance of scene text detectors.",
    "chosen": "This paper proposes TextNeRF, a method using Neural Radiance Fields to synthesize scene text images by modeling real-world scenes and rendering images from diverse perspectives. A semi-supervised learning framework ensures consistent labeling of text regions. The method improves realism of text insertion and editing within scenes.",
    "rejected": "A novel method is introduced for synthesizing scene text images using Neural Radiance Fields (NeRF), providing enriched variability and realism. The technique includes a semi-supervised learning framework for consistent labeling across viewpoints, enhancing the training of scene text detectors through a new dataset."
  },
  {
    "prompt": "summary: The lifting of a 3D structure and camera from 2D land marks is at the cornerstone of the discipline of computer vision. Traditional methods have been confined to spe cific rigid objects, such as those in Perspective-n-Point (PnP) problems, but deep learning has expanded our ca pability to reconstruct a wide range of object classes (e.g. C3DPO [18] and PAUL [24]) with resilience to noise, oc clusions, and perspective distortions. However, all these techniques have been limited by the fundamental need to establish correspondences across the 3D training data, sig nificantly limiting their utility to applications where one has an abundance of “in-correspondence” 3D data. Our approach harnesses the inherent permutation equivariance of transformers to manage varying numbers of points per 3D data instance, withstands occlusions, and generalizes *Both authors advised equally. to unseen categories. We demonstrate state-of-the-art per formance across 2D-3D lifting task benchmarks. Since our approach can be trained across such a broad class of struc tures, we refer to it simply as a 3D Lifting Foundation Model (3D-LFM) – the first of its kind.",
    "chosen": "This paper introduces 3D-LFM, a 3D lifting foundation model leveraging transformer permutation equivariance for 2D-3D lifting tasks. The model handles varying points per 3D data instance, withstands occlusions, and generalizes to unseen categories, showing state-of-the-art performance.",
    "rejected": "This work develops a transformer-based approach called 3D Lifting Foundation Model (3D-LFM) for lifting 2D landmarks into 3D structures, demonstrating state-of-the-art performance across various benchmarks. The model's capability to handle occlusions and unseen categories marks a significant advancement in the field."
  },
  {
    "prompt": "summary: Advanced Audio-Visual Speech Recognition (AVSR) sys tems have been observed to be sensitive to missing video frames, performing even worse than single-modality mod els. While applying the common dropout techniques to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when deal ing with complete data input. In this study, we delve into this contrasting phenomenon through the lens of modality bias and uncover that an excessive modality bias towards the audio modality induced by dropout constitutes the fun damental cause. Next, we present the Modality Bias Hy pothesis (MBH) to systematically describe the relationship between the modality bias and the robustness against miss ing modality in multimodal systems. Building on these find ings, we propose a novel Multimodal Distribution Approxi mation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality, maintain ing performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effective ness of our proposed approach is evaluated through com prehensive experiments on the MISP2021 and MISP2022 datasets. Our",
    "chosen": "This paper studies dropout-induced modality bias in Audio-Visual Speech Recognition (AVSR) systems. It presents the Modality Bias Hypothesis (MBH) and proposes a Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on audio, maintaining performance and robustness.",
    "rejected": "The paper explores modality bias in Audio-Visual Speech Recognition (AVSR) systems, proposing a Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on audio modality, ensuring robustness against missing video frames without compromising performance on complete data."
  },
  {
    "prompt": "summary: Retrieval tasks play central roles in real-world machine learning systems such as search engine, recommender sys tem, and retrieval-augmented generation (RAG). Achieving decent performance in these tasks often requires fine-tuning various pretrained models on specific datasets and select ing the best candidate, a process that can be both time and resource consuming. To tackle the problem, we introduce a novel and efficient method, called RetMMD, that leverages Maximum Mean Discrepancy (MMD) and kernel methods to assess the transferability of pretrained models in retrieval tasks. RetMMD is calculated on pretrained model and tar get dataset without any fine-tuning involved. Specifically, given some query, we quantify the distribution discrepancy between relevant and irrelevant document embeddings, by estimating the similarities within their mappings in the fine tuned embedding space through kernel method. This discrep ancy is averaged over multiple queries, taking into account the distribution characteristics of the target dataset. Ex periments suggest that the proposed metric calculated on pretrained models closely aligns with retrieval performance post fine-tuning. The observation holds across a variety of datasets, including image, text, and multi-modal domains, indicating the potential of using MMD and kernel meth ods for transfer learning evaluation in retrieval scenarios. In addition, we also design a way of evaluating dataset transferability for retrieval tasks, with experimental results demonstrating the effectiveness of the proposed approach.",
    "chosen": "This paper introduces RetMMD, a method using Maximum Mean Discrepancy (MMD) and kernel methods to assess pretrained model transferability in retrieval tasks. Experiments show the metric aligns with retrieval performance post fine-tuning across various datasets.",
    "rejected": "A novel metric named RetMMD is introduced for evaluating transferability of pretrained models in retrieval tasks, leveraging Maximum Mean Discrepancy (MMD) and kernel methods. Experiments demonstrate that RetMMD aligns closely with retrieval performance post fine-tuning, offering a promising evaluation approach across various domains."
  },
  {
    "prompt": "summary: It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Dis play (HMD) such as Meta Quest and PICO. In this pa per, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observa tions from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of in puts may accommodate users’ choices for both high track ing accuracy and easy-to-wear. A lightweight temporal spatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape es timation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-the art results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser’s on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our code and free-dancing motion dataset are avail able here.",
    "chosen": "This paper proposes HMD-Poser, a unified approach for real-time full-body motion tracking on VR HMDs using sparse observations from HMD and body-worn IMUs. The approach supports various input scenarios and includes a lightweight network and online body shape estimation.",
    "rejected": "HMD-Poser is presented as the first unified approach for real-time full-body motion tracking using sparse observations from HMD and body-worn IMUs. The lightweight temporal-spatial feature learning network ensures real-time operation on standalone VR headsets, achieving state-of-the-art results in both accuracy and performance."
  },
  {
    "prompt": "summary: Generative models have been very popular in the recent years for their image generation capabilities. GAN-based models are highly regarded for their disentangled latent space, which is a key feature contributing to their success in controlled image editing. On the other hand, diffusion models have emerged as powerful tools for generating high quality images. However, the latent space of diffusion mod els is not as thoroughly explored or understood. Existing methods that aim to explore the latent space of diffusion models usually relies on text prompts to pinpoint specific semantics. However, this approach may be restrictive in ar eas such as art, fashion, or specialized fields like medicine, where suitable text prompts might not be available or easy to conceive thus limiting the scope of existing work. In this paper, we propose an unsupervised method to discover la tent semantics in text-to-image diffusion models without re This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 24209",
    "chosen": "This paper presents NoiseCLR, a contrastive learning approach for unsupervised discovery of interpretable semantics in the latent space of diffusion models without relying on text prompts.",
    "rejected": "This paper explores the latent space of text-to-image diffusion models, aiming to discover latent semantics without relying on restrictive text prompts. Existing methods often depend on text inputs to pinpoint specific semantics, which can be limiting in specialized fields like art, fashion, or medicine where suitable text prompts may not be readily available. The proposed unsupervised method aims to address this limitation by discovering meaningful latent variables directly from the data, enhancing the applicability and flexibility of diffusion models in generating high-quality images across various domains."
  },
  {
    "prompt": "summary: We introduce a new family of minimal problems for re construction from multiple views. Our primary focus is a novel approach to autocalibration, a long-standing prob lem in computer vision. Traditional approaches to this problem, such as those based on Kruppa’s equations or the modulus constraint, rely explicitly on the knowledge of multiple fundamental matrices or a projective reconstruc tion. In contrast, we consider a novel formulation involv ing constraints on image points, the unknown depths of 3D points, and a partially specified calibration matrix K. For 2 and 3 views, we present a comprehensive taxonomy of minimal autocalibration problems obtained by relaxing some of these constraints. These problems are organized into classes according to the number of views and any as sumed prior knowledge of K. Within each class, we deter mine problems with the fewest—or a relatively small num ber of—solutions. From this zoo of problems, we devise three practical solvers. Experiments with synthetic and real data and interfacing our solvers with COLMAP demon strate that we achieve superior accuracy compared to state of-the-art calibration methods. The",
    "chosen": "This paper introduces a new family of minimal problems for reconstruction from multiple views, focusing on autocalibration. The approach involves constraints on image points, unknown depths, and a partially specified calibration matrix K, with practical solvers developed for minimal autocalibration problems.",
    "rejected": "The research introduces a novel family of minimal problems for autocalibration in multi-view reconstruction, focusing on a new approach that relaxes traditional constraints such as those based on Kruppa’s equations or modulus constraint. By formulating the problem with constraints on image points, unknown depths of 3D points, and partially specified calibration matrices, the study presents a comprehensive taxonomy of minimal autocalibration problems for 2 and 3 views. Practical solvers developed from this framework demonstrate superior accuracy compared to state-of-the-art methods when tested on both synthetic and real datasets. The approach also integrates seamlessly with existing tools like COLMAP, showcasing its practical utility and robustness."
  },
  {
    "prompt": "summary: In this work, we tackle the problem of domain gener alization for object detection, specifically focusing on the scenario where only a single source domain is available. We propose an effective approach that involves two key steps: diversifying the source domain and aligning detec tions based on class prediction confidence and localiza tion. Firstly, we demonstrate that by carefully selecting a set of augmentations, a base detector can outperform ex isting methods for single domain generalization by a good margin. This highlights the importance of domain diversi fication in improving the performance of object detectors. Secondly, we introduce a method to align detections from multiple views, considering both classification and localiza tion outputs. This alignment procedure leads to better gen eralized and well-calibrated object detector models, which are crucial for accurate decision-making in safety-critical applications. Our approach is detector-agnostic and can be seamlessly applied to both single-stage and two-stage detectors. To validate the effectiveness of our proposed methods, we conduct extensive experiments and ablations on challenging domain-shift scenarios. The results con sistently demonstrate the superiority of our approach com pared to existing methods.",
    "chosen": "This paper proposes a method to improve domain generalization for object detection from a single source domain. The approach involves diversifying the source domain through careful augmentation selection and aligning detections based on class prediction confidence and localization. Experiments show superior performance compared to existing methods.",
    "rejected": "In this work, the authors tackle domain generalization for object detection with a single source domain. They introduce diversification through augmentations and align detections based on class prediction confidence and localization, leading to better-generalized models. The approach is validated on challenging domain-shift scenarios, showing superior performance."
  },
  {
    "prompt": "summary: The existing facial datasets, while having plentiful im ages at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with proﬁle or pitched faces. This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k high quality images of faces at extreme poses. To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly avail able datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings. Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3D aware GAN, diffusion-based text-to-image face generation, and face reenactment. Speciﬁcally, training with EFHQ helps models generalize well across diverse poses, signif icantly improving performance in scenarios involving ex treme views, conﬁrmed by extensive experiments. Addition ally, we utilize EFHQ to deﬁne a challenging cross-view face veriﬁcation benchmark, in which the performance of SOTA face recognition models drops 5-37% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild.",
    "chosen": "This paper introduces EFHQ, a large-scale dataset of extreme pose face images. Created by curating existing datasets, EFHQ helps improve model performance on profile and pitched faces. The dataset also defines a challenging cross-view face verification benchmark.",
    "rejected": "This paper introduces EFHQ, a dataset containing 450k high-quality images of faces at extreme poses, curated from existing datasets. Training with EFHQ improves model generalizability across diverse poses, especially in extreme view scenarios. Additionally, a cross-view face verification benchmark using EFHQ reveals significant drops in SOTA models' performance compared to frontal-face scenarios."
  },
  {
    "prompt": "summary: Modeling and visualizing relationships between tasks or datasets is an important step towards solving various meta-tasks such as dataset discovery, multi-tasking, and transfer learning. However, many relationships, such as containment and transferability, are naturally asymmetric and current approaches for representation and visualiza tion (e.g., t-SNE [44]) do not readily support this. We propose TASK2BOX, an approach to represent tasks us ing box embeddings—axis-aligned hyperrectangles in low dimensional spaces—that can capture asymmetric relation ships between them through volumetric overlaps. We show that TASK2BOX accurately predicts unseen hierarchical relationships between nodes in ImageNet and iNaturalist datasets, as well as transferability between tasks in the Taskonomy benchmark. We also show that box embed dings estimated from task representations (e.g., CLIP [36], Task2Vec [4], or attribute based [15]) can be used to pre dict relationships between unseen tasks more accurately than classifiers trained on the same representations, as well as handcrafted asymmetric distances (e.g., KL divergence). This suggests that low-dimensional box embeddings can effectively capture these task relationships and have the added advantage of being interpretable. We use the ap proach to visualize relationships among publicly available image classification datasets on popular dataset hosting platform called Hugging Face.",
    "chosen": "This paper presents TASK2BOX, an approach using box embeddings to model asymmetric relationships between tasks. The method accurately predicts hierarchical relationships and transferability between tasks, offering an interpretable way to visualize task relationships.",
    "rejected": "TASK2BOX proposes representing tasks as box embeddings to capture asymmetric relationships between them. This method accurately predicts unseen hierarchical and transferability relationships among tasks, outperforming classifiers trained on the same representations. Visualization of image classification datasets on Hugging Face demonstrates its effectiveness."
  },
  {
    "prompt": "summary: Reconstructing dynamic objects from monocular videos is a severely underconstrained and challenging problem, and recent work has approached it in various directions. However, owing to the ill-posed nature of this problem, there has been no solution that can provide consistent, high quality novel views from camera positions that are signifi cantly different from the training views. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on this challenge by imposing a two-stage approach: first, we fit a low-rank neural deformation model, which then is used as regularization for non-rigid reconstruction in the second stage. The first stage learns the object’s deformations such that it preserves consistency in novel views. The second stage obtains high reconstruction quality by optimizing 3D Gaussians that are driven by the coarse model. To this end, we introduce a local 3D Gaussian representation, where temporally shared Gaussians are anchored in and deformed by local oriented volumes. The resulting combined model can be rendered as radiance fields, resulting in high-quality photo-realistic reconstructions of the non-rigidly deforming objects. We demonstrate that NPGs achieve superior results compared to previous works, especially in challenging sce narios with few multi-view cues.1 1",
    "chosen": "This paper proposes Neural Parametric Gaussians (NPGs) for reconstructing dynamic objects from monocular videos. The two-stage approach first fits a deformation model, then refines it with 3D Gaussians, achieving high-quality reconstructions of non-rigidly deforming objects.",
    "rejected": "NPGs address dynamic object reconstruction from monocular videos by imposing a two-stage approach: fitting a low-rank neural deformation model followed by optimizing 3D Gaussians. This method achieves high-quality photo-realistic reconstructions, demonstrating superiority in scenarios with few multi-view cues."
  },
  {
    "prompt": "summary: Even the best current algorithms for estimating body 3D shape and pose yield results that include body self intersections. In this paper, we present CLOAF, which exploits the diffeomorphic nature of Ordinary Differential Equations to eliminate such self-intersections while still im posing body shape constraints. We show that, unlike earlier approaches to addressing this issue, ours completely elim inates the self-intersections without compromising the ac curacy of the reconstructions. Being differentiable, CLOAF can be used to ﬁne-tune pose and shape estimation base lines to improve their overall performance and eliminate self-intersections in their predictions. Furthermore, we demonstrate how our CLOAF strategy can be applied to practically any motion ﬁeld induced by the user. CLOAF also makes it possible to edit motion to interact with the environment without worrying about potential collision or loss of body-shape prior.",
    "chosen": "This paper presents CLOAF, a method using Ordinary Differential Equations to eliminate self-intersections in human body 3D shape and pose estimation. The differentiable approach improves reconstruction accuracy without explicit detection steps.",
    "rejected": "CLOAF uses differential equations to eliminate body self-intersections while preserving shape constraints during 3D human pose estimation. Unlike iterative methods that minimize interpenetration loss, CLOAF ensures no self-intersections without compromising accuracy, making it suitable for realistic animation and robotics applications."
  },
  {
    "prompt": "summary: We present 3D Paintbrush, a technique for automati cally texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly inte grate into standard graphics pipelines. We opt to simultane ously produce a localization map (to specify the edit region) and a texture map which conforms to it. This approach im proves the quality of both the localization and the styliza tion. To enhance the details and resolution of the textured area, we leverage multiple stages of a cascaded diffusion model to supervise our local editing technique with gen erative priors learned from images at different resolutions. Our technique, referred to as Cascaded Score Distillation (CSD), simultaneously distills scores at multiple resolutions in a cascaded fashion, enabling control over both the gran ularity and global understanding of the supervision. We demonstrate the effectiveness of 3D Paintbrush to locally texture different semantic regions on a variety of shapes.",
    "chosen": "This paper introduces 3D Paintbrush, a technique for texturing local semantic regions on meshes via text descriptions. Using cascaded diffusion models, the method produces high-quality texture maps that integrate into standard graphics pipelines.",
    "rejected": "3D Paintbrush presents a technique for texturing local semantic regions on meshes via text descriptions. By leveraging cascaded diffusion models, it simultaneously produces localization and texture maps, enhancing detail and resolution. The method allows for intuitive control over edits, seamlessly integrating into standard graphics pipelines."
  },
  {
    "prompt": "summary: When building classification systems with demographic fairness considerations, there are two objectives to satisfy: 1) maximizing utility for the specific task and 2) ensuring fairness w.r.t. a known demographic attribute. These ob jectives often compete, so optimizing both can lead to a trade-off between utility and fairness. While existing works acknowledge the trade-offs and study their limits, two ques tions remain unanswered: 1) What are the optimal trade offs between utility and fairness? and 2) How can we nu merically quantify these trade-offs from data for a desired prediction task and demographic attribute of interest? This paper addresses these questions. We introduce two utility fairness trade-offs: the Data-Space and Label-Space Trade off. The trade-offs reveal three regions within the utility fairness plane, delineating what is fully and partially pos sible and impossible. We propose U-FaTE, a method to nu merically quantify the trade-offs for a given prediction task and group fairness definition from data samples. Based on the trade-offs, we introduce a new scheme for evaluating representations. An extensive evaluation of fair representa tion learning methods and representations from over 1000 pre-trained models revealed that most current approaches are far from the estimated and achievable fairness-utility trade-offs across multiple datasets and prediction tasks.",
    "chosen": "This paper addresses the utility-fairness trade-offs in classification systems with demographic fairness considerations. The authors introduce methods to quantify these trade-offs and evaluate fair representation learning methods.",
    "rejected": "The paper discusses utility-fairness trade-offs in classification systems considering demographic fairness. It introduces Data-Space and Label-Space Trade-offs, delineating what is possible, partially possible, and impossible. U-FaTE quantifies these trade-offs for given tasks and demographic attributes, revealing that most current approaches fall short of achievable fairness-utility balances."
  },
  {
    "prompt": "summary: Estimating large, extreme inter-image rotations is crit ical for numerous computer vision domains involving im ages related by limited or non-overlapping fields of view. In this work, we propose an attention-based approach with a pipeline of novel algorithmic components. First, as ro tation estimation pertains to image pairs, we introduce an inter-image distillation scheme using Decoders to improve embeddings. Second, whereas contemporary methods com pute a 4D correlation volume (4DCV) encoding inter-image relationships, we propose an Encoder-based cross-attention approach between activation maps to compute an enhanced equivalent of the 4DCV. Finally, we present a cascaded Decoder-based technique for alternately refining the cross attention and the rotation query. Our approach outperforms current state-of-the-art methods on extreme rotation estima tion. We make our code publicly available1.",
    "chosen": "This paper proposes an attention-based approach with novel components for estimating extreme 3D image rotations. The method uses inter-image distillation and cross-attention to outperform state-of-the-art rotation estimation techniques.",
    "rejected": "An attention-based approach for estimating large inter-image rotations is proposed, involving inter-image distillation and Encoder-based cross-attention to enhance 4D correlation volume computation. A cascaded Decoder-based refinement technique alternately refines cross-attention and rotation query, outperforming state-of-the-art methods on extreme rotation estimation."
  },
  {
    "prompt": "summary: Existing 3D scene understanding methods are heavily fo cused on 3D semantic and instance segmentation. How ever, identifying objects and their parts only constitutes an intermediate step towards a more ﬁne-grained goal, which is effectively interacting with the functional interactive ele ments (e.g., handles, knobs, buttons) in the scene to accom plish diverse tasks. To this end, we introduce SceneFun3D, a large-scale dataset with more than 14.8k highly accu rate interaction annotations for 710 high-resolution real world 3D indoor scenes. We accompany the annotations with motion parameter information, describing how to in teract with these elements, and a diverse set of natural lan guage descriptions of tasks that involve manipulating them in the scene context. To showcase the value of our dataset, we introduce three novel tasks, namely functionality seg mentation, task-driven affordance grounding and 3D mo tion estimation, and adapt existing state-of-the-art methods to tackle them. Our experiments show that solving these tasks in real 3D scenes remains challenging despite recent progress in closed-set and open-set 3D scene understanding methods.",
    "chosen": "This paper introduces SceneFun3D, a dataset with detailed interaction annotations for 3D indoor scenes. The dataset supports tasks like functionality segmentation and 3D motion estimation, highlighting challenges in real-world scene understanding.",
    "rejected": "SceneFun3D introduces a dataset with interaction annotations for real-world 3D indoor scenes, including motion parameters and natural language task descriptions. Novel tasks such as functionality segmentation and task-driven affordance grounding are introduced, showcasing the complexity of interacting with functional elements in 3D environments despite recent progress in scene understanding."
  },
  {
    "prompt": "summary: Recently, building on the foundation of neural radiance ﬁeld, various techniques have emerged to learn unsigned distance ﬁelds (UDF) to reconstruct 3D non-watertight models from multi-view images. Yet, a central challenge in UDF-based volume rendering is formulating a proper way to convert unsigned distance values into volume den sity, ensuring that the resulting weight function remains un biased and sensitive to occlusions. Falling short on these requirements often results in incorrect topology or large reconstruction errors in resulting models. This paper ad dresses this challenge by presenting a novel two-stage al gorithm, 2S-UDF, for learning a high-quality UDF from multi-view images. Initially, the method applies an easily trainable density function that, while slightly biased and transparent, aids in coarse reconstruction. The subsequent stage then reﬁnes the geometry and appearance of the ob ject to achieve a high-quality reconstruction by directly ad justing the weight function used in volume rendering to en sure that it is unbiased and occlusion-aware. Decoupling density and weight in two stages makes our training sta ble and robust, distinguishing our technique from existing UDF learning approaches. Evaluations on the DeepFash ion3D, DTU, and BlendedMVS datasets validate the ro bustness and effectiveness of our proposed approach. In both quantitative metrics and visual quality, the results in dicate our superior performance over other UDF learn ing techniques in reconstructing 3D non-watertight models from multi-view images. Our",
    "chosen": "This paper presents 2S-UDF, a two-stage algorithm for learning unsigned distance fields from multi-view images. The method improves reconstruction of non-watertight models by decoupling density and weight functions, ensuring unbiased and occlusion-aware results.",
    "rejected": "Building on neural radiance fields, this work explores unsigned distance fields (UDFs) for reconstructing non-watertight 3D models. Challenges in converting unsigned distances to volume density are addressed to ensure unbiased weight functions sensitive to occlusions, aiming to improve topology correctness and reduce reconstruction errors."
  }
]