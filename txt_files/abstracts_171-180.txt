Abstract
Utilizing multi-view inputs to synthesize novel-view im-
ages, Neural Radiance Fields (NeRF) have emerged as a
popular research topic in 3D vision. In this work, we in-
troduce a Generalizable Semantic Neural Radiance Fields
(GSNeRF), which uniquely takes image semantics into the
synthesis process so that both novel view image and the as-
sociated semantic maps can be produced for unseen scenes.
Our GSNeRF is composed of two stages: Semantic Geo-
Reasoning and Depth-Guided Visual rendering. The former
is able to observe multi-view image inputs to extract se-
mantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs
both image and semantic rendering with improved perfor-
mances. Our experiments not only confirm that GSNeRF
performs favorably against prior works on both novel-view
image and semantic segmentation synthesis but the effec-
tiveness of our sampling strategy for visual rendering is fur-
ther verified.
1. Introduction
3D scene understanding plays a pivotal role in many vision-
related tasks, including 3D reconstruction and 3D reason-
ing. The former domain focuses on low-level understand-
ing, including developing efficient models that interpret
and reconstruct observations from various data sources,
such as RGB, RGBD, and physical sensors. In contrast,
the latter emphasizes high-level understanding, like encod-
ing, segmentation, and common sense knowledge learning.
For real-world applications like robotic navigation or aug-
mented reality, both these facets - reconstruction and rea-
soning - are crucial to enhance interactions with the real
world [10].
Among the challenges in the reconstruction
domain, novel view synthesis has consistently been a chal-
lenging objective. Being able to generate a novel view im-
age based on a few existing views shows that a model com-
*Equal Contribution
prehends the scene’s geometry and possesses a foundational
understanding akin to reconstruction.
The Neural Radiance Field (NeRF) [1, 4, 8, 19–22, 28,
35] has recently risen as an exciting research area, provid-
ing a novel way to tackle the task of novel view synthesis.
By encoding the density and emitted radiance at each spatial
location, NeRF is able to compress a scene into a learnable
model given several images and the corresponding camera
poses of the scene. By incorporating a volumetric rendering
skill, images of unseen camera views can be generated with
convincing quality. However, NeRF primarily focuses on
reconstructing the color information of novel views and un-
derstanding the associated high-level semantic information
(e.g., semantic segmentation or object detection) remains a
significant challenge.
Recent works [9, 15, 25, 30, 31, 37] aim to amplify the
high-level scene understanding ability of NeRF by integrat-
ing NeRF with semantic segmentation. By sharing the in-
formation between the semantic object class and their cor-
responding appearances, these two tasks are capable of ben-
efiting from each other’s insights [17]. Nevertheless, these
semantic understanding methods stick closely to the orig-
inal NeRF paradigm, focusing on exploiting the model to
represent a specific scene. As mentioned in [17], such a
strategy requires additional annotations of semantic seg-
mentation maps when applied to a new scene, limiting real-
world applicability and generalizability.
To tackle this problem, generalizable NeRFs [3, 14, 18,
27, 32, 33, 36] have emerged as a promising solution. They
adopt an on-the-fly approach for building a neural radiance
field conditioned on extracted features from input images of
different scenes rather than encoding the scene representa-
tion directly into the model. For instance, PixelNeRF [36]
introduces the idea of conditioning a NeRF model with
multi-view images across multiple scenes. This innovation
boosts NeRF’s ability to generalize to unseen scenes and
avoids retraining for each individual scene. Yet, it leaves
uncertainties in its application to semantic understanding.
As a pioneer in integrating a generalizable NeRF
with semantic segmentation capabilities, Semantic-Ray (S-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20806
Abstract
Deep Neural Networks (DNNs) are widely used for vi-
sual classiﬁcation tasks, but their complex computation pro-
cess and black-box nature hinder decision transparency and
interpretability. Class activation maps (CAMs) and recent
variants provide ways to visually explain the DNN decision-
making process by displaying ‘attention’ heatmaps of the
DNNs. Nevertheless, the CAM explanation only offers rela-
tive attention information, that is, on an attention heatmap,
we can interpret which image region is more or less im-
portant than the others.
However, these regions cannot
be meaningfully compared across classes, and the contri-
bution of each region to the model’s class prediction is
not revealed. To address these challenges that ultimately
lead to better DNN Interpretation, in this paper, we pro-
pose CAPE, a novel reformulation of CAM that provides a
uniﬁed and probabilistically meaningful assessment of the
contributions of image regions. We quantitatively and qual-
itatively compare CAPE with state-of-the-art CAM methods
on CUB and ImageNet benchmark datasets to demonstrate
enhanced interpretability. We also test on a cytology imag-
ing dataset depicting a challenging Chronic Myelomono-
cytic Leukemia (CMML) diagnosis problem. Code is avail-
able at: https://github.com/AIML-MED/CAPE.
1. Introduction
Deep neural networks (DNNs), despite achieving supe-
rior performance on various tasks such as computer vision
and natural language processing, are known to be black
boxes [23] that lack the ability to explain their decision-
making process.
The black-box nature is commonly re-
garded as a result of the complex model structure charac-
terized by stacked computation layers, involving non-linear
functions and many model parameters. Explainable DNN
decisions are crucial to many life-critical scenarios [26]
†Corresponding author.
such as AI-powered autonomous driving and medical diag-
nostics. Taking the example of healthcare applications [2],
decision transparency is critical for doctors to understand
and trust AI analysis, and to use AI to make insightful and
accurate diagnoses or decisions.
DNN interpretability is an emerging and actively stud-
ied research ﬁeld. For visual classiﬁcation tasks, a com-
mon type of DNN interpretability analysis is to explain
DNN outputs via ﬁnding and displaying model attention
on the input image, i.e., identifying which image regions
the model focused on during the decision-making process.
This type of visual explanation can be achieved via methods
of gradient-based attention visualization [25], perturbation-
based input manipulation [6, 21], and class activation map
(CAM)-based visualization [11, 24]. In particular, CAM
is an inherent intermediate step of DNN prediction which
represents the actual region relevance produced by the net-
work. CAM stands out due to its efﬁcient feedforward pro-
cess, yet its attention values can not directly explain and
compose model outcomes. Speciﬁcally, CAM values are
class-wise relative probabilities. They only represent the
relative region importance compared to the highest attention
value within each class map. Thus, CAM values provide a
limited explanation within the context of one target class.
This means that they are incomparable between classes, and
cannot explain image-level predictions. Take the CAM vi-
sualization in Fig. 1 as an example, CAM assigns similar
attention values to two dog breed classes Siberian Husky
and Alaskan Malamute. Differencing the two CAM maps
between the breeds fails to yield meaningful comparisons.
The limited analytical capability of current CAM-based
approaches hinders their use in many downstream applica-
tions. For example, ﬁne-grained classiﬁcation analysis re-
quires the model’s ability to discriminate regions between
closely related concepts.
In addition, for tasks such as
weakly supervised semantic segmentation, CAM threshold-
ing is employed to initialize a segmentation method [13] but
the threshold choice is often arbitrary without any semantic
meaning attached.
In this paper, we reformulate CAM as a Probabilistic
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11072
Abstract
Music is a universal language that can communicate
emotions and feelings.
It forms an essential part of the
whole spectrum of creative media, ranging from movies to
social media posts. Machine learning models that can syn-
thesize music are predominantly conditioned on textual de-
scriptions of it. Inspired by how musicians compose music
not just from a movie script, but also through visualizations,
we propose MELFUSION, a model that can effectively use
cues from a textual description and the corresponding im-
age to synthesize music. MELFUSION is a text-to-music
diffusion model with a novel “visual synapse”, which effec-
tively infuses the semantics from the visual modality into the
generated music. To facilitate research in this area, we in-
troduce a new dataset MeLBench, and propose a new eval-
uation metric IMSM. Our exhaustive experimental evalua-
tion suggests that adding visual information to the music
synthesis pipeline significantly improves the quality of gen-
erated music, measured both objectively and subjectively,
with a relative gain of up to 67.98% on the FAD score. We
hope that our work will gather attention to this pragmatic,
yet relatively under-explored research area.
1. Introduction
Music is an essential tool for creative professionals and con-
tent creators. It can complement and set the mood for an
accompanying still image, animation, video, or even text
descriptions while creating a social media post. Finding
music that matches a specific setting, can indeed be an ar-
duous task. A conditional music generation approach, that
can synthesize a music track by analyzing the visual content
and the textual description can find a wide range of practical
∗Equal contribution.
†Work done during internship at Adobe Research.
MELFUSION
(Ours)
Long Caption
A tranquil scenery is captured with the view of the
night sky just before sunrise. The sky consists of
dynamic spiralling clouds which symbolizes
movement and aliveness. The stars are bright and
prominent with strokes of yellows and whites
represent a vivid yet peaceful moment. The village
in the scene has houses whose windows emit warm
and glowing light, giving a contrast to the cool,
celestial tones of sky depicting pleasant emotions.
OVL: 83.86
Caption
Generator
A soft musical track of folk
acoustic genre played on violin.
OVL: 88.45
We introduce a new
problem setting and an
approach to synthesize
music, conditioned on both
visual and textual
modalities.
A soft musical track of folk acoustic genre played
on violin.
An
Alternate
Approach
Text-to-Music
Figure 1.
We present MELFUSION, a music diffusion model
equipped with a novel “visual synapse”, that can effectively in-
fuse image semantics into a text-to-music diffusion model. This
task indeed requires a detailed understanding of the concepts in
the image. An alternate approach like using a caption generator to
convert image to text space to be further used with existing text-to-
music methods leads to a sub-optimal overall audio quality (OVL)
score. Our approach can knit together complementary information
from both modalities to synthesize high-quality music.
applications in various fields including social media.
Inspired by the progress in generative modeling of im-
ages, music generation has also garnered significant atten-
tion from the community [1, 30, 42]. Recently, Agostinelli
et al. [1], Copet et al. [4] proposed conditioning in the form
of melody or humming. While Sheffer and Adi [43] pursue
image-guided audio generation. Despite these efforts, mu-
sic generation conditioned on multiple modalities like text
and image, is largely uncharted.
Images are more expressive [13] than text-only infor-
mation and capture more fine-grained semantic information
about various visual aspects. For example, as depicted in
Fig 1, to generate a musical track that goes well with a
given image, without indeed using it, one has to make the
tedious effort of producing long, descriptive captions (either
generated by an image captioning model or human annota-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26826
Abstract
Open-vocabulary semantic segmentation presents the
challenge of labeling each pixel within an image based on a
wide range of text descriptions. In this work, we introduce a
novel cost-based approach to adapt vision-language foun-
dation models, notably CLIP, for the intricate task of se-
mantic segmentation. Through aggregating the cosine sim-
ilarity score, i.e., the cost volume between image and text
embeddings, our method potently adapts CLIP for segment-
ing seen and unseen classes by fine-tuning its encoders, ad-
dressing the challenges faced by existing methods in han-
dling unseen classes. Building upon this, we explore meth-
ods to effectively aggregate the cost volume considering its
multi-modal nature of being established between image and
text embeddings. Furthermore, we examine various meth-
ods for efficiently fine-tuning CLIP.
1. Introduction
Open-vocabulary semantic segmentation aims to assign
each pixel in an image to a class label from an unbounded
range, defined by text descriptions.
To handle the chal-
lenge of associating an image with a wide variety of text
descriptions, pre-trained vision-language foundation mod-
els, e.g., CLIP [43] and ALIGN [22], have drawn attention
as they exerted strong open-vocabulary recognition capa-
bilities achieved through training on extensive image-text
datasets.
Nonetheless, these foundation models primar-
ily receive image-level supervision during training, which
introduces a notable disparity when applying them to the
pixel-level segmentation tasks [66].
To address this gap, recent works [9, 14, 30, 55–57, 60]
have reformulated the task into a region-level problem by
utilizing mask proposal generators.
While this partially
bridges the discrepancy between the pre-training and the
∗Equal contribution. †Corresponding authors.
0
10
20
30
40
mIoU on ADE20K
🔥
❄
🔥
❄
Feature Agg.
Cost Agg.
(a) mIoU of seen classes
0
10
20
30
40
mIoU on ADE20K
🔥
❄
🔥
❄
Feature Agg.
Cost Agg.
🔥: Fine-tuned CLIP
❄: Frozen CLIP
(b) mIoU of unseen classes
Figure 1. Comparison between feature and cost aggregation
for open-vocabulary semantic segmentation task. In contrast
to feature aggregation suffering severe overfitting to seen classes,
cost aggregation can generalize to unseen classes and achieve sig-
nificant performance improvements upon fine-tuning of CLIP.
downstream task, a discernible gap persists between the
conceptualization of regions and the entire image for CLIP.
In this work, we investigate methods to transfer the holis-
tic understanding capability of images to the pixel-level
task of segmentation.
While a straightforward approach
would be to fine-tune the encoders of CLIP, existing meth-
ods struggle in such attempt [57, 60, 66] as they encounter
significant overfitting problems to the seen classes. This
results in the misalignment of the joint embedding space
for unseen classes, as the CLIP features undergo decoder
modules for aggregating them into segmentation masks,
hence losing their alignment. Consequently, most meth-
ods [9, 14, 30, 55–57, 60] opt for freezing the encoders of
CLIP instead, remaining the challenge underexplored.
In this regard, we extend the exploration of adapting
CLIP for open-vocabulary semantic segmentation and in-
troduce a novel cost-based framework. We propose to ag-
gregate the cosine similarity between image and text em-
beddings of CLIP, i.e., the matching cost, drawing parallels
to the visual correspondence literature [26]. Surprisingly,
we find that fine-tuning CLIP upon this framework effec-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4113
Abstract
We present a new open-vocabulary detection framework.
Our framework uses both image-level labels and detailed
detection annotations when available. Our framework pro-
ceeds in three steps. We first train a language-conditioned
object detector on fully-supervised detection data. This de-
tector gets to see the presence or absence of ground truth
classes during training, and conditions prediction on the
set of present classes.
We use this detector to pseudo-
label images with image-level labels. Our detector pro-
vides much more accurate pseudo-labels than prior ap-
proaches with its conditioning mechanism.
Finally, we
train an unconditioned open-vocabulary detector on the
pseudo-annotated images. The resulting detector, named
DECOLA, shows strong zero-shot performance in open-
vocabulary LVIS benchmark as well as direct zero-shot trans-
fer benchmarks on LVIS, COCO, Object365, and OpenIm-
ages. DECOLA outperforms the prior arts by 17.1 APrare
and 9.4 mAP on zero-shot LVIS benchmark.
DECOLA
achieves state-of-the-art results in various model sizes, ar-
chitectures, and datasets by only training on open-sourced
data and academic-scale computing. Code is available at
https://github.com/janghyuncho/DECOLA.
1. Introduction
Object detection has seen immense progress over the past
decade. Classical object detectors reason over datasets of
fixed predefined classes. This simplifies the design, train-
ing, and evaluation of new methods, and allows for rapid
prototyping [2–5, 17, 18, 25, 37, 42, 60, 70, 72, 75]. How-
ever, it complicates deployment to downstream applica-
tions too. A classical detector requires a new dataset to
further finetune for every new concept it encounters. Col-
lecting sufficient data for every new concept is not scal-
able [20].
Open-vocabulary detection offers an alterna-
tive [19, 43, 61, 65, 66, 73]. Open-vocabulary detectors
reason about any arbitrary concept with free-form text,
using the generalization ability of vision-language mod-
els. Yet, common open-vocabulary detectors reuse clas-
sical detectors and either replace the last classification layer
DECOLA
Language-conditioned Detections
Image-level tags: 
“cat”, “cola”, “mentos”
text encoder
conditioned
Standard 
OV Detector
image-text dataset of size N
filtering & argmax
select scores
training data with pseudo-labels
argmax
x N
training data with pseudo-labels
image-text dataset of size N
x N
x N
x N
(a) Standard self-training
(b) DECOLA self-training
Figure 1. An illustration of how standard open-vocabulary detec-
tors and DECOLA generate pseudo-labels using image-level data.
Standard detectors use image-level information later in the pipeline
after initial box proposals, which may result in low coverage of
unseen classes (e.g., “mentos” and “cola”). DECOLA adjusts the
prediction to the information and ensures sufficient coverage.
with [19, 43, 66, 73], or fuse box feature with [31, 65] text
representation from pretrained vision-language model. The
inner workings of the detector remain unchanged.
In this paper, we introduce a transformer-based object
detector that adjusts its inner workings to any arbitrary set
of concepts represented in language. The detector considers
only the queried set of concepts as foreground and disre-
gards any other objects as background. It learns to adapt
detection to the language embedding of queried concepts at
run-time. Specifically, the detector conditions proposal gen-
eration with respect to the text embedding of each queried
concept and refines the conditioned proposals into predic-
tions. Our detection transformer conditioned on language
(DECOLA) offers a powerful alternative to classical architec-
tures in open-vocabulary detection. Adapting the detector to
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
16593
Abstract
While GAN-based models have been successful in image
stylization tasks, they often struggle with structure preser-
vation while stylizing a wide range of input images. Re-
cently, diffusion models have been adopted for image styl-
ization but still lack the capability to maintain the original
quality of input images. Building on this, we propose OS-
ASIS: a novel one-shot stylization method that is robust in
structure preservation. We show that OSASIS is able to ef-
fectively disentangle the semantics from the structure of an
image, allowing it to control the level of content and style
implemented to a given input. We apply OSASIS to var-
ious experimental settings, including stylization with out-
of-domain reference images and stylization with text-driven
manipulation. Results show that OSASIS outperforms other
stylization methods, especially for input images that were
rarely encountered during training, providing a promising
solution to stylization via diffusion models. The source code
can be found at https://github.com/hansam95/OSASIS.
*Work done during an internship at NAVER Cloud.
†Corresponding author.
1. Introduction
In the literature of generative models, image stylization
refers to training a model in order to transfer the style of
a reference image to various input images during infer-
ence [21, 22, 28]. However, collecting a sufficient num-
ber of images that share a particular style for training
can be difficult.
Consequently, one-shot stylization has
emerged as a viable and practical solution, with genera-
tive adversarial networks (GANs) showing promising re-
sults [2, 16, 36, 38, 39].
Despite significant advancements in GAN-based styliza-
tion techniques, the accurate preservation of an input im-
age’s structure continues to pose a significant challenge.
This difficulty is particularly pronounced for input images
that contain elements infrequently encountered during train-
ing, often characterized by complex structural nuances that
diverge from those observed in more commonly presented
images. Figure 1(a) illustrates this challenge, where entities
such as hands and microphones, when processed through
GAN-based stylization, diverge considerably from their
original structural integrity. In addition, GAN-based styl-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8302
Abstract
Video Frame Interpolation (VFI), which aims at gener-
ating high-frame-rate videos from low-frame-rate inputs, is
a highly challenging task. The emergence of bio-inspired
sensors known as event cameras, which boast microsecond-
level temporal resolution, has ushered in a transformative
era for VFI. Nonetheless, the application of event-based
VFI techniques in domains with distinct environments from
the training data can be problematic. This is mainly be-
cause event camera data distribution can undergo substan-
tial variations based on camera settings and scene condi-
tions, presenting challenges for effective adaptation. In this
paper, we propose a test-time adaptation method for event-
based VFI to address the gap between the source and target
domains. Our approach enables sequential learning in an
online manner on the target domain, which only provides
low-frame-rate videos. We present an approach that lever-
ages confident pixels as pseudo ground-truths, enabling
stable and accurate online learning from low-frame-rate
videos. Furthermore, to prevent overfitting during the con-
tinuous online process where the same scene is encountered
repeatedly, we propose a method of blending historical sam-
ples with current scenes. Extensive experiments validate the
effectiveness of our method, both in cross-domain and con-
tinuous domain shifting setups. The code is available at
https://github.com/Chohoonhee/TTA-EVF.
1. Introduction
Video Frame Interpolation (VFI) is a well-established prob-
lem in the field of computer vision, aiming to enhance
the temporal resolution of videos. In recent times, deep
learning-based VFI approaches [2, 3, 15, 20–22, 33, 35, 37,
38, 50, 60] have achieved remarkable performances across
various benchmark datasets. However, accurate motion es-
timation becomes challenging in scenes with complex and
non-linear motions due to the lack of information, often re-
sulting in the generation of erroneous inter frames.
Time
Pre-trained
TTA-EVF
Frame 297
Frame 441
Frame 596
No Adaptation
Test-Time Adaptation
Frame 297
Frame 297
Frame 441
Frame 596
Figure 1. TTA-EVF efficiently produces high-quality results by
adapting the network to the target domain in an online manner, al-
leviating the need for offline data supply and addressing the perfor-
mance degradation observed when applying a well-trained event-
based VFI network to a domain with a different distribution.
To address the inherent lack of intermediate information
in frame cameras, recent researches [16, 24, 51, 52, 56] have
explored the potential of event cameras as a promising so-
lution for VFI, especially in scenarios involving complex
motion. Event cameras sense dynamic changes in pixel in-
tensity, triggering an event when the change surpasses a pre-
defined threshold. The incorporation of event cameras as
additional devices in VFI mitigates the challenges associ-
ated with modeling complex motion.
Typically,
frame-based VFI methods [17, 33, 38]
are evaluated on test datasets (e.g., SNU-FILM [12],
X4K1000FPS [43]) with distributions different from their
training dataset (e.g., Vimeo90K [61]). Frame-based VFI
demonstrates a certain level of generalization ability even
without additional modules, providing evidence that it can
be applied to real scenario applications, such as dealing with
changes in camera devices. These device and environment
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25701
Abstract
Each photo in an image burst can be considered a sam-
ple of a complex 3D scene: the product of parallax, diffuse
and specular materials, scene motion, and illuminant vari-
ation. While decomposing all of these effects from a stack
of misaligned images is a highly ill-conditioned task, the
conventional align-and-merge burst pipeline takes the other
extreme: blending them into a single image. In this work,
we propose a versatile intermediate representation: a two-
layer alpha-composited image plus flow model constructed
with neural spline fields – networks trained to map input
coordinates to spline control points. Our method is able
to, during test-time optimization, jointly fuse a burst image
capture into one high-resolution reconstruction and decom-
pose it into transmission and obstruction layers. Then, by
discarding the obstruction layer, we can perform a range
of tasks including seeing through occlusions, reflection sup-
pression, and shadow removal. Tested on complex in-the-
wild captures we find that, with no post-processing steps
or learned priors, our generalizable model is able to out-
perform existing dedicated single-image and multi-view ob-
struction removal approaches.
1. Introduction
Over the last decade, as digital photos have increasingly
been produced by smartphones, smartphone photos have in-
creasingly been produced by burst fusion. To compensate
for less-than-ideal camera hardware – typically restricted
to a footprint of less than 1cm3 [6] – smartphones rely on
their advanced compute hardware to process and fuse mul-
tiple lower-quality images into a high-fidelity photo [10].
This proves particularly important in low-light and high-
dynamic-range settings [22,39], where a single image must
compromise between noise and motion blur, but multi-
ple images afford the opportunity to minimize both [26].
But even as mobile night- and astro-photography applica-
tions [16, 17] use increasingly long sequences of photos
as input, their output remains a static single-plane image.
Given the typically non-static and non-planar nature of the
real world, a core problem in burst image pipelines is thus
Input Scene
Burst Capture
Neural Spline Field
Layer Separation
Recovered Obstruction + Transmission
Figure 1. Fitting our two-layer neural spline field model to a stack
of images we’re able to directly estimate and separate even severe,
out-of-focus obstructions to recover hidden scene content.
the alignment [32,45] and aggregation [5,63] of pixels into
an image array – referred to as the align-and-merge process.
While existing approaches treat pixel motion as a source
of noise and artifacts, a parallel direction of work [9,20,69]
attempts to extract useful parallax cues from this pixel mo-
tion to estimate the geometry of the scene. Recent work by
Chugunov et al. [8] finds that maximizing the photometric
consistency of an RGB plus depth neural field model of an
image sequence is enough to distill dense depth estimates
of the scene. While this method is able to jointly estimate
high-quality camera motion parameters, it does not perform
high-quality image reconstruction, and rather treats its im-
age model as “a vehicle for depth optimization” [8]. In con-
trast, work by Nam et al. [50] proposes a neural field fit-
ting approach for multi-image fusion and layer separation
which focuses on the quality of the reconstructed “canon-
ical view”. By swapping in different motion models, they
can separate and remove layers such as occlusions, reflec-
tions, and moir´e patterns during image reconstruction – as
opposed to in a separate post-processing step [19,54]. This
approach, however, does not make use of a realistic cam-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25763
Abstract
Recent advancements in Spatial Transcriptomics (ST)
technology have facilitated detailed gene expression anal-
ysis within tissue contexts. However, the high costs and
methodological limitations of ST necessitate a more ro-
bust predictive model. In response, this paper introduces
TRIPLEX, a novel deep learning framework designed to
predict spatial gene expression from Whole Slide Images
(WSIs). TRIPLEX uniquely harnesses multi-resolution fea-
tures, capturing cellular morphology at individual spots,
the local context around these spots, and the global tissue
organization. By integrating these features through an ef-
fective fusion strategy, TRIPLEX achieves accurate gene ex-
pression prediction. Our comprehensive benchmark study,
conducted on three public ST datasets and supplemented
with Visium data from 10X Genomics, demonstrates that
TRIPLEX outperforms current state-of-the-art models in
Mean Squared Error (MSE), Mean Absolute Error (MAE),
and Pearson Correlation Coefficient (PCC). The model’s
predictions align closely with ground truth gene expression
profiles and tumor annotations, underscoring TRIPLEX’s
potential in advancing cancer diagnosis and treatment.
1. Introduction
The emergence of large-scale Spatial Transcriptomics
(ST) technology has facilitated the quantification of mRNA
expression across a multitude of genes within the spatial
context of tissue samples [22]. ST technology segments
centimeter-scale Whole Slide Images (WSIs) into hundreds
of thousands of small spots, each providing its gene ex-
pression profile.
Considering the substantial cost asso-
ciated with ST sequencing technology, coupled with the
widespread availability of WSIs, a pressing question is how
to best predict spatial gene expression based on WSIs using
rapidly evolving computer vision techniques.
A number of studies have endeavored to address this
*Corresponding author.
challenge [7, 18, 24–26]. Approaches vary, with some pre-
dicting gene expression strictly from the tissue image con-
fined within the spot’s boundaries [7], while others also
take into account spatial dependencies between spot im-
ages [18, 26], or consider similarities to reference spots
[24, 25]. However, we have noted several limitations inher-
ent to these existing methodologies. Firstly, current meth-
ods primarily focus on spot images, neglecting the wealth of
biological information available in the wider image context.
By integrating both the specific spot and its surrounding en-
vironment, along with the holistic view of the entire histol-
ogy image, we can access richer information, encompass-
ing varied biological contexts. Secondly, models that con-
sider interactions between spots [18, 26] face a limitation
in processing the embedding of all patches in a WSI simul-
taneously. This approach, common in handling hundreds
to thousands of patches within a WSI, limits the scalability
of the patch embedding model due to resource constraints.
Such limitations significantly impede the extraction of fine-
grained, rich representations from each spot, thereby affect-
ing the model’s ability to perform detailed analysis of WSIs.
Thirdly, model performance is frequently overestimated be-
cause of inadequate validation, such as using the limited
size of dataset [7] sometimes without cross-validation [24]
and training/testing with replicates from the same patient
[18, 25, 26]. The limited size of ST datasets means that
exclusive reliance on a single dataset for model evaluation
can hinder an accurate assessment of the model’s capabili-
ties, thereby emphasizing the necessity for cross-validation.
The issue is compounded when replicate data from the same
patient, often featuring nearly identical image-gene expres-
sion pairs, are used in both training and testing phases. This
can lead to an inflated perception of a model’s effective-
ness, as it may not accurately reflect the model’s ability to
generalize to new, unseen data. Lastly, the use of disparate
datasets, diverse normalization methods, and varied evalua-
tion techniques in existing research studies compounds the
challenge of conducting fair comparisons of the models.
Addressing these limitations, we present TRIPLEX, an
innovative deep learning framework designed to leverage
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11591
Abstract
Existing research based on deep learning has extensively
explored the problem of daytime image dehazing. However,
few studies have considered the characteristics of nighttime
hazy scenes. There are two distinctions between nighttime
and daytime haze. First, there may be multiple active col-
ored light sources with lower illumination intensity in night-
time scenes, which may cause haze, glow and noise with
localized, coupled and frequency inconsistent characteris-
tics. Second, due to the domain discrepancy between simu-
lated and real-world data, unrealistic brightness may occur
when applying a dehazing model trained on simulated data
to real-world data. To address the above two issues, we
propose a semi-supervised model for real-world nighttime
dehazing. First, the spatial attention and frequency spec-
trum ﬁltering are implemented as a spatial-frequency do-
main information interaction module to handle the ﬁrst is-
sue. Second, a pseudo-label-based retraining strategy and
a local window-based brightness loss for semi-supervised
training process is designed to suppress haze and glow
while achieving realistic brightness. Experiments on pub-
lic benchmarks validate the effectiveness of the proposed
method and its superiority over state-of-the-art methods.
The source code and Supplementary Materials are placed
in the https://github.com/Xiaofeng-life/SFSNiD.
1. Introduction
Nighttime and daytime images may contain hazy effects,
which may cause their quality to be degraded [7, 15, 39,
50]. Therefore, two valuable research ﬁelds are proposed,
which are daytime single image dehazing (DaSID) [2, 38,
54] and nighttime single image dehazing (NiSID) [14, 19,
31], respectively. Compared with the daytime hazy image,
the imaging of the nighttime hazy image is more complex
[28, 49]. Currently, NiSID is still a challenging problem.
Existing research on DaSID [20, 25, 27, 34, 43, 48, 52,
*Corresponding author
(a) Hazy
(b) IM-YellowHaze [26]
(c) IM-NightHaze [26]
(d) IM-NHR [51]
(e) GE-UNREAL-NH [31]
(f) Ours
Figure 1. Visualization of real-world dehazed images, where the
“IM-” and “GE-” denote the dehazed results obtained by training
on imaging model (IM) and game engine (GE) simulated datasets,
respectively.
The curve ﬁgure represents the pixel histogram,
where the x and y coordinates represent the pixel values and cor-
responding numbers, respectively. The x and y coordinates of the
bar ﬁgure represent the color channel and the corresponding aver-
age pixel value, respectively.
53] have achieved impressive performance. Various effec-
tive DaSID algorithms have been proposed and veriﬁed on
benchmark daytime datasets [21]. However, these DaSID
algorithms are designed for the properties of daytime hazy
and haze-free images, without taking into account the char-
acteristics of nighttime hazy and haze-free images.
Currently, NiSID research is divided into two types,
namely non-deep learning-based NiSID and deep learning-
based NiSID. On the one hand, the prior hypotheses and
statistical laws are explored [50, 51]. The maximum re-
ﬂectance prior to estimate the varying ambient illumina-
tion is proposed by [50]. The illumination estimation, color
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2631
