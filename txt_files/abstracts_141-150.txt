Abstract
Recent works on text-to-3d generation show that using
only 2D diffusion supervision for 3D generation tends to
produce results with inconsistent appearances (e.g., faces
on the back view) and inaccurate shapes (e.g., animals
with extra legs). Existing methods mainly address this is-
sue by retraining diffusion models with images rendered
from 3D data to ensure multi-view consistency while strug-
gling to balance 2D generation quality with 3D consistency.
In this paper, we present a new framework Sculpt3D that
equips the current pipeline with explicit injection of 3D
priors from retrieved reference objects without re-training
the 2D diffusion model. Specifically, we demonstrate that
high-quality and diverse 3D geometry can be guaran-
teed by keypoints supervision through a sparse ray sam-
pling approach.
Moreover, to ensure accurate appear-
ances of different views, we further modulate the output of
the 2D diffusion model to the correct patterns of the tem-
plate views without altering the generated object’s style.
These two decoupled designs effectively harness 3D in-
formation from reference objects to generate 3D objects
while preserving the generation quality of the 2D diffu-
sion model. Extensive experiments show our method can
largely improve the multi-view consistency while retaining
fidelity and diversity.
Our project page is available at:
https://stellarcheng.github.io/Sculpt3D/.
1. Introduction
There has been growing research attention towards text-to-
3d generation.
Compared to image generation, the data
available for 3D generation is less in quantity and lower
in quality. Thus, many studies [19, 28, 41] have begun to
generate 3D objects using 2D text-to-image models [9, 30]
as supervision to leverage their strong priors learned from
billions of real images.
∗Corresponding authors.
Lifelike tiger with fierce expression
An antique glass perfume bottle
Prolificdreamer
Ours
Reference Sets
Figure 1. Comparison of objects generated by our method and
ProlificDreamer. We retain the 2D model’s capability to produce
high-fidelity objects and adaptively learn 3D information from ref-
erence templates retrieved from external datasets.
These methods mainly contain two steps: the first step
is to continuously sample images from different views of
a randomly initialized 3D representation (e.g. NeRF [26],
DMTet [33]). The second step uses a 2D diffusion model
to individually judge whether each image is a high-quality
image that conforms to the text description. Compared to
2D image generation, 3D generation not only requires pro-
ducing high-quality images for each individual viewpoint
but also needs to create plausible shapes and appearances
as a whole 3D object. Thus, a high-quality 2D generative
model and a mechanism that can accurately provide 3D pri-
ors are two keys to achieving decent 3D generation results.
Since early works [19, 28, 41] mainly use the sole 2D diffu-
sion model as supervision, they tend to produce inaccurate
shapes (shape ambiguity) and appearances that are incon-
sistent across viewpoints (appearance ambiguity), as shown
in Figure 1 left, where examples include incomplete bottles,
tigers with multiple legs, and tails.
Recently, some efforts have been made to expand the
3D datasets [10]. Following this, there were immediate at-
tempts to retrain 2D diffusion models on these 3D datasets
to learn 3D information [21, 22, 35]. Although these meth-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10228
Abstract
Recently, transformer-based methods have achieved
state-of-the-art prediction quality on human pose esti-
mation(HPE). Nonetheless, most of these top-performing
transformer-based models are too computation-consuming
and storage-demanding to deploy on edge computing plat-
forms. Those transformer-based models that require fewer
resources are prone to under-fitting due to their smaller
scale and thus perform notably worse than their larger
counterparts.
Given this conundrum, we introduce SD-
Pose, a new self-distillation method for improving the per-
formance of small transformer-based models.
To miti-
gate the problem of under-fitting, we design a transformer
module named Multi-Cycled Transformer(MCT) based on
multiple-cycled forwards to more fully exploit the poten-
tial of small model parameters. Further, in order to pre-
vent the additional inference compute-consuming brought
by MCT, we introduce a self-distillation scheme, extracting
the knowledge from the MCT module to a naive forward
model. Specifically, on the MSCOCO validation dataset,
SDPose-T obtains 69.7% mAP with 4.4M parameters and
1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5%
mAP on the MSCOCO validation dataset with 6.2M pa-
rameters and 4.7 GFLOPs, achieving a new state-of-the-art
among predominant tiny neural network methods.
1. Introduction
Human Pose Estimation (HPE) aims to estimate the posi-
tion of each joint point of the human body in a given im-
age. HPE tasks support a wide range of downstream tasks
such as activity recognition[1], motion capture[2], etc. Re-
cently with the ViT model being proven effective on many
visual tasks, many transformer-based methods[3–5] have
achieved excellent performance on HPE tasks. Compared
with past CNN-based methods[6], transformer-based mod-
*Equal Contribution.
†Corresponding Authors.
Figure 1. Comparsions between other small models and our meth-
ods on MSCOCO validation dataset. Compared to other methods,
our approach can significantly reduce the scale while maintaining
the same performance, or greatly improve performance under the
same scale.
els are much more powerful in capturing the relationship
between visual elements. However, most of them are large
and computationally expensive. The state-of-the-art(SOTA)
transformer-based model[5] has 632 million parameters and
requires 122.9 billion floating-point operations.
Such a
large-scale model is difficult to deploy on edge computing
devices and cannot accommodate the growing development
of embodied intelligence. However, when the CNN or ViT
used as a backbone is not of sufficient scale, transformer-
based models are not able to learn the relationship between
keypoints and visual elements well resulting in poor per-
formance.
Stacking more transformer layers is a viable
approach[4], but this also increases the scale of the network
resulting in larger parameters and the difficulty of edge de-
ployment.
To help small models learn better, one possible way is
to distill knowledge from big model to small model[7, 8].
However, previous distillation methods have the following
drawbacks: (1) To align the vector space, an additional
manipulation is required during feature distillation[9] and
leads to a potential performance decrease. (2) A huge extra
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1082
Abstract
Category-level object pose estimation, aiming to predict
the 6D pose and 3D size of objects from known categories,
typically struggles with large intra-class shape variation.
Existing works utilizing mean shapes often fall short of cap-
turing this variation. To address this issue, we present Sec-
ondPose, a novel approach integrating object-specific ge-
ometric features with semantic category priors from DI-
NOv2.
Leveraging the advantage of DINOv2 in provid-
ing SE(3)-consistent semantic features, we hierarchically
extract two types of SE(3)-invariant geometric features to
further encapsulate local-to-global object-specific informa-
tion. These geometric features are then point-aligned with
DINOv2 features to establish a consistent object represen-
tation under SE(3) transformations, facilitating the map-
ping from camera space to the pre-defined canonical space,
thus further enhancing pose estimation. Extensive exper-
iments on NOCS-REAL275 demonstrate that SecondPose
achieves a 12.4% leap forward over the state-of-the-art.
Moreover, on a more complex dataset HouseCat6D which
provides photometrically challenging objects, SecondPose
still surpasses other competitors by a large margin.
1. Introduction
Category-level pose estimation involves estimating the
complete 9 degrees-of-freedom (DoF) object pose, encom-
passing 3D rotation, 3D translation, and 3D metric size, for
arbitrary objects within a known set of categories. This task
has garnered significant research interest due to its essen-
tial role in various applications, including the AR/VR in-
dustry [30, 38, 40, 55], robotics [51, 52, 54], and scene un-
∗Equal contributions.
† Corresponding author (e-mail: guangyao.zhai@tum.de).
Figure 1. Categorical SE(3)-consistent features. We visualize
our fused features by PCA. Colored points highlight the most cor-
responding parts, where our proposed feature achieves consistent
alignment cross instances (left vs. middle) and maintains consis-
tency on the same instance of different poses (middle vs. right).
derstanding [1, 7, 53]. In contrast to traditional instance-
level pose estimation methods [6,21], which rely on specific
3D CAD models for each target object, the category-level
approach necessitates greater adaptability to accommodate
inherent shape diversity within each category. Effectively
addressing intra-class shape variations has thus become a
central focus, crucial for real-world applications where ob-
jects within a category may exhibit significant differences
in shape while sharing the same general category label.
Mean Shape vs. Semantic Priors. One common ap-
proach to handle intra-class shape variation involves us-
ing explicit mean shapes as prior knowledge [24, 39, 57].
These methods typically consist of two functional modules:
one for reconstructing the target object by slightly deform-
ing the mean shape and another for regressing the 9D pose
based on the reconstructed object [24,39] or enhanced inter-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
9959
Abstract
We propose a novel self-supervised embedding to learn
how actions sound from narrated in-the-wild egocentric
videos. Whereas existing methods rely on curated data
with known audio-visual correspondence, our multimodal
contrastive-consensus coding (MC3) embedding reinforces
the associations between audio, language, and vision when
all modality pairs agree, while diminishing those associa-
tions when any one pair does not. We show our approach
can successfully discover how the long tail of human ac-
tions sound from egocentric video, outperforming an array
of recent multimodal embedding techniques on two datasets
(Ego4D and EPIC-Sounds) and multiple cross-modal tasks.
1. Introduction
Human activity often produces sounds. Closing a door, chop-
ping vegetables, typing on a keyboard, talking with a friend—
our interactions with the objects and people around us gener-
ate audio that reveals our physical behaviors. These sounds
can be strongly associated with the subjects of our activity
and how we perform it. For example, opening a water bottle
sounds different than opening a cabinet; chopping sweet
potatoes sounds different than chopping onions; chopping
onions sounds different than mincing onions (the same ob-
ject). Understanding the link between sounds and actions is
valuable for a number of applications, such as multimodal ac-
tivity recognition, cross-modal retrieval, content generation,
or forecasting the physical effects of a person’s actions.
How should AI learn about sounding actions? Exist-
ing work typically curates annotated datasets for supervised
learning [9, 22, 28, 45], taking care to select events or actions
that have associated sounds (e.g., lawnmowing, chopping),
while others deliberately collect videos of object collisions
(e.g., striking objects with a drumstick [43] or crashing into
them with a robot [10, 17]), or develop physics-based simula-
tions [16]. On the one hand, these approaches are appealing
for their ability to focus on meaningful audio-visual corre-
spondences. On the other hand, their curated nature risks
limiting the scope of sounding actions that can be learned.
Action: C digs the 
soil with a hoe.
Action: C moves a 
metal cutting machine 
with hands.
Does the action sound?
Does the action sound?
Figure 1. We aim to distinguish sounds that are directly caused
by human actions (bottom) from those that are not (top). Given
egocentric training videos with language descriptions of the camera
wearer’s (“C") current action, we learn an embedding where the
audio and visual features of any given clip are best aligned only
when both are also consistent with the language. This allows
discerning clips where the audio and vision may be correlated (e.g.,
the cutting machine running making loud noise in top row) versus
those where the sounds are driven by human action (digging in
bottom row)—importantly, without language at inference time.
Instead, we aim to learn how human actions sound from
narrated in-the-wild egocentric videos. See Figure 1. Given
a pool of videos of everyday human activity, the goal is to
learn a cross-modal representation where sounding actions
cluster together based on how they look and sound. By sam-
pling the videos freely, we can broaden the scope to discover
the breadth of sounding actions without having to rely on a
closed, pre-defined set of action categories. In particular, by
focusing on unscripted egocentric video from wearable cam-
eras in daily-life settings [11, 26], we aim to include subtle
and long-tail scenarios unavailable in curated datasets, such
as sounds of keys jangling when unlocking a door, scissors
snipping when cutting the dog’s fur, or fingernails scratch-
ing on one’s own arm. Egocentric video is a particularly
attractive source here because 1) human interaction sounds
are more audible in near-field egocentric recordings and 2)
passively captured long-form ego-video simply covers more
1
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27252
Abstract
Learning generalizable visual representations from Inter-
net data has yielded promising results for robotics. Yet, pre-
vailing approaches focus on pre-training 2D representations,
being sub-optimal to deal with occlusions and accurately
localize objects in complex 3D scenes. Meanwhile, 3D rep-
resentation learning has been limited to single-object under-
standing. To address these limitations, we introduce a novel
3D pre-training framework for robotics named SUGAR that
captures semantic, geometric and affordance properties of
objects through 3D point clouds. We underscore the impor-
tance of cluttered scenes in 3D representation learning, and
automatically construct a multi-object dataset benefiting
from cost-free supervision in simulation. SUGAR employs
a versatile transformer-based model to jointly address five
pre-training tasks, namely cross-modal knowledge distilla-
tion for semantic learning, masked point modeling to un-
derstand geometry structures, grasping pose synthesis for
object affordance, 3D instance segmentation and referring
expression grounding to analyze cluttered scenes. We evalu-
ate our learned representation on three robotic-related tasks,
namely, zero-shot 3D object recognition, referring expres-
sion grounding, and language-driven robotic manipulation.
Experimental results show that SUGAR’s 3D representation
outperforms state-of-the-art 2D and 3D representations.
1. Introduction
Visual perception plays an essential role in robotics and en-
ables autonomous agents to understand and interact with
their physical environment. Nevertheless, learning general-
izable visual representations for robotics is challenging due
to the scarcity of real robot data and the large variety of
real-world scenes. Despite substantial efforts in robot data
accumulation [2, 76, 77] and augmentation [39, 85], it re-
mains prohibitively expensive to collect large-scale datasets
comprising a broad range of robotic tasks.
To alleviate the burden of data collection, recent endeav-
ors [36, 37, 48, 49, 51, 62] have sought to leverage large-
scale internet data to pre-train 2D visual representations for
Pre-training
Robotics tasks
Semantic
a blue plastic 
water bottle
with a black lid
mUlti-object scenes
RGB
Depth
Label
Geometry
Affordance
pick up the knife and leave it
on the chopping board
grasp the red cup and lift it
Figure 1. We introduce SUGAR
, a pre-training framework for
robotic-related tasks, which learns semantic, geometry and affor-
dance on both single- and multi-object scenes.
robotics. For example, MVP [62], VIP [48] and VC-1 [49]
use self-supervised learning on image or video datasets,
while EmbCLIP [37], R3M [51] and Voltron [36] further per-
form cross-modal pre-training based on videos with aligned
language descriptions. While these 2D representations have
demonstrated promising performance, they still fall short in
addressing occlusions in complex cluttered scenes [79] and
accurately predicting robotic actions [7] in the 3D world.
Recently, growing research attention has been paid to 3D
visual representations for robotics. A majority of approaches
train 3D-based models from scratch [7, 43, 69], potentially
losing the generalization ability. Several recent works lift pre-
trained 2D features to the 3D space [20, 34, 56, 89], which
compromise efficiency due to processing multi-view images
and do not fully take advantage of 3D data. To improve
3D representation learning, prior endeavours propose self-
supervised pre-training of 3D models [44, 53, 59, 86]. Pre-
training in existing work, however, is typically limited to
single objects and complete point clouds, hence, ignoring
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18049
Abstract
Automatic text-to-3D generation that combines Score Dis-
tillation Sampling (SDS) with the optimization of volume
rendering has achieved remarkable progress in synthesizing
realistic 3D objects. Yet most existing text-to-3D methods by
SDS and volume rendering suffer from inaccurate geometry,
e.g., the Janus issue, since it is hard to explicitly integrate 3D
priors into implicit 3D representations. Besides, it is usually
time-consuming for them to generate elaborate 3D models
with rich colors. In response, this paper proposes GSGEN, a
novel method that adopts Gaussian Splatting, a recent state-
of-the-art representation, to text-to-3D generation. GSGEN
aims at generating high-quality 3D objects and address-
ing existing shortcomings by exploiting the explicit nature
of Gaussian Splatting that enables the incorporation of 3D
prior. Specifically, our method adopts a progressive optimiza-
tion strategy, which includes a geometry optimization stage
and an appearance refinement stage. In geometry optimiza-
†Corresponding author
tion, a coarse representation is established under 3D point
cloud diffusion prior along with the ordinary 2D SDS opti-
mization, ensuring a sensible and 3D-consistent rough shape.
Subsequently, the obtained Gaussians undergo an iterative
appearance refinement to enrich texture details. In this stage,
we increase the number of Gaussians by compactness-based
densification to enhance continuity and improve fidelity. With
these designs, our approach can generate 3D assets with del-
icate details and accurate geometry. Extensive evaluations
demonstrate the effectiveness of our method, especially for
capturing high-frequency components. Our code is available
at https://github.com/gsgen3d/gsgen.
1. Introduction
Diffusion model based text-to-image generation [1, 55, 57,
58] has achieved remarkable success in synthesizing photo-
realistic images from textual prompts. Nevertheless, for high-
quality text-to-3D content generation, the advancements lag
behind that of image generation due to the inherent complex-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
21401
Abstract
Federated learning facilitates the collaborative learning
of a global model across multiple distributed medical in-
stitutions without centralizing data. Nevertheless, the ex-
pensive cost of annotation on local clients remains an ob-
stacle to effectively utilizing local data. To mitigate this
issue, federated active learning methods suggest leverag-
ing local and global model predictions to select a rela-
tively small amount of informative local data for annota-
tion.
However, existing methods mainly focus on all lo-
cal data sampled from the same domain, making them un-
reliable in realistic medical scenarios with domain shifts
among different clients. In this paper, we make the first at-
tempt to assess the informativeness of local data derived
from diverse domains and propose a novel methodology
termed Federated Evidential Active Learning (FEAL) to
calibrate the data evaluation under domain shift. Specif-
ically, we introduce a Dirichlet prior distribution in both
local and global models to treat the prediction as a distribu-
tion over the probability simplex and capture both aleatoric
and epistemic uncertainties by using the Dirichlet-based
evidential model.
Then we employ the epistemic uncer-
tainty to calibrate the aleatoric uncertainty. Afterward, we
design a diversity relaxation strategy to reduce data re-
dundancy and maintain data diversity. Extensive experi-
ments and analysis on five real multi-center medical im-
age datasets demonstrate the superiority of FEAL over the
state-of-the-art active learning methods in federated sce-
narios with domain shifts. The code will be available at
https://github.com/JiayiChen815/FEAL.
∗Equal contribution.
†Yong Xia is the corresponding author. This
work was supported in part by Shenzhen Science and Technology Pro-
gram under Grants JCYJ20220530161616036, National Natural Science
Foundation of China under Grants 62171377 and 62271405, Ningbo Clin-
ical Research Center for Medical Imaging under Grant 2021L003 (Open
Project: 2022LYKFZD06), and Foshan HKUST Projects under Grants
FSUST21-HKUST10E and FSUST21-HKUST11E.
?
𝑈!
𝐿!
?
𝑈"
𝐿"
𝑈#
𝐿#
Annotate
Distribute
Aggregate
⋯
Client K
Server
Client 1
Client 2
Train
(a) FAL scheme
0.0
2.5
5.0
7.5
10.0
12.5
Energy score
0.00
0.05
0.10
0.15
0.20
0.25
Density
Client 1
Client 2
Client 3
Client 4
(b) KDE of energy score
1
2
3
4
Client
1
2
3
4
Client
1.0000
0.0001
0.0001
0.0001
0.0001
1.0000
0.0130
0.0001
0.0001
0.0130
1.0000
0.0031
0.0001
0.0001
0.0031
1.0000
0.0
0.2
0.4
0.6
0.8
1.0
(c) p-value
Figure 1. Illustration of federated active learning (FAL) in the
presence of domain shift. (a) FAL comprises model distribution,
local training, model aggregation, and data annotation. (b) The
KDE of energy scores depicts domain shifts across clients. (c)
The low p-values in cross-client KDE of energy scores indicate
the existence of significant domain shifts between all client pairs.
1. Introduction
Federated learning enables collaborative learning across
multiple clinical institutions (i.e., clients) to learn a uni-
fied model on the central server through model aggregation
while preserving the data privacy at each client [21, 36, 57]
(see Fig. 1 (a)). Unfortunately, such a learning pipeline re-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11439
Abstract
Despite many attempts to leverage pre-trained text-to-
image models (T2I) like Stable Diffusion (SD) [25] for con-
trollable image editing, producing good predictable results
remains a challenge.
Previous approaches have focused
on either fine-tuning pre-trained T2I models on specific
datasets to generate certain kinds of images (e.g., with a
specific object or person), or on optimizing the weights,
text prompts, and/or learning features for each input im-
age in an attempt to coax the image generator to produce
the desired result.
However, these approaches all have
shortcomings and fail to produce good results in a pre-
dictable and controllable manner. To address this problem,
we present TiNO-Edit, an SD-based method that focuses on
optimizing the noise patterns and diffusion timesteps dur-
ing editing, something previously unexplored in the liter-
ature. With this simple change, we are able to generate
results that both better align with the original images and
reflect the desired result. Furthermore, we propose a set
of new loss functions that operate in the latent domain of
SD, greatly speeding up the optimization when compared
to prior losses, which operate in the pixel domain. Our
method can be easily applied to variations of SD includ-
ing Textual Inversion [13] and DreamBooth [27] that en-
code new concepts and incorporate them into the edited re-
sults. We present a host of image-editing capabilities en-
abled by our approach. Our code is publicly available at
https://github.com/SherryXTChen/TiNO-Edit.
1. Introduction
Computer-generated image synthesis has been studied
for decades for its wide range of applications includ-
ing content creation, marketing and advertising, visual-
ization/simulation, entertainment, and storytelling.
Re-
*Corresponding author email: xchen774@ucsb.edu
Pure text-guided & Reference-guided image editing
Original
Ref
w/o ref
w/ref
“sandwich” →“cake”
+ “a magenta hat”
Stroke-guided image editing
Original
User input
Result
Original
User input
Result
+ “a sunflower”
+ “curtains”
Image composition
Original
User input
Result
Original
User input
Result
+ “a scarecrow”
“white” →“yellow plaid”
Image editing with DreamBooth (DB) or Textual Inversion (TI)
Original
DB
Result
Original
TI
Result
“photo” →⟨concept⟩
“sky” →⟨concept⟩
Figure 1.
Overview of capabilities enabled by TiNO-Edit.
TiNO-Edit offers various image-editing capabilities and can be run
with DreamBooth (DB) [27] or Textual Inversion (TI) [13]. By
leveraging diffusion timestep and noise optimization techniques,
it can generate realistic and high quality outputs.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6337
Abstract
We present a method to generate full-body selfies from
photographs originally taken at arms length. Because self-
captured photos are typically taken close up, they have lim-
ited field of view and exaggerated perspective that distorts
facial shapes. We instead seek to generate the photo some
one else would take of you from a few feet away. Our ap-
proach takes as input four selfies of your face and body, a
background image, and generates a full-body selfie in a de-
sired target pose. We introduce a novel diffusion-based ap-
proach to combine all of this information into high-quality,
well-composed photos of you with the desired pose and
background.
1. Introduction
The prevalence of selfies has skyrocketed in recent years,
with an estimated 93 million taken each day. Despite their
popularity, they suffer from multiple shortcomings: (1) they
capture only the upper portion of the subject, (2) the close-
up camera viewpoint distorts faces and requires awkward
poses (e.g., with arm reaching out), and (3) it is difficult to
compose a shot that optimally captures both the subject and
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6701
Abstract
Tumor synthesis enables the creation of artificial tumors
in medical images, facilitating the training of AI models for
tumor detection and segmentation. However, success in tu-
mor synthesis hinges on creating visually realistic tumors
that are generalizable across multiple organs and, further-
more, the resulting AI models being capable of detecting
real tumors in images sourced from different domains (e.g.,
hospitals). This paper made a progressive stride toward
generalizable tumor synthesis by leveraging a critical ob-
servation: early-stage tumors (< 2cm) tend to have simi-
lar imaging characteristics in computed tomography (CT),
whether they originate in the liver, pancreas, or kidneys.
We have ascertained that generative AI models, e.g., Dif-
fusion Models, can create realistic tumors generalized to a
range of organs even when trained on a limited number of
tumor examples from only one organ. Moreover, we have
shown that AI models trained on these synthetic tumors can
be generalized to detect and segment real tumors from CT
volumes, encompassing a broad spectrum of patient demo-
graphics, imaging protocols, and healthcare facilities.
1. Introduction
Tumor synthesis enables the creation of artificial tumor ex-
amples in medical images [11, 42, 88], it is particularly
valuable when there is a dearth or complete absence of per-
voxel annotated real tumors (e.g., early-stage tumors) for
effective AI training. Typically, to train AI models for tu-
mor detection in multiple (N) organs, annotated real tu-
mor examples from each of these organs are necessary, and
ideally, in substantial numbers [14, 43, 54, 55, 97]. Fur-
thermore, AI models often fail to generalize across images
from different hospitals, which may vary due to variations
*Correspondence to Zongwei Zhou (ZZHOU82@JH.EDU)
in imaging protocols, patient demographics, and scanner
manufacturers [61, 95, 96]. The challenge amplifies with
the need for extensive manual annotations, a task that could
demand up to 25 human years for annotating just one tu-
mor type [1, 9, 83]. The task of collecting and annotating
a comprehensive dataset encompassing tumors from multi-
ple organs (N) and images from numerous hospitals (M) is
daunting, considering both annotation cost and complexity
(N × M). We hypothesize that tumor synthesis could solve
this challenge by creating various tumor types across non-
tumor images from multiple hospitals, even when only one
tumor type is available, thereby simplifying the complexity
from N × M to 1 × M.
Success in tumor synthesis hinges on creating visually
realistic tumors that are generalizable across multiple or-
gans and, furthermore, the resulting AI models being gen-
eralizable in detecting real tumors in images sourced from
different hospitals. Previous studies have introduced gen-
erative models to create synthetic medical data (not lim-
ited to tumors) such as polyp detection from colonoscopy
videos [69], COVID-19 detection from Chest X-ray [26,
56, 87], and diabetic lesion detection from retinal im-
ages [78]—refer to §5 for a comprehensive review. How-
ever, these studies have primarily focused on enhancing the
detection and segmentation of specific tumors without fully
exploring the wider generalizability of these models across
different organs and patient demographics.
This paper made a progressive stride toward generaliz-
able tumor synthesis by leveraging a critical observation:
early-stage tumors (< 2cm) tend to have similar imaging
characteristics in computed tomography (CT)1. Early-stage
tumors typically present small, round, or oval shapes with
minimal deformation and exhibit relatively simple and uni-
1Note that, owing to the public dataset constraints, we have only ver-
ified the similarity across early hepatocellular carcinoma and intrahep-
atic cholangiocarcinoma from the liver, pancreatic ductal adenocarcinoma
from the pancreas, and renal cell carcinoma from kidneys.
1
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11147
