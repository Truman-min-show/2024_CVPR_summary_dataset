Abstract
Due to the high potential for abuse of GenAI systems, the
task of detecting synthetic images has recently become of
great interest to the research community. Unfortunately, ex-
isting image-space detectors quickly become obsolete as new
high-fidelity text-to-image models are developed at blinding
speed. In this work, we propose a new synthetic image detec-
tor that uses features obtained by inverting an open-source
pre-trained Stable Diffusion model. We show that these inver-
sion features enable our detector to generalize well to unseen
generators of high visual fidelity (e.g., DALL¬∑E 3) even when
the detector is trained only on lower fidelity fake images
generated via Stable Diffusion. This detector achieves new
state-of-the-art across multiple training and evaluation se-
tups. Moreover, we introduce a new challenging evaluation
protocol that uses reverse image search to mitigate stylistic
and thematic biases in the detector evaluation. We show that
the resulting evaluation scores align well with detectors‚Äô in-
the-wild performance, and release these datasets as public
benchmarks for future research.
*Work done during an internship at Google Research.
1. Introduction
Recent advances in text-to-image modeling have made it
easier than ever to generate harmful or misrepresentative
content at scale. Moreover, new versions of most photoreal-
istic commercial models are being continuously updated and
released behind closed APIs, making it harder to keep fake
image detectors up to date. In this work, we make significant
strides towards building a GenAI detector that can reliably
identify images from unseen photorealistic text-to-image
models. Specifically, we propose a model that can be trained
using fake images only from Stable Diffusion (SD) [45] and
reliably detect images generated by recent open (Kandin-
sky [51], W¬®uerstchen [39], PixArt-Œ± [16], etc.) and closed-
source text-to-image models (Imagen [46], Midjourney [2],
DALL¬∑E 3 [12], etc.) of significantly higher visual fidelity.
Existing methods [17, 37, 54] focus primarily on detect-
ing traces left by convolutional generators in a way that is ro-
bust to re-compression, resizing and other in-the-wild trans-
formations. While these methods worked well for GANs and
early diffusion models, we show that they, unfortunately, fail
to generalize well to current photorealistic generative models,
even when re-trained using better data. Recent diffusion de-
tectors that rely on CLIP embeddings [37] or inversions [55]
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10759
Abstract
Detecting edges in images suffers from the problems of
(P1) heavy imbalance between positive and negative classes
as well as (P2) label uncertainty owing to disagreement be-
tween different annotators. Existing solutions address P1
using class-balanced cross-entropy loss and dice loss and
P2 by only predicting edges agreed upon by most annota-
tors. In this paper, we propose RankED, a uniÔ¨Åed ranking-
based approach that addresses both the imbalance problem
(P1) and the uncertainty problem (P2). RankED tackles
these two problems with two components: One component
which ranks positive pixels over negative pixels, and the
second which promotes high conÔ¨Ådence edge pixels to have
more label certainty. We show that RankED outperforms
previous studies and sets a new state-of-the-art on NYUD-
v2, BSDS500 and Multi-cue datasets. Code is available at
https://ranked-cvpr24.github.io.
1. Introduction
Detecting contours of objects in a given image is a fun-
damental problem in Computer Vision.
It has been ap-
proached as a machine learning problem since the intro-
duction of the inÔ¨Çuential BSDS dataset [1]. As with any
learning-based approach, characteristics of the training data
affects performance. One striking issue regarding ground-
truth contour data is that contours are rare events. For ex-
ample, in the BSDS dataset, only 7% of all pixels within an
image are marked as edge pixels1. This creates a signiÔ¨Åcant
imbalance between the positive (edge) and negative (non-
edge) classes, which hinders the training of machine learn-
ing models. Another important issue observed in edge data
is the uncertainty regarding the ground-truth annotations.
‚Ä†Equal contribution.
1Although some studies call such high-level, semantic edges as ‚Äúcon-
tour‚Äù and low-level edges as ‚Äúedge‚Äù, we follow the recent literature
[12, 15, 39, 44, 55] and use the term ‚Äúedge‚Äù for contours in the rest of
the paper.
1
1
Annotator 1
1
1
Annotator 2
1
1
1
Annotator 3
.33
.66
1
.33
Pixel-wise 
Averaging
Threshold
1
1
Training 
Target (ùê≤)
0.4
0.4
0.6
0.5
0.7
0.3
0.4
0.2
0.2
Prediction (ùê©)
Score-based
Loss
(a) Existing Approaches
Rank Positives
Above Negatives
0.4
0.4
0.6
0.5
0.5
0.3
0.4
0.2
0.2
ùíëùüë
"
(b) RankED with its Ranking and Sorting Components
Sort Positives
wrt Certianty
Certainty of
Labels (c)
ùíëùüê
"
ùíëùüé
"
ùíëùüè
"
Prediction (ùê©)
ùëù&
" > ùëù'
(
for each positive ùëñ& negative ùëó
ùëù&
" > ùëù'
" ‚áíùëê&
" > ùëê'
"
if each positive ùëñ, ùëó
Pos./Neg.
Annotation
Positive Pred.
ùëê)
"
ùëê*
"
ùëê+
"
ùëê,
"
Negative Pred.
Figure 1. (a) Current approaches threshold label certainties and
class-balanced cross-entropy loss for training edge detectors. (b)
With RankED, we propose a uniÔ¨Åed approach which ranks pos-
itives over negatives to handle the imbalance problem and sorts
positives with respect to their certainties.
There exist a non-trivial amount of variation between the
annotations produced by different human annotators, which
essentially creates noise in the supervisory signal.
These two issues of edge ground-truth data, namely the
imbalance and uncertainty, have long been known, how-
ever, efforts to address them have remained limited. For
the imbalance problem, although there is a vast literature on
long-tailed and imbalance learning (see, e.g., [46, 53]), re-
searchers have only explored using Dice Loss [10‚Äì12] and
weighted cross-entropy loss [20, 29, 39, 44, 49], which mit-
igate the problem to a certain extent. However, as we show
in this paper, they are far from Ô¨Ånding the optimal solu-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3239
Abstract
Detecting objects in 3D under various (normal and ad-
verse) weather conditions is essential for safe autonomous
driving systems. Recent approaches have focused on em-
ploying weather-insensitive 4D radar sensors and lever-
aging them with other modalities, such as LiDAR. How-
ever, they fuse multi-modal information without considering
the sensor characteristics and weather conditions, and lose
some height information which could be useful for localiz-
ing 3D objects. In this paper, we propose a novel frame-
work for robust LiDAR and 4D radar-based 3D object de-
tection. Specifically, we propose a 3D-LRF module that
considers the distinct patterns they exhibit in 3D space (e.g.,
precise 3D mapping of LiDAR and wide-range, weather-
insensitive measurement of 4D radar) and extract fusion
features based on their 3D spatial relationship.
Then,
our weather-conditional radar-flow gating network mod-
ulates the information flow of fusion features depending
on weather conditions, and obtains enhanced feature that
effectively incorporates the strength of two domains un-
der various weather conditions. The extensive experiments
demonstrate that our model achieves SoTA performance for
3D object detection under various weather conditions.
1. Introduction
Detecting 3D objects, which aims to classify the objects
and localize them in 3D coordinates, plays a crucial role in
various applications such as autonomous driving, robotic,
and drone systems [1, 8, 44]. Many attempts have been
made to utilize various sensors, such as camera, LiDAR,
and radar, for 3D object detection [9, 11, 16, 24, 31, 51, 53].
These methods are typically trained and tested in ideal au-
tonomous driving scenarios, demonstrating satisfactory per-
formance under normal conditions. Since real-world driv-
ing situations have diverse weather conditions, robust mod-
els operating in various conditions are needed.
*Code: https://github.com/yujeong-star/RL_3DOD.
(b) Point-based Fusion
LiDAR
Radar
Image
or
Fusion
Poor
Poor
4D Radar
Image
LiDAR
(d) BEV-based Fusion
Fusion
Poor
LiDAR
Radar
Image
or
Poor
(c) Pseudo-Image-based Fusion
LiDAR
Radar
Fusion
(e) Proposed Fusion
LiDAR
4D Radar
Fusion
Image
Weather-
informed
(a) Camera, LiDAR, 4D Radar in Adverse Weather
Figure 1. In adverse weather conditions, as depicted in (a), radar
exhibits the highest robustness, followed by LiDAR, while the im-
age is significantly degraded. Prior multi-modal 3D object detec-
tion research follows the fusion method in (b)-(d). They suffer
from inaccurate information from images or sub-optimal perfor-
mance due to the compression of critical 3D information from Li-
DAR and radar into BEV or pseudo-images for fusion. In con-
trast, our approach (e) effectively fuses LiDAR and 4D radar in
3D space, taking the strengths of each sensor through weather in-
formation, showing robust performance under adverse conditions.
Recently, several research has focused on addressing
these challenges by employing radar sensors capable of
handling various weather conditions [28], and has re-
leased datasets containing diverse weather environments
[4, 28, 35]. Moreover, research on a new novel sensor, 4D
radar, which includes height information, has been initiated
[22, 28, 38, 43]. Since the radar relies on radio waves, it
has the advantages of long-range detection and robustness
under adverse weather conditions. However, it does not pro-
vide precise distance or detailed 3D maps and struggles with
standalone deployment [12, 19, 43]. Therefore, ongoing
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15162
Abstract
Group robustness strategies aim to mitigate learned bi-
ases in deep learning models that arise from spurious cor-
relations present in their training datasets. However, most
existing methods rely on the access to the label distribution
of the groups, which is time-consuming and expensive to
obtain. As a result, unsupervised group robustness strate-
gies are sought. Based on the insight that a trained model‚Äôs
classiÔ¨Åcation strategies can be inferred accurately based on
explainability heatmaps, we introduce ExMap, an unsuper-
vised two stage mechanism designed to enhance group ro-
bustness in traditional classiÔ¨Åers. ExMap utilizes a cluster-
ing module to infer pseudo-labels based on a model‚Äôs ex-
plainability heatmaps, which are then used during training
in lieu of actual labels. Our empirical studies validate the
efÔ¨Åcacy of ExMap - We demonstrate that it bridges the per-
formance gap with its supervised counterparts and outper-
forms existing partially supervised and unsupervised meth-
ods. Additionally, ExMap can be seamlessly integrated with
existing group robustness learning strategies. Finally, we
demonstrate its potential in tackling the emerging issue of
multiple shortcut mitigation1.
1. Introduction
Deep neural network classiÔ¨Åers trained for classiÔ¨Åcation
tasks, have invited increased scrutiny from the research
community due to their overreliance on spurious correla-
tions present in the training data [4, 5, 9, 31, 38]. This is
related to the broader aspect of Shortcut Learning [10], or
the Clever Hans effect [15], where a model picks the path
of least resistance to predict data, thus relying on shortcut
features that are not causally linked to the label. The con-
sequence of this phenomenon is that, although such models
may demonstrate impressive mean accuracy on the test data,
they may still fail on challenging subsets of the data, i.e. the
groups [7, 8, 27]. As a result, group robustness is a natural
1Code available at https://github.com/rwchakra/exmap
objective to be met to mitigate reliance on spurious corre-
lations. Thus, instead of evaluating models based on mean
test accuracy, evaluating them on worst group accuracy has
been the recent paradigm [12, 21, 25, 40], resulting in the
emergence of group robustness techniques. By dividing a
dataset into pre-determined groups of spurious correlations,
classiÔ¨Åers are then trained to maximize the worst group ac-
curacy - As a result, the spurious attribute that the model is
most susceptible to is considered the shortcut of interest.
In Figure 1, we illustrate the group robustness paradigm.
Given a dataset, a robustness strategy takes as input the
group labels and retrains a base classiÔ¨Åer (such as Expected
Risk Minimization, i.e. ERM) to improve the worst group
accuracy (G3 in this case). GroupDRO [28] was one of the
early inÔ¨Çuential works that introduced the group robustness
paradigm. Further, it demonstrated a strategy that could in-
deed improve worst group accuracy. One limitation of this
approach was the reliance on group labels in the training
data, which was replaced with the reliance on group labels
in the validation data in successive works [13, 19]. How-
ever, while these efforts have made strides in enhancing the
accuracy of trained classiÔ¨Åers for underperforming groups,
many hinge on the assumption that the underlying groups
are known apriori and that the group labels are available,
which is often impractical in real-world contexts. An unsu-
pervised approach, as illustrated in Figure 1, would ideally
estimate pseudo-labels that could be inputs to any robust-
ness strategy, leading to improved worst group robustness.
An example of such a fully unsupervised worst group ro-
bustness approach is (GEORGE) [32]. GEORGE clusters
the penultimate layer features in a UMAP reduced space,
demonstrating impressive results on multiple datasets. In
this work, we instead show that clustering explainability
heatmaps instead, is more beneÔ¨Åcial in improving worst
group robustness. Intuitively, this stems from the fact that a
pixel-attribution based explainability method in input space
focuses only on the relevant image features (pixel space) in
the task, discarding other entangled features irrelevant for
the Ô¨Ånal prediction.
In our work, we circumvent the need for a group labeled
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12017
Abstract
Bird‚Äôs-eye View (BeV) representations have emerged as
the de-facto shared space in driving applications, offer-
ing a unified space for sensor data fusion and supporting
various downstream tasks. However, conventional models
use grids with fixed resolution and range and face compu-
tational inefficiencies due to the uniform allocation of re-
sources across all cells. To address this, we propose Point-
BeV, a novel sparse BeV segmentation model operating on
sparse BeV cells instead of dense grids. This approach of-
fers precise control over memory usage, enabling the use
of long temporal contexts and accommodating memory-
constrained platforms. PointBeV employs an efficient two-
pass strategy for training, enabling focused computation
on regions of interest. At inference time, it can be used
with various memory/performance trade-offs and flexibly
adjusts to new specific use cases. PointBeV achieves state-
of-the-art results on the nuScenes dataset for vehicle, pedes-
trian, and lane segmentation, showcasing superior perfor-
mance in static and temporal settings despite being trained
solely with sparse signals. We release our code with two
new efficient modules used in the architecture: Sparse Fea-
ture Pulling, designed for the effective extraction of features
from images to BeV, and Submanifold Attention, which en-
ables efficient temporal modeling. The code is available at
https://github.com/valeoai/PointBeV.
1. Introduction
Bird‚Äôs-eye View (BeV) representations are now ubiqui-
tously in driving applications.
Indeed, a top-view ego-
centric grid is not only a convenient shared space for fus-
ing inputs from multiple sensors [28, 39, 42, 49], but also
a space-aware representation relevant for many downstream
tasks such as detection [31, 34], segmentation [15, 39], fore-
casting [11], tracking [48], or planning [8, 18, 40]. BeV
segmentation encompasses a broad family of tasks such
as 2D instance segmentation [15], 3D instance segmenta-
tion [20, 21, 54], occupancy forecasting [22, 35] and online
*Work done at Valeo.ai.
0.5
1
2
4
31
32
CVT
Memory Usage (GiB) (‚Üê)
35
36
37
38
PointBeV
40k
23k
13k
8k
5.5k
4.1k
3.6k
Simple-BEV
BEVFormer
LaRa
IoU (‚Üí)
Figure 1. BeV vehicle IoU vs. memory footprint on nuScenes
[3] validation set. Models are evaluated without visibility filtering
(i.e all annotated vehicles are considered) at resolution 224 √ó 480.
The memory consumption is calculated using a 40GB A100 GPU.
The size of a dot represents the number of BeV points being eval-
uated, the smaller the better. PointBeV has the capacity to explore
various trade-offs between efficiency and performance by varying
the number of points being considered. The remaining points are
considered as zeros in the final prediction. Using PointBeV we
can achieve state-of-the-art performance with only a small portion
of the points and without losing performance.
mapping [27]. In this paper, we focus on BeV segmentation
from multiple cameras, in scenarios with or without past
frames, respectively referred to as temporal and static.
BeV representations are usually implemented using
grids of fixed resolution and range [9, 13, 15, 26, 31].
This limits their efficiency in terms of compute, even more
clearly when considering temporal tasks, where aggregat-
ing past frames for long horizons can be a very costly en-
deavor. Departing from these dense BeV grid approaches,
we present in this paper PointBeV, a camera-based BeV
segmentation model that operates on sparse BeV features.
Our approach offers control over the model‚Äôs memory us-
age by restricting the number of points considered, and en-
ables adaptive focus on specific regions of interest. No-
tably, we develop two modules for efficient sparse opera-
tions: the Sparse Feature Pulling module, which retrieves
features from multiple cameras using sparse coordinates,
and the Submanifold Attention module, adapted from sub-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15195
Abstract
Coordinate based implicit neural representations have
gained rapid popularity in recent years as they have been
successfully used in image, geometry and scene modeling
tasks. In this work, we present a novel use case for such
implicit representations in the context of learning anatomi-
cally constrained face models. Actor speciÔ¨Åc anatomically
constrained face models are the state of the art in both facial
performance capture and performance retargeting. Despite
their practical success, these anatomical models are slow to
evaluate and often require extensive data capture to be built.
We propose the anatomical implicit face model; an ensem-
ble of implicit neural networks that jointly learn to model
the facial anatomy and the skin surface with high-Ô¨Ådelity,
and can readily be used as a drop in replacement to con-
ventional blendshape models. Given an arbitrary set of skin
surface meshes of an actor and only a neutral shape with
estimated skull and jaw bones, our method can recover a
dense anatomical substructure which constrains every point
on the facial surface. We demonstrate the usefulness of our
approach in several tasks ranging from shape Ô¨Åtting, shape
editing, and performance retargeting.
1. Introduction
Deformable face models are an important tool in the arse-
nal of visual effects artists dealing with facial animation.
As they are ubiquitously used both in high-end production
workÔ¨Çows and lightweight consumer applications, build-
ing expressive face models for various applications contin-
ues to remain an active area of research [17]. Face mod-
els today can range from simple linear global shape mod-
els [4, 27, 29] to highly complex local models that incorpo-
rate the underlying facial anatomy through physical simula-
tion [15, 44, 48] or through anatomical constraints [47].
In this work, we concern ourselves primarily with the
high-quality facial animation workÔ¨Çow where actor spe-
ciÔ¨Åc linear blendshape models [27] continue to remain
the most commonly used tool for creating facial anima-
tions [10, 33, 47]. We propose a new class of actor speciÔ¨Åc
shape models named the Anatomical Implicit face Model
(AIM) which provides several unique advantages over the
existing actor speciÔ¨Åc face models, and can be used as a
drop-in replacement for traditional blendshape models.
An actor speciÔ¨Åc blendshape model is a collection of
3D shapes of the given actor performing a number of fa-
cial expressions, usually created by face scanning [2] or by
an artist. While the user-friendliness of such actor speciÔ¨Åc
blendshape models contributes to their wide adoption, it is
a well known limitation that such models often require hun-
dreds of shapes to accurately model complex facial defor-
mation [27]. To address these shortcomings, local blend-
shape models [10, 42, 47] were proposed. By splitting the
face into regions, and allowing the individual regions to de-
form independently, local shape models are able to capture
complex deformations with a limited number of shapes.
While local models address the lack of expressivity in
global shape models, state-of-the-art methods in facial per-
formance capture [47] and retargeting [10] often incorpo-
rate anatomical constraints on the facial surface to plausibly
restrict the range of the skin deformations. The anatom-
ical constraints employed by these models [10, 47] pro-
vide a few hidden advantages that end up contributing to-
wards their practical success. For example, in the context
of facial performance capture, Wu et al. [47] demonstrated
that including anatomical constraints derived from the re-
lationship between the facial skin and underlying bones
(skull and mandible) helps to separate the rigid and non-
rigid components of facial deformation, leading to better
face performance capture. In the context of facial perfor-
mance retargeting, Chandran et al. [10] made use of such an
anatomically constrained local face model to restrict a retar-
geted shape to lie within the space of anatomically plausible
shapes of the target actor.
Despite their practical success, anatomical constraints
are often formulated in practice as regularization terms that
have to be satisÔ¨Åed as part of complex optimization prob-
lems involving several objectives. As a result, Ô¨Åtting these
anatomical face models to a target scan or an image for in-
stance, is a computationally intensive procedure taking sev-
eral minutes per frame on a CPU, or requires hand crafted
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2220
Abstract
Superpixels play a crucial role in image processing by
partitioning an image into clusters of pixels with similar vi-
sual attributes. This facilitates subsequent image process-
ing tasks, offering computational advantages over the ma-
nipulation of individual pixels. While numerous overseg-
mentation techniques have emerged in recent years, many
rely on predefined initialization and termination criteria. In
this paper, a novel top-down superpixel segmentation algo-
rithm called Hierarchical Histogram Threshold Segmenta-
tion (HHTS) is introduced. It eliminates the need for ini-
tialization and implements auto-termination, outperform-
ing state-of-the-art methods w.r.t. boundary recall.
This
is achieved by iteratively partitioning individual pixel seg-
ments into foreground and background and applying inten-
sity thresholding across multiple color channels. The un-
derlying iterative process constructs a superpixel hierarchy
that adapts to local detail distributions until color infor-
mation exhaustion. Experimental results demonstrate the
superiority of the proposed approach in terms of bound-
ary adherence, while maintaining competitive runtime per-
formance on the BSDS500 and NYUV2 datasets. Further-
more, an application of HHTS in refining machine learning-
based semantic segmentation masks produced by the Seg-
ment Anything Foundation Model (SAM) is presented.
1. Introduction
Superpixel segmentation is an important preprocessing step
in computer vision. It groups pixels with similar proper-
ties to reduce the number of primitives and enhance ob-
ject representation. Various image processing tasks benefit
from high-quality superpixels, such as semantic segmenta-
tion [12, 33, 42, 52], object tracking [47], object categoriza-
tion [11], simultaneous localization and mapping (SLAM)
[6, 20], image segmentation [10, 34, 50], video segmen-
tation [39, 40], and stereo matching [37, 44]. There ex-
ists a wide range of approaches for image oversegmention
(a)
(b)
(c)
(d)
Figure 1. Visual comparison of 500 superpixels resulting from
(a, c) ETPS [previous], (b, d) HHTS [proposed] segmentation.
(a)
(b)
(c)
Figure 2. Visual comparison of semantic segment masks (a) orig-
inal image, (b) semantic segment (SAM ViT-H) [previous] and
(c) refined semantic segment (SAM + HHTS) [proposed]
[1, 5, 7‚Äì9, 18, 21, 28, 30, 46, 49, 51], often involving trade-
offs between boundary adherence, regular segment sizes,
and computational efficiency. However, there are also vari-
ous applications e.g. in computer graphics or medical imag-
ing [37, 50] requiring the maximization of boundary recall.
In this work, a novel hierarchical oversegmentation ap-
proach based on auto-terminating local histogram thresh-
olding is introduced, resulting in superpixels with signifi-
cantly higher boundary adherence. The corresponding re-
sults can be utilized e.g. for fine-tuning semantic segmen-
tation masks, as will be shown for the Segment Anything
Model (SAM) [15]. Most established superpixel algorithms
depend on a priori knowledge of image content and struc-
ture: Examples are seed-based methods requiring infor-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3195
Abstract
The study of complex human interactions and group ac-
tivities has become a focal point in human-centric computer
vision. However, progress in related tasks is often hindered
by the challenges of obtaining large-scale labeled datasets
from real-world scenarios. To address the limitation, we in-
troduce M3Act, a synthetic data generator for multi-view
multi-group multi-person human atomic actions and group
activities. Powered by Unity Engine, M3Act features mul-
tiple semantic groups, highly diverse and photorealistic im-
ages, and a comprehensive set of annotations, which facil-
itates the learning of human-centered tasks across single-
person, multi-person, and multi-group conditions.
We
demonstrate the advantages of M3Act across three core ex-
periments. The results suggest our synthetic dataset can sig-
nificantly improve the performance of several downstream
methods and replace real-world datasets to reduce cost.
Notably, M3Act improves the state-of-the-art MOTRv2 on
DanceTrack dataset, leading to a hop on the leaderboard
from 10th to 2nd place. Moreover, M3Act opens new re-
search for controllable 3D group activity generation. We
define multiple metrics and propose a competitive baseline
for the novel task. Our code and data are available at our
project page: http://cjerry1243.github.io/M3Act.
1. Introduction
Understanding collective human activities and social
groups carries significant implications across diverse do-
mains, as it contributes to bolstering public safety within
‚Ä†Work done during internship at Roblox
surveillance systems, ensuring safe navigation for au-
tonomous robots and vehicles amidst human crowds, and
enriching social awareness in human-robot interactions [8,
9, 11, 12, 21, 37, 49, 51]. However, the advancement in
related tasks is often impeded by the challenges of obtain-
ing large-scale human group activity datasets in real-world
scenarios with fine-grained multifaceted annotations.
Generating synthetic data is an emerging alternative to
collecting real-world data due to its capability of produc-
ing large-scale datasets with perfect annotations. Nonethe-
less, most synthetic datasets [4, 20, 40, 48, 53] are pri-
marily designed to facilitate human pose and shape esti-
mation. They can only provide data with independently-
animated persons, which is unsuitable for tasks in single-
group and multi-group conditions [51]. To address the lim-
itation, we propose M3Act, a synthetic data generator, with
multi-view multi-group multi-person human actions and
group activities.
As presented in Tab. 1, M3Act stands
out by offering comprehensive annotations including both
2D and 3D annotations as well as fine-grained person-level
and group-level labels, thereby making it an ideal synthetic
dataset generator to support tasks such as human activity
recognition and multi-person tracking across all listed real-
world datasets.
Illustrated in Fig. 1, our synthetic data generator features
multiple semantic groups, highly diverse and photorealistic
images, and a rich set of annotations. It encompasses 25
photometric 3D scenes, 104 HDRIs (High Dynamic Range
Images), 5 lighting volumes, 2200 human models, 384 an-
imations (categorized into 14 atomic action classes), and 6
group activities. For our experiments, We generated two
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
21922
Abstract
3D visual grounding involves matching natural language
descriptions with their corresponding objects in 3D spaces.
Existing methods often face challenges with accuracy in
object recognition and struggle in interpreting complex
linguistic queries,
particularly with descriptions that
involve multiple anchors or are view-dependent.
In
response,
we present the MiKASA (Multi-Key-Anchor
Scene-Aware) Transformer. Our novel end-to-end trained
model
integrates
a
self-attention-based
scene-aware
object
encoder
and
an
original
multi-key-anchor
technique, enhancing object recognition accuracy and
the understanding of spatial relationships. Furthermore,
MiKASA improves the explainability of decision-making,
facilitating error diagnosis.
Our model achieves the
highest overall accuracy in the Referit3D challenge
for both the Sr3D and Nr3D datasets,
particularly
excelling by a large margin in categories that require
viewpoint-dependent descriptions.
The source code and
additional resources for this project are available on
GitHub: https://github.com/dfki-av/MiKASA-3DVG
1. Introduction
3D visual grounding serves as a crucial component in the
intersection of natural language processing and computer
vision. This task aims to identify and localize objects within
a 3D space, using linguistic cues for spatial and semantic
grounding. While existing research has made significant
strides, challenges remain.
Key issues include the lack
of explainability in current models, limitations in object
recognition within point cloud data, and the complexity of
handling intricate spatial relationships.
Most existing 3D visual grounding models [2, 15, 16,
33, 40] consist of three parts:
(1) object encoder, (2)
text encoder, and (3) fusion model.
The object encoder
processes the provided point cloud and generates features
in the embedding space. However, because the points in a
(a) Target category: ‚Äúchair‚Äù
(b) ‚ÄúThe chair in the front of the
blue-lit monitor.‚Äù
Figure 1. Our methodology utilizes a dual-prediction framework
for 3D visual grounding. First, we assign a target category score
based on object categorization, as detailed in Fig. 1a. Next, a
spatial score is integrated according to the object‚Äôs alignment with
the textual description, as shown in Fig. 1b.
point cloud are unordered and inconsistent in sparsity [26],
it is not straightforward to apply the methodology typically
used for 2D images. An additional challenge is that 3D
point cloud datasets are not as extensive as those for 2D
images [10, 25], which makes it difficult for the models to
correctly recognize object categories. While enlarging the
dataset could conceivably improve performance, we refrain
from doing so to ensure a fair comparison with existing
state-of-the-art methods. Existing works [6, 18, 27] mainly
use different techniques such as noise addition, dropping
out colors, and transformations to expand the sample space.
Though these techniques may increase the stability of the
produced object embeddings, the improvement is limited.
Inspired by previous works [20, 21, 39] which aims to
solve object recognition problem, we leverage the fact
that data availability on objects within a specific space
can provide valuable insights into the characteristics and
relationships of their surrounding entities.
For instance,
when we come across a cuboid-shaped object in a
kitchen, we may naturally assume that it is a dishwasher.
Conversely, if we spot the same shape in a bathroom, it is
more plausible that it is a washing machine. Contextual
information is crucial in determining the identity of objects
and gives us a nuanced understanding of our surroundings.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14131
Abstract
The spiking cameras offer the benefits of high dynamic
range (HDR), high temporal resolution, and low data redun-
dancy. However, reconstructing HDR videos in high-speed
conditions using single-bit spikings presents challenges due
to the limited bit depth. Increasing the bit depth of the spik-
ings is advantageous for boosting HDR performance, but the
readout efficiency will be decreased, which is unfavorable
for achieving a high frame rate (HFR) video. To address
these challenges, we propose a readout mechanism to ob-
tain rolling-mixed-bit (RMB) spikings, which involves inter-
leaving multi-bit spikings within the single-bit spikings in a
rolling manner, thereby combining the characteristics of high
bit depth and efficient readout. Furthermore, we introduce
RMB-Net for reconstructing HDR and HFR videos. RMB-
Net comprises a cross-bit attention block for fusing mixed-bit
spikings and a cross-time attention block for achieving tem-
poral fusion. Extensive experiments conducted on synthetic
and real-synthetic data demonstrate the superiority of our
method. For instance, pure 3-bit spikings result in 3 times
of data volume, whereas our method achieves comparable
performance with less than 2% increase in data volume.
1. Introduction
Real-world scenes possess a significantly wider dynamic
range that exceeds the capability of conventional sensors.
Typical high dynamic range (HDR) video reconstruction
methods [3, 23, 24, 53] with conventional sensors encode
exposure times to capture images with alternating exposures.
And by fusing the low dynamic range (LDR) images taken
under different exposures, the pitfalls of underexposure and
# Equal contribution. ‚àóCorresponding author.
‚Ä† Majority of this work was done at Peking University.
Project page: https://github.com/yongqiye00/RMB-Net
Input
HDR and HFR video
(c)
‚Ä¶
7
1
0
Qc
time
rolling-mixed-bit spikings
‚Ä¶
N = 255
(b)
N = 1
multi-bit spikings
‚Ä¶
‚Ä¶
7
0
Qc
(a)
N = 1
N = 255
single-bit spikings
‚Ä¶
‚Ä¶
1
0
Qc
Figure 1. The HDR performance of a spiking camera is closely tied
to the bit depth of the spikings. (a) From left to right: Single-bit
quantization (Qc is the accumulated photon electrons), diagram of
single-bit spiking planes, and reconstructed image by accumulating
N spiking planes. Increasing N to 255 significantly boosts HDR
performance. (b) HDR can be boosted by reading out multi-bit spik-
ings. However, multi-bit spikings decrease the readout efficiency,
which is not conducive to obtaining HFR videos. (c) The proposed
RMB spikings with time-varying quantization. We further recon-
struct HDR and HFR videos from RMB spikings.
overexposure are alleviated. This kind of approach has a
dilemma between the frame rate and exposure time [20], i.e.,
long exposure restricts the improvement of frame rate [2],
which makes it challenging to capture high frame rate (HFR)
videos with conventional sensors in high-speed scenes.
Recent advancements in the field of HDR and HFR pho-
tography have benefited from the integration of neuromor-
phic sensors such as event cameras [6, 29, 30] and spiking
cameras [2, 21]. These sensors offer appealing characteris-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
25117
