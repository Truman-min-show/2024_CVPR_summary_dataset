[
{
    "prompt": "summary: Abstract\nUnsupervised (US) video anomaly detection (VAD) in\nsurveillance applications is gaining more popularity re-\ncently due to its practical real-world applications.\nAs\nsurveillance videos are privacy sensitive and the availabil-\nity of large-scale video data may enable better US-VAD sys-\ntems, collaborative learning can be highly rewarding in this\nsetting. However, due to the extremely challenging nature\nof the US-VAD task, where learning is carried out without\nany annotations, privacy-preserving collaborative learning\nof US-VAD systems has not been studied yet. In this pa-\nper, we propose a new baseline for anomaly detection ca-\npable of localizing anomalous events in complex surveil-\nlance videos in a fully unsupervised fashion without any la-\nbels on a privacy-preserving participant-based distributed\ntraining configuration. Additionally, we propose three new\nevaluation protocols to benchmark anomaly detection ap-\nproaches on various scenarios of collaborations and data\navailability. Based on these protocols, we modify existing\nVAD datasets to extensively evaluate our approach as well\nas existing US SOTA methods on two large-scale datasets\nincluding UCF-Crime and XD-Violence. All proposed eval-\nuation protocols, dataset splits, and codes are available\nhere: https://github.com/AnasEmad11/CLAP.\n1. Introduction\nRecent years have seen a surge in federated learning based\nmethods, where the goal is to enable collaborative training\nof machine learning models without transferring any train-\ning data to a central server. This direction of research in ma-\nchine learning is of notable importance as it enables learn-\ning with multiple participants that can contribute data with-\nout compromising privacy. Several researchers have studied\nfederated learning for different applications such as medical\ndiagnosis [1, 3, 6, 22], network security [10, 19, 21, 30], and\nlarge-scale classification models [4, 14, 42].\nAnomaly detection in surveillance videos, being one\n...\nCentral\nParameter Sharing\nServer\nServer\nConventional Centralized Training\nOurs: Collaborative Training\n...\n...\n...\nFigure 1. a) Conventional central training requires all training data\nto be on the server to carry out the training. This setting cannot en-\nsure privacy, thus hindering collaborations between different enti-\nties holding large-scale surveillance data. b) Our proposed unsu-\npervised video anomaly detection technique does not require the\ntransfer of training data between the server and participants, thus\nensuring complete privacy.\nof the large-scale applications of computer vision, may\ngreatly benefit from autonomous collaborative training al-\ngorithms.\nVAD in surveillance videos is privacy sensi-\ntive and may involve data belonging to several organiza-\ntions/establishments. This may result in hectic bureaucratic\nprocesses to obtain data from each establishment for cen-\ntralized training. For example, the police department of a\ncity may not be willing to share street surveillance videos\ndue to privacy concerns of the general public, or a daycare\nfacility may have to obtain the consent of all parents to be\nallowed to share its CCTV footage. Such restrictions may\nhinder the possibility of obtaining large-scale data to train\nefficient anomaly detectors making a central training requir-\ning all training data the least preferred option in the real-\nworld scenarios. Unfortunately, to the best of our knowl-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n12416",
    "chosen": "Al-lahham Collaborative Learning of Anomalies with Privacy CLAP for Unsupervised Video",
    "rejected": "Welcome to a new assessment of surveillance video collaborations, which aims to improve the availability of large-scale video data. Here are some of the key points to our findings. Each of them is based on the US SOTA system."
},
{
    "prompt": "summary: Abstract\nThe reflective nature of the human eye is an underappre-\nciated source of information about what the world around\nus looks like. By imaging the eyes of a moving person, we\ncapture multiple views of a scene outside the camera’s di-\nrect line of sight through the reflections in the eyes. In this\npaper, we reconstruct a radiance field beyond the camera’s\nline of sight using portrait images containing eye reflec-\ntions. This task is challenging due to 1) the difficulty of ac-\ncurately estimating eye poses and 2) the entangled appear-\nance of the iris textures and the scene reflections. To address\nthese, our method jointly optimizes the cornea poses, the ra-\ndiance field depicting the scene, and the observer’s eye iris\ntexture. We further present a regularization prior on the iris\ntexture to improve scene reconstruction quality. Through\nvarious experiments on synthetic and real-world captures\nfeaturing people with varied eye colors, and lighting con-\nditions, we demonstrate the feasibility of our approach to\nrecover the radiance field using cornea reflections.\n*Equal contribution\n1. Introduction\nThe human eye is a remarkable organ that enables vi-\nsion and holds valuable information about the surrounding\nworld. While we typically use our own eyes as two lenses\nto focus light onto the photosensitive cells composing our\nretina, we would also capture the light reflected from the\ncornea if we look at someone else’s eyes. When we use\na camera to image the eyes of another, we effectively turn\ntheir eyes into a pair of mirrors in the overall imaging sys-\ntem. Since the light that reflects off the observer’s eyes\nshare the same source as the light that reaches their retina,\nour camera can form images containing information about\nthe surrounding world the observer sees.\nPrior studies have explored recovering a panoramic im-\nage of the world the observer sees and simple 3D struc-\ntures like boxes that the observer is looking at from man-\nually specified correspondences from a single image of two\neyes [27, 28]. Follow-up works have further explored ap-\nplications such as personal identification [10, 29], detecting\ngrasp posture [53], focused object estimation [40], illumina-\ntion estimation [48], and relighting [26]. Given the recent\nadvancements in 3D vision and graphics, we wonder: Can\nwe do more than reconstruct a single panoramic environ-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4864",
    "chosen": "Alzayer Seeing the World through Your Eyes",
    "rejected": "In our series of letters from African journalists, Dr John Wright looks at how the human eye works to reconstruct a radiance field beyond the sight of an moving person using portrait images containing eye reflec- tions."
},
{
    "prompt": "summary: Abstract\nWe introduce FaceTalk1, a novel generative approach de-\nsigned for synthesizing high-fidelity 3D motion sequences\nof talking human heads from input audio signal. To capture\nthe expressive, detailed nature of human heads, including\nhair, ears, and finer-scale eye movements, we propose to\ncouple speech signal with the latent space of neural para-\nmetric head models to create high-fidelity, temporally co-\nherent motion sequences. We propose a new latent diffu-\nsion model for this task, operating in the expression space of\nneural parametric head models, to synthesize audio-driven\nrealistic head sequences. In the absence of a dataset with\ncorresponding NPHM expressions to audio, we optimize for\nthese correspondences to produce a dataset of temporally-\noptimized NPHM expressions fit to audio-video recordings\nof people talking.\nTo the best of our knowledge, this is\nthe first work to propose a generative approach for realis-\ntic and high-quality motion synthesis of volumetric human\nheads, representing a significant advancement in the field of\naudio-driven 3D animation. Notably, our approach stands\nout in its ability to generate plausible motion sequences that\ncan produce high-fidelity head animation coupled with the\n1Project Page:\nhttps://shivangi- aneja.github.io/\nprojects/facetalk\nNPHM shape space. Our experimental results substantiate\nthe effectiveness of FaceTalk, consistently achieving supe-\nrior and visually natural motion, encompassing diverse fa-\ncial expressions and styles, outperforming existing methods\nby 75% in perceptual user study evaluation.\n1. Introduction\nModeling 3D animation of humans has a wide range of\napplications in the realm of digital media, including an-\nimated movies, computer games, and virtual assistants.\nIn recent years, there have been numerous works propos-\ning generative approaches for motion synthesis of human\nbodies, enabling the animation of human skeletons condi-\ntioned on various signals such as action [3, 25, 46], lan-\nguage [1, 4, 36, 47, 64, 77] and music [2, 67]. While human\nfaces are critical to synthesis of humans, generative syn-\nthesis of 3D faces in motion has focused on 3D morphable\nmodels (3DMMs) leveraging linear blendshapes [7, 40] to\nrepresent head motion and expression. Such models char-\nacterize a disentangled space of head shape and motion,\nbut lack the capacity to comprehensively represent the com-\nplexity and fine-grained details of human face geometry in\nmotion (e.g., hair, skin furrowing during motion, etc.).\nThus, we propose to represent animated head sequences\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21263",
    "chosen": "Aneja FaceTalk Audio-Driven Motion Diffusion for Neural Parametric Head Models",
    "rejected": "This is a full transcript of FaceTalk1, the generative approach de- signed for making high-fidelity 3D motion sequences of talking human heads from input audio signal. Here is the full text of the project."
},
{
    "prompt": "summary: Abstract\nAutomating visual inspection in industrial production\nlines is essential for increasing product quality across var-\nious industries.\nAnomaly detection (AD) methods serve\nas robust tools for this purpose. However, existing pub-\nlic datasets primarily consist of images without anomalies,\nlimiting the practical application of AD methods in pro-\nduction settings. To address this challenge, we present (1)\nthe Valeo Anomaly Dataset (VAD), a novel real-world in-\ndustrial dataset comprising 5000 images, including 2000\ninstances of challenging real defects across more than 20\nsubclasses.\nAcknowledging that traditional AD methods\nstruggle with this dataset, we introduce (2) Segmentation-\nbased Anomaly Detector (SegAD). First, SegAD leverages\nanomaly maps as well as segmentation maps to compute\nlocal statistics. Next, SegAD uses these statistics and an\noptional supervised classifier score as input features for a\nBoosted Random Forest (BRF) classifier, yielding the final\nanomaly score. Our SegAD achieves state-of-the-art perfor-\nmance on both VAD (+2.1% AUROC) and the VisA dataset\n(+0.4% AUROC). The code and the models are publicly\navailable1.\n1. Introduction\nWithin the manufacturing process, industrial visual inspec-\ntion plays a crucial role in identifying defects in produced\ncomponents. This operation holds significant importance in\nminimizing costs by identifying and removing faulty parts\nearly in the production stages and, more importantly, in pre-\nventing the dispatch of defective components to customers.\nTraditionally, this task has relied on human operators; how-\never, the likelihood of overlooking certain defects can be as\nhigh as 25% for specific defect types [9].\nWhen the inspected product comprises numerous com-\nponents, its examination may create a bottleneck in the pro-\nduction process, causing delays across the entire line. While\n1https://github.com/abc-125/segad\nconventional computer vision methods applied to this task\ndemonstrate superior speed and lower error rates compared\nto human operators [16], their inflexibility and lack of sat-\nisfactory accuracy [15] limit their effectiveness.\nIndustrial deep learning anomaly detection has been an\nactive research field in recent years. Most of anomaly de-\ntection methods use only good images for training and try\nto detect deviations from the training data [1, 7, 23, 29].\nDevelopment of the new methods is restrained by pub-\nlicly available datasets. Recent industrial anomaly detec-\ntion datasets typically contain approximately one hundred\n(or even fewer) abnormal images, showcasing defects in the\ntesting set only [2, 32]. This poses a challenge for super-\nvised anomaly detection methods aiming to utilize both nor-\nmal and defective parts during training. Supervised models\noften undergo training with just ten abnormal images from\nthe testing set, resulting in overfitting and reduced sensitiv-\nity to previously unseen defects [28].\nIn response to the limitations of current datasets, we\nintroduce and publicly release VAD (Valeo Anomaly\nDataset), which contains 1000 bad and 2000 good parts in\nthe training set and 1000 bad and 1000 good parts in the\ntest set, see Fig. 1(a). Among the defective parts used for\ntesting, 165 contain specific defect types not present in the\ntraining data; these parts are explicitly labeled in the re-\nleased dataset. All images in VAD are captured from an\nactual production line, showcasing a diverse range of de-\nfects, from highly obvious to extremely subtle. This dataset\nbridges the gap between the academic community and the\nindustry, offering researchers the chance to advance the per-\nformance of methods in tackling more intricate real-world\nchallenges.\nCurrent approaches to supervised anomaly detection ei-\nther yield unsatisfactory results; see Tab. 3, or demand\npixel-level labels for defective parts, as seen in [28, 31].\nConsequently, we introduce a novel method named SegAD\n(Segmentation-based Anomaly Detector), which is de-\nscribed in Fig. 1(b). The employed approach eliminates the\nneed for pixel-level labels, requiring only a flag for each\nimage. Anomaly map scores from each segment are used\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17754",
    "chosen": "Baitieva Supervised Anomaly Detection for Complex Industrial Images",
    "rejected": "Welcome to the Valeo Anomaly Dataset, which aims to identify defects in industrial production lines. Here is a guide to how it works and what it takes to tackle the challenges of visual inspection."
},
{
    "prompt": "summary: Abstract\nNon-isometric shape correspondence remains a funda-\nmental challenge in computer vision. Traditional methods\nusing Laplace-Beltrami operator (LBO) eigenmodes face\nlimitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel ap-\nproach of combining the non-orthogonal extrinsic basis of\neigenfunctions of the elastic thin-shell hessian with the in-\ntrinsic ones of the LBO, creating a hybrid spectral space\nin which we construct functional maps. To this end, we\npresent a theoretical framework to effectively integrate non-\northogonal basis functions into descriptor- and learning-\nbased functional map methods. Our approach can be in-\ncorporated easily into existing functional map pipelines\nacross varying applications and can handle complex de-\nformations beyond isometries. We show extensive evalua-\ntions across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our ap-\nproach achieves up to 15% better mean geodesic error for\nnon-isometric correspondence settings and up to 45% im-\nprovement in scenarios with topological noise. Code is avail-\nable at: https://hybridfmaps.github.io/\n∗Equal Contribution\n\u0000 : {lennart.bastian,yizheng.xie}@tum.de\n1. Introduction\nEstablishing dense correspondences between 3D shapes is\na cornerstone for numerous computer vision and graphics\ntasks such as object recognition, character animation, and\ntexture transfer. The complexity of this task varies signifi-\ncantly depending on the nature of the transformation a shape\nundergoes. Many classic correspondence methods leverage\nthat rigid transformations can be represented in six degrees\nof freedom in R3 and preserve the Euclidean distance be-\ntween pairs of points. Iterative closest point (ICP) [4], and\nits variations [25, 40], which alternate between transforma-\ntion and correspondence estimation, are very popular due\nto their simplicity. In this setting, local extrinsic surface\nproperties in the embedding space stay invariant under rigid\ntransformations such that they can be used as features during\noptimization, for example, the change of normals. For the\nwider class of isometric deformations (w.r.t. the geodesic\ndistance), the relative embedding of the shape can change\nsignificantly, and Euclidean distances between points may\nnot be preserved. In this class, only intrinsic properties –\nthose that do not depend on a specific embedding of the\nsurface – stay invariant, and the correspondence problem\nbecomes much harder due to the quadratic size of the so-\nlution space. For example, solving a quadratic assignment\nproblem preserving geodesic distances [24] or heat kernel\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3313",
    "chosen": "Bastian Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching",
    "rejected": "In our series of letters from African journalists, Zeinab Owens looks at the challenges facing non-isometric shape correspondence between 3D objects. Here is a guide to how it works."
},
{
    "prompt": "summary: Abstract\nLearning compatible representations enables the inter-\nchangeable use of semantic features as models are updated\nover time. This is particularly relevant in search and re-\ntrieval systems where it is crucial to avoid reprocessing of\nthe gallery images with the updated model. While recent\nresearch has shown promising empirical evidence, there\nis still a lack of comprehensive theoretical understanding\nabout learning compatible representations. In this paper, we\ndemonstrate that the stationary representations learned by\nthe d-Simplex fixed classifier optimally approximate compat-\nibility representation according to the two inequality con-\nstraints of its formal definition. This not only establishes a\nsolid foundation for future works in this line of research but\nalso presents implications that can be exploited in practical\nlearning scenarios. An exemplary application is the now-\nstandard practice of downloading and fine-tuning new pre-\ntrained models. Specifically, we show the strengths and criti-\ncal issues of stationary representations in the case in which\na model undergoing sequential fine-tuning is asynchronously\nreplaced by downloading a better-performing model pre-\ntrained elsewhere. Such a representation enables seamless\ndelivery of retrieval service (i.e., no reprocessing of gallery\nimages) and offers improved performance without opera-\ntional disruptions during model replacement. Code available\nat: https://github.com/miccunifi/iamcl2r.\n1. Introduction\nBy learning powerful internal feature representations from\ndata, Deep Neural Networks (DNNs) [1–4] have made\ntremendous progress in some of the most challenging\nsearch tasks such as face recognition [5–9], person re-\nidentification [10–12], image retrieval [13–15] and this sig-\nnificance also extends to a variety of other data modalities\n[16, 17]. Although all of the works mentioned above have\nfocused on learning feature representations from static and,\n𝜙𝑇\n \ntask 2\ntask 𝑇\ntask 1\n𝜙2\n \n𝜙1\n \nStationary \nRepresentation\n(⟹ Compatible)\n𝜙 \nFigure 1. Improved Asynchronous Model Compatible Lifelong\nLearning Representation (IAM-CL2R pronounced “I am clear”). In\nthe process of lifelong learning, a model is sequentially fine-tuned\nand asynchronously replaced with improved third-party models\nthat are pre-trained externally. Stationary representations ensure\nseamless retrieval services and better performance, without the\nneed to reprocess gallery images.\nmore recently, dynamic datasets [18–21], the now-standard\npractice is downloading and fine-tuning representations from\nmodels pre-trained elsewhere [22, 23]. These “third-party”\npre-trained models often incorporate new data, utilize alter-\nnative architectures, adopt different loss functions or more\nin general provide novel methodologies. Whether applied\nindividually or combined, these advancements aim to en-\ncapsulate the field’s rapid progress within a single unified\nmodel [24]. This greatly facilitates the exploitation of in-\nternally learned semantic representations, particularly as\nmodels, datasets, and computational infrastructure continue\nto expand in size, complexity, and cost [25, 26].\nThe challenge of fully exploiting such standard practice\nin retrieval/search systems has to deal with the underlying\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n28793",
    "chosen": "Biondi Stationary Representations Optimally Approximating Compatibility and Implications for Improved Model",
    "rejected": "In our series of letters from academics, scientists and researchers have published a detailed understanding of learning compatible representations (IAM-CL2R) . Here are some of the key findings."
},
{
    "prompt": "summary: Abstract\nKnowledge of lane topology is a core problem in au-\ntonomous driving. Aerial imagery can provide high res-\nolution, quickly updatable lane source data but detecting\nlanes from such data has so far been an expensive man-\nual process or, where automated solutions exist, undriv-\nable and requiring of downstream processing.\nWe pro-\npose a method for large-scale lane topology extraction from\naerial imagery while ensuring that the resulting lanes are\nrealistic and drivable by introducing a novel B´ezier Graph\nshared parameterisation of B´ezier curves. We develop a\ntransformer-based model to predict these B´ezier Graphs\nfrom input aerial images, demonstrating competitive results\non the UrbanLaneGraph dataset. We demonstrate that our\nmethod generates realistic lane graphs which require both\nminimal input, and minimal downstream processing. We\nmake our code publicly available at https://github.\ncom/driskai/BGFormer\n1. Introduction\nAutonomous Vehicles (AVs) require knowledge of their sur-\nroundings to operate. So far, all systems not reliant on a\nsafety driver have required pre-built High Deﬁnition maps\n(HD maps). HD maps can deliver a strong prior about the\nroad and lane topology, e.g. so the AV can navigate amid\nocclusions. But creation of HD maps is time-consuming\nand expensive, typically requiring a ﬂeet of road vehicles\nequipped with LiDAR and cameras followed by extensive\nmanual curation and human annotation [18].\nOne of the core components of an HD map is precise\nlane geometry and topology data. Previous works have at-\nFor the purpose of open access, the author has applied a Creative Com-\nmons Attribution (CC BY) license to any Author Accepted Manuscript\nversion arising. This work was funded by the Centre for Connected and\nAutonomous Vehicles, delivered by Innovate UK, part of UK Research\nand Innovation.\nFigure 1. Our trained model detecting a lane B´ezier Graph on an\nintersection. B´ezier endpoints P 0,3 shown in cyan, control points\nP 1,2 in white, resulting curves in red.\ntempted to automate the generation of this data from on-\nboard sensors [4, 5, 10] or - recently - from aerial imagery\n[3, 9, 30, 31]. Generating lane-level data from aerial im-\nagery is of particular interest since this can be efﬁciently\nobtained at large scale using Unmanned Aerial Vehicles\n(UAVs) or satellites. In this way, AVs could be equipped\nwith regularly updated HD maps that cover a large area.\nHowever, these approaches are not without their chal-\nlenges. Lane geometry and topology is typically predicted\nin the form of a lane graph: a graph in which nodes repre-\nsent a discrete sampling of lane centre lines, with edges rep-\nresenting connectivity. Existing methods typically predict\nthe position of each node in 2D space, but this frequently\nresults in noisy, non-physical lane centre lines requiring ex-\ntensive downstream processing and human oversight.\nWe improve on these methods by introducing a shared\nparameterisation of cubic B´ezier curves in a graph struc-\nture that we refer to as a B´ezier Graph.\nBy associating\nB´ezier direction vectors with nodes, we enforce a strong\nprior that lane direction should be continuous at boundaries.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n15365",
    "chosen": "Blayney Bezier Everywhere All at Once Learning Drivable Lanes as Bezier",
    "rejected": "Welcome to the latest chapter in a series of letters from the author Driskai. Here is the full text of his work - The theory of lane topology, which aims to solve the problem."
},
{
    "prompt": "summary: Abstract\nIn the context of autonomous navigation of terrestrial robots,\nthe creation of realistic models for agent dynamics and sens-\ning is a widespread habit in the robotics literature and in\ncommercial applications, where they are used for model\nbased control and/or for localization and mapping. The\nmore recent Embodied AI literature, on the other hand, fo-\ncuses on modular or end-to-end agents trained in simulators\nlike Habitat or AI-Thor, where the emphasis is put on photo-\nrealistic rendering and scene diversity, but high-fidelity robot\nmotion is assigned a less privileged role. The resulting\nsim2real gap significantly impacts transfer of the trained\nmodels to real robotic platforms. In this work we explore\nend-to-end training of agents in simulation in settings which\nminimize the sim2real gap both, in sensing and in actuation.\nOur agent directly predicts (discretized) velocity commands,\nwhich are maintained through closed-loop control in the real\nrobot. The behavior of the real robot (including the under-\nlying low-level controller) is identified and simulated in a\nmodified Habitat simulator. Noise models for odometry and\nlocalization further contribute in lowering the sim2real gap.\nWe evaluate on real navigation scenarios, explore different\nlocalization and point goal calculation methods and report\nsignificant gains in performance and robustness compared\nto prior work.\n1. Introduction\nPoint goal navigation of terrestrial robots in indoor buildings\nhas traditionally been addressed in the robotics community\nwith mapping and planning [6, 30, 50], which led to solu-\ntions capable of operating on robots in real environments.\nThe field of computer vision and embodied AI has addressed\nthis problem through large-scale machine learning in sim-\nulated 3D environments from reward with RL [24, 34] or\nwith imitation learning [14]. Learning from large-scale data\nallows the agent to pick up more complex regularities, to\nprocess more subtle cues and therefore (in principle) to be\nmore robust, to exploit data regularities to infer hidden and\noccluded information, and generally to learn more powerful\nTraining/sim (ours)\nTest/real\nxt\n1\n2\n1\n2 xt+1\nTraining/sim (classical)\nxt\n1\n2 xt+1\nPolicy: \n∆π\nOnboard ctrl\n∆φ\n333ms/3 Hz (Unknown)\nPolicy: \n∆π\nDyn. model\n∆φ\n333ms/3 Hz 33ms/30 Hz\nPolicy: \n∆π\nconstant\n333ms/3 Hz velocities\nPiecewise\nFigure 1. Efficient navigation with policies end-to-end trained in\n3D photorealistic simulators requires closing the sim2real gap in\nsensing and actuation. Efficiency demands that the robot continues\nto move during decision taking (as opposed to stopping for each\nsensing operation), and this requires a realistic motion model in\nsimulation allowing the agent to internally anticipate its future state.\nThis requirement is exacerbated by the delay between sensing\n①and actuation ②caused by the computational complexity of\nhigh-capacity deep networks (visual encoders, policy). To model\nrealistic motion while training in simulation, we create a 2nd order\ndynamical model running with higher frequency, which models\nthe robot and its low-level closed-loop controller. We identify the\nmodel from real data and add it to the Habitat [44] Simulator.\ndecision rules. In this context, the specific task of point goal\nnavigation (navigation to coordinates) is now sometimes\nconsidered “solved” in the literature [38], incorrectly, as we\nargue. While the machine learning and computer vision\ncommunity turns its attention towards the exciting goal of\nintegrating language models into navigation [15, 23], we\nthink that further improvements are required to make trained\nagents perform reliably in real environments with sufficient\nspeed.\nExperiments and evaluations of trained models in real\nenvironments and the impact of the sim2real gap do exist in\nthe Embodied AI literature [11, 20, 43], but they are rare and\nwere performed in restricted environments. Known models\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17837",
    "chosen": "Bono Learning to Navigate Efficiently and Precisely in Real Environments",
    "rejected": "In our series of letters from academics, scientists and developers, we look at the challenges facing terrestrial robots in indoor buildings. Here is a summary of what we learned about the problem."
},
{
    "prompt": "summary: Abstract\nWe study the underexplored but fundamental problem of\nmachine understanding of abstract freehand scene sketches.\nWe introduce a sketch encoder that ensures a semantically-\naware feature space, which we evaluate by testing its per-\nformance on a semantic sketch segmentation task. To train\nour model, we rely only on bitmap sketches accompanied\nby brief captions, avoiding the need for pixel-level anno-\ntations. To generalize to a large set of sketches and cat-\negories, we build upon a vision transformer encoder pre-\ntrained with the CLIP model. We freeze the text encoder and\nperform visual-prompt tuning of the visual encoder branch\nwhile introducing a set of critical modifications. First, we\naugment the classical key-query (k-q) self-attention blocks\nwith value-value (v-v) self-attention blocks. Central to our\nmodel is a two-level hierarchical training that enables ef-\nficient semantic disentanglement: The first level ensures\nholistic scene sketch encoding, and the second level focuses\non individual categories. In the second level of the hierar-\nchy, we introduce cross-attention between the text and vi-\nsion branches.\nOur method outperforms zero-shot CLIP\nsegmentation results by 37 points, reaching a pixel accu-\nracy of 85.5% on the FS-COCO sketch dataset. Finally, we\nconduct a user study that allows us to identify further im-\nprovements needed over our method to reconcile machine\nand human understanding of freehand scene sketches.\n1. Introduction\nEven a quick sketch can convey rich information about what\nis relevant in a visual scene: what objects there are and how\nthey are arranged. However, little work has been devoted\nto the task of machine scene sketch understanding, largely\ndue to a lack of data. Understanding sketches with meth-\nods designed for images is challenging because sketches\nhave very different statistics from images – they are sparser\nand lack detailed color and texture information. Moreover,\nsketches contain abstraction at multiple levels: the holis-\nCLIP zero shot\nsegmentation\nCLIP zero shot\nsegmentation\nOur\nsegmentation\nOur\nsegmentation\nA giraffe and a zebra are \nstanding on the grass.\nA man with a kite and a tree \nin the background.\nFigure 1. Comparison of the segmentation result obtained with\nCLIP visual encoder features and features from our model.\ntic scene level and the object level. Here we explore the\npromise of two main ideas: (1) the use of language to guide\nthe learning of how to parse scene sketches and (2) a two-\nlevel training network design for holistic scene understand-\ning and individual categories recognition.\nFreehand sketches can be represented as a sequence or\ncloud of individual strokes, or as a bitmap image. As one\nof the first works on scene sketch understanding, we target\na general setting where we assume only the availability of\nbitmap representations. We also aim at the method that can\ngeneralize to a large number of scenes and object categories.\nTo this end, we build our sketch encoder on a Visual Trans-\nformer (ViT) encoder pre-trained with a popular CLIP [44]\nfoundation model (Fig. 1). We propose a two-level hierar-\nchical training of our network, where the two levels (“Holis-\ntic” and “Category-level”) share the weights of our visual\nencoder. The first level focuses on ensuring that our model\ncan capture holistic scene understanding (Fig. 2: I. Holis-\ntic), while the second level ensures that the encoder can effi-\nciently encode and distinguish individual categories (Fig. 2:\nII. Category-level). We avoid reliance on tedious user per-\npixel annotations by leveraging sketch-caption pairs from\nthe FS-COCO dataset [9], and aligning the visual tokens of\nsketch patches with textual tokens from the sketch captions,\nusing triplet loss training. We strengthen the alignment by\nintroducing sketch-text cross-attention in the second level\nof the network’s hierarchy (Fig. 2: g.). Additionally, we in-\ntroduce a modified self-attention computation to the visual\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n4176",
    "chosen": "Bourouis Open Vocabulary Semantic Scene Sketch Understanding",
    "rejected": "In our series of letters from African journalists, scientists and academics examine the understanding of freehand scene sketches. Here are some of the key steps to reconcile machine and human Understanding."
},
{
    "prompt": "summary: Abstract\nVision-language foundation models have shown remark-\nable performance in various zero-shot settings such as im-\nage retrieval, classification, or captioning. But so far, those\nmodels seem to fall behind when it comes to zero-shot lo-\ncalization of referential expressions and objects in images.\nAs a result, they need to be fine-tuned for this task.\nIn\nthis paper, we show that pretrained vision-language (VL)\nmodels allow for zero-shot open-vocabulary object local-\nization without any fine-tuning. To leverage those capabil-\nities, we propose a Grounding Everything Module (GEM)\nthat generalizes the idea of value-value attention introduced\nby CLIPSurgery [17] to a self-self attention path. We show\nthat the concept of self-self attention corresponds to cluster-\ning, thus enforcing groups of tokens arising from the same\nobject to be similar while preserving the alignment with\nthe language space. To further guide the group formation,\nwe propose a set of regularizations that allows the model\nto finally generalize across datasets and backbones.\nWe\nevaluate the proposed GEM framework on various bench-\nmark tasks and datasets for semantic segmentation. GEM\nnot only outperforms other training-free open-vocabulary\nlocalization methods, but also achieves state-of-the-art re-\nsults on the recently proposed OpenImagesV7 large-scale\nsegmentation benchmark. 1\n1. Introduction\nVision-Language models, trained on large-scale web-based\ndatasets such as WIT-400M [29], LAION400M [30], or\nmetaclip-400M [35] with image-text supervision only, have\nso far shown a remarkable set of capabilities. These mod-\nels such as CLIP [29], OpenCLIP [30], BLIP [15], or re-\ncently MetaCLIP [35] exhibit the ability to generalize to a\nbroad range of downstream tasks like zero-shot image clas-\nsification [6, 12, 29], visual question answering [13], action\nrecognition [38, 40], image captioning [15, 16], and view\nsynthesis [11]. However, models trained with image-level\nobjectives such as contrastive loss, image-text matching, or\n1Code is available at https://github.com/WalBouss/GEM.\nCLIP\nGEM (ours)\nCLIPSurgery\nMaskCLIP\nBowl\nMan\nHamburger\nJet ski\nFigure 1. Qualitative results of training-free methods: given a text\nprompt, the similarity of each image token with the prompt is cal-\nculated (red:high, blue:low). The proposed GEM method provides\nimproved grouping and alignment compared to other approaches.\nimage captioning struggle to maintain their zero-shot ca-\npabilities for tasks related to visual localization [17, 42].\nEven worse, when prompting such models, e.g., for spe-\ncific objects, they often exhibit an inverse vision-language\nrelation: the prompt embedding has a larger distance from\nimage patches containing the object compared to patches of\nthe background, as shown in Figure 1.\nIn order to leverage vision-language models (VLMs)\nto localize objects in an open-vocabulary setting, different\nsets of approaches have been proposed. The first line of\nwork trains models to detect or segment regions in an im-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3828",
    "chosen": "Bousselham Grounding Everything Emerging Localization Properties in Vision-Language Transformers",
    "rejected": "In our series of letters from African journalists, academics and readers, we look at a range of vision-language foundation models for zero-shot objects in images. Here are some of the key points."
},
{
    "prompt": "summary: Abstract\nText-to-image diffusion models have recently received in-\ncreasing interest for their astonishing ability to produce\nhigh-ﬁdelity images from solely text inputs.\nSubsequent\nresearch efforts aim to exploit and apply their capabili-\nties to real image editing.\nHowever, existing image-to-\nimage methods are often inefﬁcient, imprecise, and of lim-\nited versatility. They either require time-consuming ﬁne-\ntuning, deviate unnecessarily strongly from the input im-\nage, and/or lack support for multiple, simultaneous edits.\nTo address these issues, we introduce LEDITS++, an efﬁ-\ncient yet versatile and precise textual image manipulation\ntechnique. LEDITS++’s novel inversion approach requires\nno tuning nor optimization and produces high-ﬁdelity re-\nsults with a few diffusion steps. Second, our methodology\nsupports multiple simultaneous edits and is architecture-\nagnostic. Third, we use a novel implicit masking technique\nthat limits changes to relevant image regions. We propose\nthe novel TEdBench++ benchmark as part of our exhaus-\ntive evaluation. Our results demonstrate the capabilities of\nLEDITS++ and its improvements over previous methods.\n1. Introduction\nText-to-image diffusion models (DM) have garnered recog-\nnition for their ability to generate high-quality images from\ntextual descriptions. A growing body of research has re-\ncently been dedicated to utilizing these models for manip-\nulating real images.\nHowever, several barriers prevent\nmany real-world applications of diffusion-based image edit-\ning. Current methods often entail computationally expen-\nsive model tuning or other optimization, presenting prac-\ntical challenges [6, 18, 28, 30, 44]. Additionally, existing\ntechniques have the proclivity to induce profound changes\nto the original image [17, 26], often resulting in completely\ndifferent images. Lastly, all these approaches are inherently\nconstrained when editing multiple (arbitrary) concepts si-\nmultaneously.\nWe tackle these problems by introducing\nLEDITS++1, a diffusion-based image editing technique ad-\ndressing these limitations.\nLEDITS++2 offers a streamlined approach for textual\nimage editing, eliminating the need for extensive parameter\ntuning. To this end, we derive image inversion for a more ef-\nﬁcient diffusion sampling algorithm to a) drastically reduce\ncomputational resources and b) guarantee perfect image re-\nconstruction. Thus, we overcome computational obstacles\nand avoid changes in the edited image in the ﬁrst place. Fur-\nthermore, we use a novel implicit masking approach to se-\nmantically ground each edit instruction to its relevant image\nregion. This further optimizes changes to the image by re-\ntaining the overall image composition and object identity.\n*Equal contribution\n†Partially as research intern at Adobe\n1LEDITS++ stands for Limitless Edits with sde-dpm-solver++.\n2https://huggingface.co/spaces/leditsplusplus/project\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8861",
    "chosen": "Brack LEDITS Limitless Image Editing using Text-to-Image Models",
    "rejected": "In our series of letters from readers, we examine the challenges facing text- to- image editing techniques and their ability to make high- fidelity images from only text inputs. We explain why."
},
{
    "prompt": "summary: Abstract\nThis paper introduces a versatile paradigm for inte-\ngrating multi-view reﬂectance (optional) and normal maps\nacquired through photometric stereo. Our approach em-\nploys a pixel-wise joint re-parameterization of reﬂectance\nand normal, considering them as a vector of radiances\nrendered under simulated, varying illumination. This re-\nparameterization enables the seamless integration of re-\nﬂectance and normal maps as input data in neural volume\nrendering-based 3D reconstruction while preserving a sin-\ngle optimization objective. In contrast, recent multi-view\nphotometric stereo (MVPS) methods depend on multiple,\npotentially conﬂicting objectives. Despite its apparent sim-\nplicity, our proposed approach outperforms state-of-the-art\napproaches in MVPS benchmarks across F-score, Chamfer\ndistance, and mean angular error metrics. Notably, it sig-\nniﬁcantly improves the detailed 3D reconstruction of areas\nwith high curvature or low visibility.\n1. Introduction\nAutomatic 3D reconstruction is pivotal in various ﬁelds,\nsuch as archaeological and cultural heritage (virtual recon-\nstruction), medical imaging (surgical planning), virtual and\naugmented reality, games and ﬁlm production.\nMulti-view stereo (MVS) [5], which retrieves the geom-\netry of a scene seen from multiple viewpoints, is the most\nfamous 3D reconstruction solution. Coupled with neural\nvolumetric rendering (NVR) techniques [22], it effectively\nhandles complex structures and self-occlusions. However,\ndealing with non-Lambertian scenes remains a challenge\ndue to the breakdown of the underlying brightness consis-\ntency assumption. The problem is also ill-posed in certain\nconﬁgurations e.g., poorly textured scene [25] or degener-\n*Equal contributions. brument.bcb@gmail.com / rb@di.ku.dk\nFigure 1. One image from DiLiGenT-MV’s Buddha dataset [12],\nand 3D reconstruction results from several recent MVPS methods:\n[11, 26, 27] and ours. The latter provides the ﬁne details closest to\nthe ground truth (GT), while being remarkably simpler.\nate viewpoints conﬁgurations with limited baselines. More-\nover, despite recent efforts in this direction [13], recovering\nthe thinnest geometric details remains difﬁcult under ﬁxed\nillumination. In such a setting, estimating the reﬂectance of\nthe scene also remains a challenge.\nOn the other hand, photometric stereo (PS) [24], which\nrelies on a collection of images acquired under varying\nlighting, excels in the recovery of high-frequency details\nunder the form of normal maps. It is also the only pho-\ntographic technique that can estimate reﬂectance.\nAnd,\nwith the recent advent of deep learning techniques [8], PS\ngained enough maturity to handle non-Lambertian surfaces\nand complex illumination. Yet, its reconstruction of geom-\netry’s low frequencies remains suboptimal.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5230",
    "chosen": "Brument RNb-NeuS Reflectance and Normal-based Multi-View 3D Reconstruction",
    "rejected": "In our series of letters from African journalists, academics and writers, we explain the challenges facing automatic 3D reconstruction in the UK. Here is a summary of what we learned about the technique."
},
{
    "prompt": "summary: Abstract\nRadiologists highly desire fully automated versatile AI\nfor medical imaging interpretation. However, the lack of ex-\ntensively annotated large-scale multi-disease datasets has\nhindered the achievement of this goal. In this paper, we\nexplore the feasibility of leveraging language as a natu-\nrally high-quality supervision for chest CT imaging. In light\nof the limited availability of image-report pairs, we boot-\nstrap the understanding of 3D chest CT images by distill-\ning chest-related diagnostic knowledge from an extensively\npre-trained 2D X-ray expert model. Speciﬁcally, we pro-\npose a language-guided retrieval method to match each 3D\nCT image with its semantically closest 2D X-ray image, and\nperform pair-wise and semantic relation knowledge distil-\nlation. Subsequently, we use contrastive learning to align\nimages and reports within the same patient while distin-\nguishing them from the other patients. However, the chal-\nlenge arises when patients have similar semantic diagnoses,\nsuch as healthy patients, potentially confusing if treated as\nnegatives. We introduce a robust contrastive learning that\nidentiﬁes and corrects these false negatives. We train our\nmodel with over 12K pairs of chest CT images and radi-\nology reports. Extensive experiments across multiple sce-\nnarios, including zero-shot learning, report generation, and\nﬁne-tuning processes, demonstrate the model’s feasibility in\ninterpreting chest CT images.\n1. Introduction\nUnderstanding medical images is crucial for precise clinical\ndiagnosis, with doctors routinely engaging in multi-disease\n*Correspondence to Jianpeng Zhang and Jian Zheng. The work was\ndone during Weiwei Cao’s internship at DAMO Academy.\nModel\nSupervision\nFunctionality\nSingle-disease\nNodule\nEmphysema\nInflammation\nCalcification\n…\nOurs\nNodule\nEmphysema\nInflammation\nCalcification\n…\nNodule\nEmphysema\nInflammation\nCalcification\n…\n+\nMulti-disease\nFigure 1. Models tailored for speciﬁc diseases demand doctors\nto annotate each image. Creating a multi-disease model involves\nmore time and effort for comprehensive data annotation. In con-\ntrast, our model learns to diagnose various diseases from both im-\nages and reports, eliminating the need for additional annotations.\ndetection and diagnosis in imaging scans. Deep learning\nmodels require extensive datasets with fully annotated dis-\neases for comprehensive training [19, 24, 28]. In practice,\nthe prevailing approach to developing a diagnostic model\ntypically concentrates on a speciﬁc disease, as illustrated in\nFigure 1. This workﬂow results in models with limited us-\nage, failing to be applied to other diseases. To augment the\ncapability for a new disease, a signiﬁcant additional effort\nis needed for data collection, annotation, and model train-\ning, which is not scalable, as there might be hundreds of\ndiseases and tasks. Therefore, there is a strong anticipation\nfor a new model development paradigm beyond the current\nlabor-intensive annotation and task-speciﬁc training. Imag-\ning diagnostic reports, the core output of radiologists’ in-\ntellectual work, inherently offer disease-level labels. The\nchallenge lies in effectively leveraging this text data to en-\nhance medical image understanding.\nRecent advances in vision-language pretraining (VLP)\nhave yielded valuable insights [2, 11, 20, 22, 27]. Con-\ntrastive learning, a powerful technique in VLP, effectively\nbridges the image and text modalities. This method uniﬁes\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11238",
    "chosen": "Cao Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray",
    "rejected": "Chinese doctors are routinely engaging in multi-disease diagnosis using artificial intelligence (AI) to interpret medical images, according to a study by Chinese scientist Weiwei Cao. Here is the full transcript of the study."
},
{
    "prompt": "summary: Abstract\n3D visual grounding involves matching natural language\ndescriptions with their corresponding objects in 3D spaces.\nExisting methods often face challenges with accuracy in\nobject recognition and struggle in interpreting complex\nlinguistic queries,\nparticularly with descriptions that\ninvolve multiple anchors or are view-dependent.\nIn\nresponse,\nwe present the MiKASA (Multi-Key-Anchor\nScene-Aware) Transformer. Our novel end-to-end trained\nmodel\nintegrates\na\nself-attention-based\nscene-aware\nobject\nencoder\nand\nan\noriginal\nmulti-key-anchor\ntechnique, enhancing object recognition accuracy and\nthe understanding of spatial relationships. Furthermore,\nMiKASA improves the explainability of decision-making,\nfacilitating error diagnosis.\nOur model achieves the\nhighest overall accuracy in the Referit3D challenge\nfor both the Sr3D and Nr3D datasets,\nparticularly\nexcelling by a large margin in categories that require\nviewpoint-dependent descriptions.\nThe source code and\nadditional resources for this project are available on\nGitHub: https://github.com/dfki-av/MiKASA-3DVG\n1. Introduction\n3D visual grounding serves as a crucial component in the\nintersection of natural language processing and computer\nvision. This task aims to identify and localize objects within\na 3D space, using linguistic cues for spatial and semantic\ngrounding. While existing research has made significant\nstrides, challenges remain.\nKey issues include the lack\nof explainability in current models, limitations in object\nrecognition within point cloud data, and the complexity of\nhandling intricate spatial relationships.\nMost existing 3D visual grounding models [2, 15, 16,\n33, 40] consist of three parts:\n(1) object encoder, (2)\ntext encoder, and (3) fusion model.\nThe object encoder\nprocesses the provided point cloud and generates features\nin the embedding space. However, because the points in a\n(a) Target category: “chair”\n(b) “The chair in the front of the\nblue-lit monitor.”\nFigure 1. Our methodology utilizes a dual-prediction framework\nfor 3D visual grounding. First, we assign a target category score\nbased on object categorization, as detailed in Fig. 1a. Next, a\nspatial score is integrated according to the object’s alignment with\nthe textual description, as shown in Fig. 1b.\npoint cloud are unordered and inconsistent in sparsity [26],\nit is not straightforward to apply the methodology typically\nused for 2D images. An additional challenge is that 3D\npoint cloud datasets are not as extensive as those for 2D\nimages [10, 25], which makes it difficult for the models to\ncorrectly recognize object categories. While enlarging the\ndataset could conceivably improve performance, we refrain\nfrom doing so to ensure a fair comparison with existing\nstate-of-the-art methods. Existing works [6, 18, 27] mainly\nuse different techniques such as noise addition, dropping\nout colors, and transformations to expand the sample space.\nThough these techniques may increase the stability of the\nproduced object embeddings, the improvement is limited.\nInspired by previous works [20, 21, 39] which aims to\nsolve object recognition problem, we leverage the fact\nthat data availability on objects within a specific space\ncan provide valuable insights into the characteristics and\nrelationships of their surrounding entities.\nFor instance,\nwhen we come across a cuboid-shaped object in a\nkitchen, we may naturally assume that it is a dishwasher.\nConversely, if we spot the same shape in a bathroom, it is\nmore plausible that it is a washing machine. Contextual\ninformation is crucial in determining the identity of objects\nand gives us a nuanced understanding of our surroundings.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14131",
    "chosen": "Chang MiKASA Multi-Key-Anchor  Scene-Aware Transformer for 3D Visual Grounding",
    "rejected": "Welcome to a new 3D visual grounding project, which aims to identify and localise objects within their spaces. Here are the key points for the project and why it is being developed."
},
{
    "prompt": "summary: Abstract\nImage denoising is a fundamental task in computer vi-\nsion. While prevailing deep learning-based supervised and\nself-supervised methods have excelled in eliminating in-\ndistribution noise, their susceptibility to out-of-distribution\n(OOD) noise remains a significant challenge.\nThe re-\ncent emergence of contrastive language-image pre-training\n(CLIP) model has showcased exceptional capabilities in\nopen-world image recognition and segmentation. Yet, the\npotential for leveraging CLIP to enhance the robustness of\nlow-level tasks remains largely unexplored. This paper un-\ncovers that certain dense features extracted from the frozen\nResNet image encoder of CLIP exhibit distortion-invariant\nand content-related properties, which are highly desirable\nfor generalizable denoising. Leveraging these properties,\nwe devise an asymmetrical encoder-decoder denoising net-\nwork, which incorporates dense features including the noisy\nimage and its multi-scale features from the frozen ResNet\nencoder of CLIP into a learnable image decoder to achieve\ngeneralizable denoising. The progressive feature augmen-\ntation strategy is further proposed to mitigate feature over-\nfitting and improve the robustness of the learnable decoder.\nExtensive experiments and comparisons conducted across\ndiverse OOD noises, including synthetic noise, real-world\nsRGB noise, and low-dose CT image noise, demonstrate the\nsuperior generalization ability of our method.\n1. Introduction\nImage denoising is a significant task in computer vision\nand image processing. Current supervised denoising meth-\nods leveraging powerful deep neural networks and large-\nscale datasets have achieved exceptional performance in\nboth synthetic and real-world noise removal [33, 60]. How-\never, these supervised denoisers tend to overfit the noise\npresent in the training datasets, resulting in poor general-\nization to out-of-distribution (OOD) noise [6]. On the other\nhand, unsupervised and self-supervised denoising methods\n*Corresponding author.\n[12, 18, 26, 28, 44, 54] directly focus on the target domain\nin which the target noisy images reside and hence bypass\nOOD generalization. Nevertheless, these methods are in-\nherently vulnerable to unseen noise [7] and the collection\nof target noisy datasets is not always available. Therefore,\nit is critical to enhance the generalization of deep denoisers.\nOOD generalization has been popular research in high-\nlevel vision tasks like image recognition and segmentation\n[51, 70]. In contrast, attention to OOD generalization within\nimage denoising is limited. Existing research in this area\nprimarily consists of two aspects: generalization across\ndegradation levels and generalization across degradation\ntypes. Regarding the former, some works trained blind de-\nnoisers [18, 60, 62] or bias-free networks [41, 63] to handle\nnoise with varying levels. However, these methods are con-\nfined to specific noise and cannot generalize to unseen noise\ntypes. For the latter, several works aimed to fortify mod-\nels against general OOD noise. Particularly, MaskDenois-\ning [6] incorporated dropout units into the model training\nto enforce the denoiser to learn the reconstruction of image\ncontents. DIL [32] built upon causality and meta-learning\nand encouraged the model to learn distortion-invariant rep-\nresentations. HAT [58] designed an adversarial attack for\ndeep denoisers and then conducted adversarial training.\nRecently, through solving the image-text alignment\nproblem based on hyper-scale datasets, the contrastive\nlanguage-image pre-training (CLIP) model [48] has demon-\nstrated remarkable generalization capacity in downstream\nopen-world image recognition tasks. A series of extensions\non CLIP through frozen models [65, 69], model fine-tuning\n[49], visual prompts [71], distillations [19, 31], and so on\n[34] have been proposed to transfer the generalization abil-\nity of CLIP from classification to dense prediction tasks,\nincluding open-vocabulary segmentation [34] and zero-shot\ndepth estimation [65]. However, the feasibility of CLIP for\nrobust restoration in low-level tasks remains unexplored.\nWe therefore ask, is CLIP robust to image noise and can\nwe transfer it for generalizable image denoising?\nIn this paper, we find that the dense feature maps from\nthe frozen ResNet image encoder of CLIP within specific\nscales exhibit remarkable resilience to noise, a property that\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25974",
    "chosen": "Cheng Transfer CLIP for Generalizable Image Denoising",
    "rejected": "In our series of letters from African journalists, academics and readers, we explain the challenges facing the denoising techniques of artificial intelligence (AI) technology. Here are some of the key facts."
},
{
    "prompt": "summary: Abstract\nThis work introduces ArtAdapter, a transformative text-\nto-image (T2I) style transfer framework that transcends tra-\nditional limitations of color, brushstrokes, and object shape,\ncapturing high-level style elements such as composition and\ndistinctive artistic expression. The integration of a multi-\nlevel style encoder with our proposed explicit adaptation\nmechanism enables ArtAdapter to achieve unprecedented\nfidelity in style transfer, ensuring close alignment with tex-\ntual descriptions. Additionally, the incorporation of an Aux-\niliary Content Adapter (ACA) effectively separates content\nfrom style, alleviating the borrowing of content from style\nreferences. Moreover, our novel fast finetuning approach\ncould further enhance zero-shot style representation while\nmitigating the risk of overfitting. Comprehensive evalua-\ntions confirm that ArtAdapter surpasses current state-of-\nthe-art methods.\n1. Introduction\nBridging the realms of artificial intelligence and artistic cre-\nativity, text-to-image (T2I) style transfer [13, 24, 25] stands\nout as a captivating domain that masterfully transforms\ntextual descriptions into visually rich and stylistic repre-\nsentations. The core challenge lies in not just generating\ntext-resonant images but infusing them with artistic depth\nand nuance, spanning from delicate brushstrokes to bold\ncompositional elements—thereby capturing the essence of\nartistic vision. Conventional arbitrary style transfer (AST)\nmethods [4, 7–9, 15, 21, 28, 54] typically struggle beyond\nlow-level features such as medium and colors, failing to\ngrasp the more sophisticated realms of artistic expression.\nDiffusion approaches [25, 55], including Textual Inversion\n[14], DreamBooth [39], and Low-Rank Adaptation (LoRA)\n[2, 20], have shown potential in style representation. Yet,\nthese methods are hindered by laborious finetuning pro-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8619",
    "chosen": "Chen ArtAdapter Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit",
    "rejected": "This is a full transcript of the Comprehensive Review of Art Transfer (CVPR) . This paper aims to explain the challenges facing artists in the digital world, including the use of artificial intelligence."
},
{
    "prompt": "summary: Abstract\nAn effective pre-training framework with universal 3D\nrepresentations is extremely desired in perceiving large-\nscale dynamic scenes.\nHowever, establishing such an\nideal framework that is both task-generic and label-efficient\nposes a challenge in unifying the representation of the\nsame primitive across diverse scenes.\nThe current con-\ntrastive 3D pre-training methods typically follow a frame-\nlevel consistency, which focuses on the 2D-3D relationships\nin each detached image. Such inconsiderate consistency\ngreatly hampers the promising path of reaching an uni-\nversal pre-training framework: (1) The cross-scene seman-\ntic self-conflict, i.e., the intense collision between primitive\nsegments of the same semantics from different scenes; (2)\nLacking a globally unified bond that pushes the cross-scene\nsemantic consistency into 3D representation learning. To\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n19925",
    "chosen": "Chen Building a Strong Pre-Training Baseline for Universal 3D Large-Scale Perception",
    "rejected": "This is a full transcript of the CVPR paper on 3D pre-training frameworks. The BBC looks at the challenges facing each other in the process of perceiving large- scale dynamic scenes ."
},
{
    "prompt": "summary: Abstract\nDiffusion models have shown an impressive ability to\nmodel complex data distributions, with several key advan-\ntages over GANs, such as stable training, better coverage\nof the training distribution’s modes, and the ability to solve\ninverse problems without extra training. However, most dif-\nfusion models learn the distribution of fixed-resolution im-\nages. We propose to learn the distribution of continuous\nimages by training diffusion models on image neural fields,\nwhich can be rendered at any resolution, and show its ad-\nvantages over fixed-resolution models. To achieve this, a\nkey challenge is to obtain a latent space that represents pho-\ntorealistic image neural fields. We propose a simple and\neffective method, inspired by several recent techniques but\nwith key changes to make the image neural fields photo-\nrealistic. Our method can be used to convert existing la-\ntent diffusion autoencoders into image neural field autoen-\ncoders.\nWe show that image neural field diffusion mod-\nels can be trained using mixed-resolution image datasets,\noutperform fixed-resolution diffusion models followed by\nsuper-resolution models, and can solve inverse problems\nwith conditions applied at different scales efficiently.\n1. Introduction\nDiffusion models [16, 34, 50] have recently become attrac-\ntive alternatives to GANs. These likelihood-based models\n†Equal advising.\nexhibit fewer artifacts, stable training, can model complex\ndata distributions, do not suffer from mode collapse, and\ncan solve inverse problems using the score function with-\nout extra training. Since diffusion typically requires many\niterations at a fixed dimension, directly modeling the dif-\nfusion process in the pixel space [17, 40, 43] can be in-\nefficient for high-resolution image synthesis. Latent diffu-\nsion models (LDMs) [41, 57] were proposed as a more ef-\nficient alternative. The key idea is to learn an autoencoder\nto map images to a latent representation from which the im-\nage can be decoded back, and train a diffusion model on\nthe lower-dimensional latent representation. Despite their\nsuccess, LDMs’ latent space still represents images at fixed\nresolution (for example, 256 in LDM [41] and 512 in Stable\nDiffusion). To generate higher-resolution images (e.g., 2K),\nLDMs usually first generate a low-resolution image and up-\nsample it using a separate super-resolution model.\nIn this work, we propose Image Neural Field Diffusion\nmodels (INFD). Our method is based on the latent diffusion\nframework, where we first learn a latent representation that\nrepresents an image neural field (which can be rendered at\nany resolution), then learn a diffusion model on this latent\nrepresentation. A key challenge of our approach is to learn a\nlatent space of photorealistic image neural fields where the\ndiffusion model is applied. We propose a simple and effec-\ntive method that can convert an existing autoencoder of la-\ntent diffusion models to a neural field autoencoder. We find\nthat directly implementing an autoencoder with LIIF [8]\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8007",
    "chosen": "Chen Image Neural Field Diffusion Models",
    "rejected": "Scientists at the University of Cambridge have published a series of ways to train dif- fusion models. Here, we explain why they are being trained to make images more realistic and efficiently."
},
{
    "prompt": "summary: Abstract\nMultimodal Large Language Models (MLLMs) have\nendowed LLMs with the ability to perceive and under-\nstand multi-modal signals.\nHowever, most of the exist-\ning MLLMs mainly adopt vision encoders pretrained on\ncoarsely aligned image-text pairs, leading to insufficient ex-\ntraction and reasoning of visual knowledge. To address this\nissue, we devise a dual-Level vIsual knOwledge eNhanced\nMultimodal Large Language Model (LION), which empow-\ners the MLLM by injecting visual knowledge in two lev-\nels. 1) Progressive incorporation of fine-grained spatial-\naware visual knowledge. We design a vision aggregator\ncooperated with region-level vision-language (VL) tasks to\nincorporate fine-grained spatial-aware visual knowledge\ninto the MLLM. To alleviate the conflict between image-\nlevel and region-level VL tasks during incorporation, we\ndevise a dedicated stage-wise instruction-tuning strategy\nwith mixture-of-adapters. This progressive incorporation\nscheme contributes to the mutual promotion between these\ntwo kinds of VL tasks. 2) Soft prompting of high-level se-\nmantic visual evidence. We facilitate the MLLM with high-\nlevel semantic visual evidence by leveraging diverse image\ntags. To mitigate the potential influence caused by imper-\nfect predicted tags, we propose a soft prompting method by\nembedding a learnable token into the tailored text instruc-\ntion. Comprehensive experiments on several multi-modal\nbenchmarks demonstrate the superiority of our model (e.g.,\nimprovement of 5% accuracy on VSR and 3% CIDEr on\nTextCaps over InstructBLIP, 5% accuracy on RefCOCOg\nover Kosmos-2).\n1. Introduction\nRecently, Large Language Models (LLMs) have demon-\nstrated remarkable zero-shot abilities on various linguistic\ntasks.\nAssisted by LLMs, several multimodal large lan-\nguage models (MLLMs), such as MiniGPT-4 [54], Otter\n†Corresponding author\nInstruction:\nWhat is in the photo?\nLLM\nVision\nEncoder\nLION\nchicken, potatoes\nand cauliflower on a\nwhite plate with a\nknife and fork in\nthe middle of the\ntable next to a red\nwine bottle that\nsits on the table\nnext to it.\nInstruction:\nWhat is in the photo?\nLLM\nGrilled chicken breast [                      ]\nSpatial-aware Visual Knowledge:\nSemantic Visual Evidence:\n cauliflower, chicken, table,\nplate, food, meat,  potato, white\nChicken, potatoes,\nand cauliflower on a\nwhite plate with a\nwooden table in\nthe foreground.\nThe plate is sitting\non a wooden table.\nResponse:\nResponse:\nExsiting MLLMs\nBridge\nModule\nBridge\nModule\nVision\nEncoder\nA piece of steamed cauliflower [                      ] \nRoasted baby yellow potato [                      ]  \nMixture\nof\nAdapters\nFigure 1. Comparison between existing MLLMs and LION\n.\nThe existing MLLM generates a vague and inaccurate response,\nwhile LION provides a more precise and contextually accurate de-\nscription by progressively incorporating spatial-aware knowledge\nand softly prompting semantic visual evidence.\n[19], and InstructBLIP [7], achieve significant improve-\nments in reasoning abilities to deal with various vision-\nlanguage (VL) tasks.\nIn most of the existing MLLMs, the visual informa-\ntion is mainly extracted from a vision encoder pretrained\nwith image-level supervision (e.g., CLIP [32]), and then\nare adapted to a LLM by using a tiny bridge module. This\nmakes these MLLMs inherently possess limited image un-\nderstanding capabilities [21]. As shown in Fig. 1, the in-\nsufficient visual information misleads MLLMs to provide\nerroneous and hallucinated responses. An intuitive solution\nto this problem is to replace or tune the vision encoder [41].\nHowever, it requires pretraining on massive data or suffers\nfrom the catastrophic forgetting issue [35, 36, 50], which\ndiminishes the practical efficacy of this strategy.\nThese\npredicaments highlight that the insufficient extraction of vi-\nsual knowledge has become a central obstacle impeding the\ndevelopment of MLLMs.\nTo overcome this dilemma, as depicted in Fig. 1, we de-\nvise a dual-Level vIsual knOwledge eNhanced Multimodal\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n26540",
    "chosen": "Chen LION Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge",
    "rejected": "In our series of letters from African journalists, academics and researchers explain how a multi-modal Large Language Model (MLLs) has been developed. Here are some of the key ways to tackle the problem."
},
{
    "prompt": "summary: Abstract\nFor image super-resolution (SR), bridging the gap be-\ntween the performance on synthetic datasets and real-world\ndegradation scenarios remains a challenge. This work in-\ntroduces a novel ”Low-Res Leads the Way” (LWay) train-\ning framework, merging Supervised Pre-training with Self-\nsupervised Learning to enhance the adaptability of SR mod-\nels to real-world images.\nOur approach utilizes a low-\nresolution (LR) reconstruction network to extract degrada-\ntion embeddings from LR images, merging them with super-\nresolved outputs for LR reconstruction. Leveraging unseen\nLR images for self-supervised learning guides the model\nto adapt its modeling space to the target domain, facili-\ntating fine-tuning of SR models without requiring paired\nhigh-resolution (HR) images. The integration of Discrete\nWavelet Transform (DWT) further refines the focus on high-\nfrequency details.\nExtensive evaluations show that our\nmethod significantly improves the generalization and de-\ntail restoration capabilities of SR models on unseen real-\nworld datasets, outperforming existing methods. Our train-\ning regime is universally compatible, requiring no network\narchitecture modifications, making it a practical solution\nfor real-world SR applications.\n1. Introduction\nImage super-resolution (SR) aims to restore high-resolution\n(HR) images from their low-resolution (LR) or degraded\ncounterparts. The inception of the deep-learning-based SR\nmodel can be traced back to SRCNN [14]. Recently, ad-\nvancements in deep learning models have substantially en-\nhanced SR performance [1, 6, 8–10, 12, 25–27, 39, 51, 52,\n54, 56], particularly in addressing specific degradation types\nlike bicubic downsampling. Nevertheless, the efficacy of\nSR models is generally restricted by the degradation strate-\ngies employed during the training phase, posing great chal-\nlenges in complex real-world applications.\n*Lei Zhu (leizhu@ust.hk) is the corresponding author.\nSL space on \nsynthetic data\nSSL space on \nreal test data\nUnseen \nReal-world Image\nHigh quality\nLow fidelity\nLow quality\nHigh fidelity\nGround Truth\nHigh quality\nHigh fidelity\nOurs\npull\nSL space on \nsynthetic data\nSSL space on \nreal test data\nZoom In\nZoom In\nZoom In\nZoom In\nZoom In\nFigure 1.\nOur proposed training method combine the benefits\nof supervised learning (SL) on synthetic data and self-supervised\nlearning (SSL) on the unseen test images, achieve high quality and\nhigh fidelity SR results.\nIn the realm of real-world SR, as shown in Figure 2,\ntraining approaches can primarily be categorized into three\nmain paradigms.\n(a) Unsupervised Learning with Un-\npaired Data: Methods within this paradigm [2, 3, 15, 38,\n40, 45, 46, 55] commonly utilize Generative Adversarial\nNetworks (GAN) architecture to learn target distributions\nwithout paired data. Using one or multiple discriminators,\nthey distinguish between generated images and actual sam-\nples, guiding the generator to model accurately. However,\nas this approach heavily relies on external data, it encoun-\nters significant challenges when facing scarce target domain\ndata, particularly in real-world scenarios. The GAN frame-\nwork for unsupervised learning also has some drawbacks.\nFirstly, it inherently struggles with stability during training,\nleading to noticeable artifacts in SR outputs. Secondly, it\nis difficult for a single 0/1 plane modelled by a discrimina-\ntor to accurately separate the target domain [31]. This can\nresult in imprecise distribution learning. (b) Supervised\nLearning with Paired Synthetic Data: BSRGAN [47] and\nReal-ESRGAN [42] have largely enhanced the SR model’s\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25857",
    "chosen": "Chen Low-Res Leads the Way Improving Generalization for Super-Resolution by Self-Supervised",
    "rejected": "In our series of letters from African journalists, academics and students, we explain the challenges facing the train- ing regime of high-resolution images on unseen real- world synthetic data ."
},
{
    "prompt": "summary: Abstract\nWeakly-supervised Video Anomaly Detection (wVAD)\naims to detect frame-level anomalies using only video-\nlevel labels in training. Due to the limitation of coarse-\ngrained labels, Multi-Instance Learning (MIL) is prevail-\ning in wVAD. However, MIL suffers from insufficiency of\nbinary supervision to model diverse abnormal patterns. Be-\nsides, the coupling between abnormality and its context hin-\nders the learning of clear abnormal event boundary.\nIn\nthis paper, we propose prompt-enhanced MIL to detect var-\nious abnormal events while ensuring clear event bound-\naries. Concretely, we design the abnormal-aware prompts\nby using abnormal class annotations together with learn-\nable prompt, which can incorporate semantic priors into\nvideo features dynamically.\nThe detector can utilize the\nsemantic-rich features to capture diverse abnormal pat-\nterns. In addition, normal context prompt is introduced to\namplify the distinction between abnormality and its context,\nfacilitating the generation of clear boundary. With the mu-\ntual enhancement of abnormal-aware and normal context\nprompt, the model can construct discriminative represen-\ntations to detect divergent anomalies without ambiguous\nevent boundaries. Extensive experiments demonstrate our\nmethod achieves SOTA performance on three public bench-\nmarks.\nThe code is available at https://github.\ncom/Junxi-Chen/PE-MIL.\n1. Introduction\nTo identify anomaly at frame level in video, Video Anomaly\nDetection (VAD) has become vital in critical areas, e.g.,\nsurveillance systems [19], medical imaging [33] and au-\ntonomous driving [1].\nFor great generalization ability\nacross diverse scenes, researchers [6, 8, 29, 31, 32, 43, 45]\n*Corresponding authors\nFigure 1. (a) Illustration of prompt-enhanced MIL. In multi-modal\nfeature space, text prompts integrate abnormal-aware semantic pri-\nors into visual features. NCP incorporates normal semantic into\nambiguous context feature. In such manner, our method learns\na more discriminative representation to deliver a precise anomaly\ndetection. (b) Anomaly detection examples of our method.\nturn to weakly-supervised VAD (wVAD) which only lever-\nages video-level labels. Primarily, wVAD faces two key\nchallenges: 1) detecting complex anomalous patterns in var-\nied scenarios where temporal relationship and visual ap-\npearance of anomalies exhibit substantial discrepancies; 2)\ngenerating clear abnormal event boundaries in the absence\nof fine-grained boundary annotations.\nTo tackle wVAD, prior works [52, 55] generate noisy\nframe-level abnormal labels and reduce noise subsequently,\nbut such manner limits the generalization ability to un-\nseen scenarios. Recently, Multi-Instance Learning (MIL) is\nleveraged by most methods [5, 6, 8, 23, 29, 45] to tackle\nwVAD due to its ability to model patterns under coarse-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18319",
    "chosen": "Chen Prompt-Enhanced Multiple Instance Learning for Weakly Supervised Video Anomaly Detection",
    "rejected": "Scientists have developed a way to detect video anomalies using only video- level labels in training. Here, they explain why the method is being used in surveillance systems, medical imaging systems and au- tonomous driving."
},
{
    "prompt": "summary: Abstract\nWe propose SceneTex, a novel method for effectively gen-\nerating high-quality and style-consistent textures for indoor\nscenes using depth-to-image diffusion priors. Unlike pre-\nvious methods that either iteratively warp 2D views onto\na mesh surface or distillate diffusion latent features with-\nout accurate geometric and style cues, SceneTex formulates\nthe texture synthesis task as an optimization problem in the\nRGB space where style and geometry consistency are prop-\nerly reflected. At its core, SceneTex proposes a multires-\nolution texture field to implicitly encode the mesh appear-\nance. We optimize the target texture via a score-distillation-\nbased objective function in respective RGB renderings. To\nfurther secure the style consistency across views, we intro-\nduce a cross-attention decoder to predict the RGB values\nby cross-attending to the pre-sampled reference locations\nin each instance. SceneTex enables various and accurate\ntexture synthesis for 3D-FRONT scenes, demonstrating sig-\nnificant improvements in visual quality and prompt fidelity\nover the prior texture generation methods.\n1. Introduction\nSynthesizing high-quality 3D contents is an essential yet\nhighly demanding task for numerous applications, includ-\ning gaming, film making, robotic simulation, autonomous\ndriving, and upcoming VR/AR scenarios. With an increas-\ning number of 3D content datasets, the computer vision and\ngraphics community has witnessed a soaring research inter-\nest in the field of 3D geometry generation [2, 12, 36, 38, 40,\n60, 68, 73]. Despite achieving a remarkable success in 3D\ngeometry modeling, generating the object appearance, i.e.\ntextures, is still bottlenecked by laborious human efforts.\nIt typically requires a substantially long time for designing\nand adjustment, and immense 3D modelling expertise with\ntools such as Blender. As such, automatic designing and\naugmenting the textures has not yet been fully industrial-\nized due to a huge demand for human expertise and finan-\ncial expenses.\nLeveraging the recent advances of 2D diffusion mod-\nels, tremendous progress has been made for text-to-3D\ngeneration, especially for synthesizing textures of given\nshapes [8, 39, 50]. Seminal work such as Text2Tex [8] and\nLatent-Paint [39] have achieved great success in generat-\ning high-quality appearances for objects, facilitating high-\nfidelity texture synthesis from input prompts. Despite the\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n21081",
    "chosen": "Chen SceneTex High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors",
    "rejected": "In our series of letters from academics, scientists and designers, we propose a novel method for effectively gen- erating high-quality and style-consistent textures for indoor scenes using depth- to- image diffusion priors."
},
{
    "prompt": "summary: Abstract\nWe present a method to generate full-body selfies from\nphotographs originally taken at arms length. Because self-\ncaptured photos are typically taken close up, they have lim-\nited field of view and exaggerated perspective that distorts\nfacial shapes. We instead seek to generate the photo some\none else would take of you from a few feet away. Our ap-\nproach takes as input four selfies of your face and body, a\nbackground image, and generates a full-body selfie in a de-\nsired target pose. We introduce a novel diffusion-based ap-\nproach to combine all of this information into high-quality,\nwell-composed photos of you with the desired pose and\nbackground.\n1. Introduction\nThe prevalence of selfies has skyrocketed in recent years,\nwith an estimated 93 million taken each day. Despite their\npopularity, they suffer from multiple shortcomings: (1) they\ncapture only the upper portion of the subject, (2) the close-\nup camera viewpoint distorts faces and requires awkward\nposes (e.g., with arm reaching out), and (3) it is difficult to\ncompose a shot that optimally captures both the subject and\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6701",
    "chosen": "Chen Total Selfie Generating Full-Body Selfies",
    "rejected": "This is a full-body selfie paper published by the University of Cambridge, which aims to generate high-quality, well-composed photos of you with the desired pose and background. Here are some of the key points."
},
{
    "prompt": "summary: Abstract\nTumor synthesis enables the creation of artificial tumors\nin medical images, facilitating the training of AI models for\ntumor detection and segmentation. However, success in tu-\nmor synthesis hinges on creating visually realistic tumors\nthat are generalizable across multiple organs and, further-\nmore, the resulting AI models being capable of detecting\nreal tumors in images sourced from different domains (e.g.,\nhospitals). This paper made a progressive stride toward\ngeneralizable tumor synthesis by leveraging a critical ob-\nservation: early-stage tumors (< 2cm) tend to have simi-\nlar imaging characteristics in computed tomography (CT),\nwhether they originate in the liver, pancreas, or kidneys.\nWe have ascertained that generative AI models, e.g., Dif-\nfusion Models, can create realistic tumors generalized to a\nrange of organs even when trained on a limited number of\ntumor examples from only one organ. Moreover, we have\nshown that AI models trained on these synthetic tumors can\nbe generalized to detect and segment real tumors from CT\nvolumes, encompassing a broad spectrum of patient demo-\ngraphics, imaging protocols, and healthcare facilities.\n1. Introduction\nTumor synthesis enables the creation of artificial tumor ex-\namples in medical images [11, 42, 88], it is particularly\nvaluable when there is a dearth or complete absence of per-\nvoxel annotated real tumors (e.g., early-stage tumors) for\neffective AI training. Typically, to train AI models for tu-\nmor detection in multiple (N) organs, annotated real tu-\nmor examples from each of these organs are necessary, and\nideally, in substantial numbers [14, 43, 54, 55, 97]. Fur-\nthermore, AI models often fail to generalize across images\nfrom different hospitals, which may vary due to variations\n*Correspondence to Zongwei Zhou (ZZHOU82@JH.EDU)\nin imaging protocols, patient demographics, and scanner\nmanufacturers [61, 95, 96]. The challenge amplifies with\nthe need for extensive manual annotations, a task that could\ndemand up to 25 human years for annotating just one tu-\nmor type [1, 9, 83]. The task of collecting and annotating\na comprehensive dataset encompassing tumors from multi-\nple organs (N) and images from numerous hospitals (M) is\ndaunting, considering both annotation cost and complexity\n(N × M). We hypothesize that tumor synthesis could solve\nthis challenge by creating various tumor types across non-\ntumor images from multiple hospitals, even when only one\ntumor type is available, thereby simplifying the complexity\nfrom N × M to 1 × M.\nSuccess in tumor synthesis hinges on creating visually\nrealistic tumors that are generalizable across multiple or-\ngans and, furthermore, the resulting AI models being gen-\neralizable in detecting real tumors in images sourced from\ndifferent hospitals. Previous studies have introduced gen-\nerative models to create synthetic medical data (not lim-\nited to tumors) such as polyp detection from colonoscopy\nvideos [69], COVID-19 detection from Chest X-ray [26,\n56, 87], and diabetic lesion detection from retinal im-\nages [78]—refer to §5 for a comprehensive review. How-\never, these studies have primarily focused on enhancing the\ndetection and segmentation of specific tumors without fully\nexploring the wider generalizability of these models across\ndifferent organs and patient demographics.\nThis paper made a progressive stride toward generaliz-\nable tumor synthesis by leveraging a critical observation:\nearly-stage tumors (< 2cm) tend to have similar imaging\ncharacteristics in computed tomography (CT)1. Early-stage\ntumors typically present small, round, or oval shapes with\nminimal deformation and exhibit relatively simple and uni-\n1Note that, owing to the public dataset constraints, we have only ver-\nified the similarity across early hepatocellular carcinoma and intrahep-\natic cholangiocarcinoma from the liver, pancreatic ductal adenocarcinoma\nfrom the pancreas, and renal cell carcinoma from kidneys.\n1\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n11147",
    "chosen": "Chen Towards Generalizable Tumor Synthesis",
    "rejected": "In our series of letters from African journalists, Zeinab Zhao explains why artificial intelligence (AI) is a key factor in the creation of artificial tumors in medical images, and how they can be treated."
},
{
    "prompt": "summary: Abstract\nPretrained diffusion models and their outputs are widely\naccessible due to their exceptional capacity for synthe-\nsizing high-quality images and their open-source nature.\nThe users, however, may face litigation risks owing to\nthe models’ tendency to memorize and regurgitate train-\ning data during inference. To address this, we introduce\nAnti-Memorization Guidance (AMG), a novel framework\nemploying three targeted guidance strategies for the main\ncauses of memorization: image and caption duplication,\nand highly specific user prompts. Consequently, AMG en-\nsures memorization-free outputs while maintaining high im-\nage quality and text alignment, leveraging the synergy of\nits guidance methods, each indispensable in its own right.\nAMG also features an innovative automatic detection sys-\ntem for potential memorization during each step of in-\nference process, allows selective application of guidance\nstrategies, minimally interfering with the original sampling\nprocess to preserve output utility. We applied AMG to pre-\ntrained Denoising Diffusion Probabilistic Models (DDPM)\nand Stable Diffusion across various generation tasks. The\nresults demonstrate that AMG is the first approach to suc-\ncessfully eradicates all instances of memorization with no\nor marginal impacts on image quality and text-alignment,\nas evidenced by FID and CLIP scores.\n1. Introduction\nDiffusion models [12, 23, 34] have attracted substantial in-\nterest, given their superiority in terms of diversity, fidelity,\nscalability [28] and controllability [24] over previous gener-\native models including VAEs [17], normalizing flows [29],\nand GANs [10, 14–16]. With guidance techniques [7, 11],\ndiffusion models can be further improved by the strategical\ndiversity-fidelity trade-off. State-of-the-art diffusion mod-\nels trained on vast web-scale datasets are widespreadly used\nand have seen deployment at a commercial scale [1, 30, 31].\nSuch widespread adoption, however, has significantly\nheightened the litigation risks for companies using these\nmodels, particularly due to allegations that the models\nmemorize and reproduce training data during inference\nwithout informing the data owners and the users of diffusion\nFigure 1. Stable Diffusion’s capacity to memorize training data,\nmanifested as pixel-level memorization (left) and object-level\nmemorization (right).\nOur approach successfully guides pre-\ntrained diffusion models to produce memorization-free outputs.\nmodels. This potentially violates copyright laws and intro-\nduces ethical dilemmas, further complicated by the fact that\nthe extensive size of training sets impedes detailed human\nreview, leaving the intellectual property rights of the data\nsources largely undetermined. An ongoing example is that a\nlegal action contends that Stable Diffusion is a 21st-century\ncollage tool that remixes the copyrighted works of millions\nof artists whose work was used as training data [32].\nPrior studies [4, 35, 36] have observed memorization in\npretrained diffusion models, particularly during uncondi-\ntional CIFAR-10 [18] and text-conditional LAION dataset\n[33] generations. While previous research proposed strate-\ngies to reduce memorization, these often lead to only mod-\nest improvements and fail to fully eliminate the issue. The\neffectiveness often come with reduced output quality and\ntext-alignment [36], the need for retraining models [4], and\nextensive manual intervention [19]. Moreover, these strate-\ngies lack an automated way to differentiate potential mem-\norization cases for targeted mitigation. For example, [19]\nrelies on a predefined list of text prompts prone to causing\nmemorization, and [36] applies randomization mechanisms\nuniformly without distinguishing between scenarios.\nIn this paper, we undertake the following systematic ef-\nforts to address the issue of memorization. Firstly, we have\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n8425",
    "chosen": "Chen Towards Memorization-Free Diffusion Models",
    "rejected": "A new approach to eradicate all instances of memorization has been developed by the University of Cambridge. Here is a summary of the challenges facing diffusion models and their outputs, which are being rolled out across the UK."
},
{
    "prompt": "summary: Abstract\nSignificant progress in image deblurring has been\nachieved by deep learning methods, especially the remark-\nable performance of supervised models on paired synthetic\ndata. However, real-world quality degradation is more com-\nplex than synthetic datasets, and acquiring paired data in\nreal-world scenarios poses significant challenges. To ad-\ndress these challenges, we propose a novel unsupervised\nimage deblurring framework based on self-enhancement.\nThe framework progressively generates improved pseudo-\nsharp and blurry image pairs without the need for real paired\ndatasets, and the generated image pairs with higher qualities\ncan be used to enhance the performance of the reconstruc-\ntor. To ensure the generated blurry images are closer to\nthe real blurry images, we propose a novel re-degradation\nprincipal component consistency loss, which enforces the\nprincipal components of the generated low-quality images to\nbe similar to those of re-degraded images from the original\nsharp ones. Furthermore, we introduce the self-enhancement\nstrategy that significantly improves deblurring performance\nwithout increasing the computational complexity of network\nduring inference. Through extensive experiments on multiple\nreal-world blurry datasets, we demonstrate the superiority\nof our approach over other state-of-the-art unsupervised\nmethods.\n1. Introduction\nImage deblurring is a classical problem in the field of com-\nputer vision, which aims to recover a clear image from its\nblurred version. The image deblurring tasks can be divided\ninto blind and non-blind deblurring, where the blind image\ndeblurring with unknown degradation is more challenging in\ngeneral. Conventional model-based methods for blind image\ndeblurring typically involve two main steps: estimating the\nblur kernel, and then reconstructing the sharp image from\nthe blurred input [10, 26, 40, 42]. These methods’ perfor-\nmance is largely constrained by the accuracy of blur kernal\n*Corresponding author\nCycleGAN\nUAUD\nUSR-DA\nUIDGAN\nUSDF\nFCLGAN\nSEMGUD \n(Ours)\nGoPro\nHIDE\nRealBlur-J\nRealBlur-R\nFigure 1. Performance comparison of our proposed SEMGUD with\nother unsupervised methods [12, 23, 36, 39, 48, 50] on different\ndatasets.\nestimation. For instance, in [27], the dark channel prior is\nused to estimate the blur kernel and reconstruct the sharp\nimage. However, the blurry characteristics in real-world sce-\nnarios are quite complex, making it challenging to accurately\nestimate the optimal blur kernel. In addition, these meth-\nods often require complex iterative optimisation processes,\nwhich may lead to long inference time.\nIn recent years, with the rapid development of deep learn-\ning technology, convolutional neural networks (CNNs) have\nbeen widely used in deblurring tasks, achieving significant\nsuccess. The supervised methods [4–6, 15, 16, 19, 29, 34, 44–\n46] focus on training deep neural network models using a\nlarge number of paired sharp and blurry images. This en-\nables the network to learn the mapping from blurry images\nto sharp images without the need for blur kernel estima-\ntion, achieving end-to-end reconstruction of the blurry and\nsharp images. For example, DeepDeblur [24] proposes a\nmulti-scale CNN to implement a coarse-to-fine processing\npipeline and directly restores sharp images. However, in\nthe real world cases, for the supervised learning methods,\ncollecting paired datasets from the real world is challenging,\nand manually synthesized datasets are difficult to simulate\nthe complex real image degradation processes.\nCompared to supervised deep learning methods, unsuper-\nvised deep learning methods [3, 25, 49, 50] for real world\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n25691",
    "chosen": "Chen Unsupervised Blind Image Deblurring Based on Self-Enhancement",
    "rejected": "In our series of letters from academics, readers are being asked to explain the challenges facing com- puter vision in the field of image deblurring. Here are some of the key facts."
},
{
    "prompt": "summary: Abstract\nRecent Large Language Models (LLMs) have been en-\nhanced with vision capabilities, enabling them to compre-\nhend images, videos, and interleaved vision-language con-\ntent. However, the learning methods of these large multi-\nmodal models (LMMs) typically treat videos as predeter-\nmined clips, rendering them less effective and efficient at\nhandling streaming video inputs.\nIn this paper, we pro-\npose a novel Learning-In-Video-Stream (LIVE) framework,\nwhich enables temporally aligned, long-context, and real-\ntime dialogue within a continuous video stream. Our LIVE\nframework comprises comprehensive approaches to achieve\nvideo streaming dialogue, encompassing: (1) a training ob-\njective designed to perform language modeling for contin-\nuous streaming inputs, (2) a data generation scheme that\nconverts offline temporal annotations into a streaming di-\nalogue format, and (3) an optimized inference pipeline to\nspeed up interactive chat in real-world video streams. With\nBCorresponding Author.\nour LIVE framework, we develop a simplified model called\nVideoLLM-online and demonstrate its significant advan-\ntages in processing streaming videos.\nFor instance, our\nVideoLLM-online-7B model can operate at over 10 FPS\non an A100 GPU for a 5-minute video clip from Ego4D\nnarration.\nMoreover, VideoLLM-online also showcases\nstate-of-the-art performance on public offline video bench-\nmarks, such as recognition, captioning, and forecasting.\nThe code, model, data, and demo have been made available\nat showlab.github.io/videollm-online.\n1. Introduction\nBuilding the future of always-on contextual AI agent that\ncan promptly answer any human question, digitizing inputs\nas episodic memories and forecasting future plans given any\nquery in online continuous setting represents a “holy grail”\nmission in AI research. Powered by advancements in large\nlanguage models (LLMs) [8, 34, 60, 61, 63, 72], recent large\nmultimodal models (LMMs) have unveiled impressive ca-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18407",
    "chosen": "Chen VideoLLM-online Online Video Large Language Model for Streaming Video",
    "rejected": "In our series of letters from African journalists, academics and writers, we look at the challenges of artificial intelligence (AI) in developing a new framework to help video- streaming dialogue within continuous setting."
},
{
    "prompt": "summary: Abstract\nExisting methods for synthesizing 3D human gestures\nfrom speech have shown promising results, but they do not\nexplicitly model the impact of emotions on the generated\ngestures. Instead, these methods directly output animations\nfrom speech without control over the expressed emotion. To\naddress this limitation, we present AMUSE, an emotional\nspeech-driven body animation model based on latent dif-\nfusion. Our observation is that content (i.e., gestures re-\nlated to speech rhythm and word utterances), emotion, and\npersonal style are separable. To account for this, AMUSE\nmaps the driving audio to three disentangled latent vec-\ntors: one for content, one for emotion, and one for personal\nstyle. A latent diffusion model, trained to generate gesture\nmotion sequences, is then conditioned on these latent vec-\ntors. Once trained, AMUSE synthesizes 3D human gestures\ndirectly from speech with control over the expressed emo-\ntions and style by combining the content from the driving\nspeech with the emotion and style of another speech se-\nquence. Randomly sampling the noise of the diffusion model\nfurther generates variations of the gesture with the same\nemotional expressivity. Qualitative, quantitative, and per-\nceptual evaluations demonstrate that AMUSE outputs real-\nistic gesture sequences. Compared to the state of the art, the\ngenerated gestures are better synchronized with the speech\ncontent, and better represent the emotion expressed by the\ninput speech. Our code is available at amuse.is.tue.mpg.de.\n1. Introduction\nAnimating 3D bodies from speech has a wide range of ap-\nplications, such as telepresence in AR/VR, avatar animation\nin games and movies, and to embody interactive digital as-\nsistants. While methods for speech-driven 3D body anima-\ntion have recently shown great progress [5, 7, 31, 56, 101],\nexisting methods do not adequately address one crucial fac-\ntor: the impact of emotion from the driving speech signal on\nthe generated gestures. Emotions and their expressions play\n*Now at Google.\n... choose a major that is easy to ﬁnd a good job in the future... \n... choose a major that is easy to ﬁnd a good job in the future... \nGestures from neutral speech input.\nGestures from surprise synchronized with neutral speech input.\nFigure 1. Goal. AMUSE generates realistic emotional 3D body\ngestures directly from a speech sequence (top). It provides user\ncontrol over the generated emotion by combining the driving\nspeech sequence with a different emotional audio (bottom).\na fundamental role in human communication [29, 35, 65]\nand have become an important consideration when design-\ning computer systems that interact with humans in a nat-\nural manner [78, 79]. They are of central concern when\nsynthesizing human animations for a wide variety of appli-\ncation contexts, such as Socially Interactive Agents [61].\nBecause of this, speech-driven animation systems must not\nonly align movement with the rhythm of the speech, but\nshould also be capable of generating gestures that are per-\nceived as expressing the suitable emotion.\nMany factors contribute to the perception of emotion\nand personal idiosyncrasies, such as facial expressions [19],\ngaze and eye contact [42], physiological responses [47],\ntone of voice [87], body language [66], and gestures [39].\nWhen it comes to 3D animation, the most relevant factors\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1942",
    "chosen": "Chhatre Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion",
    "rejected": "Animating 3D human gestures from speech is a key part of our efforts to understand the impact of emotions on the generated gesture. Here are some of the key facts and lessons we learned."
},
{
    "prompt": "summary: Abstract\nWe introduce a novel approach to single image denoising\nbased on the Blind Spot Denoising principle, which we call\nMAsked and SHuffled Blind Spot Denoising (MASH). We\nfocus on the case of correlated noise, which often plagues\nreal images. MASH is the result of a careful analysis to\ndetermine the relationships between the level of blindness\n(masking) of the input and the (unknown) noise correlation.\nMoreover, we introduce a shuffling technique to weaken the\nlocal correlation of noise, which in turn yields an addi-\ntional denoising performance improvement. We evaluate\nMASH via extensive experiments on real-world noisy image\ndatasets. We demonstrate state-of-the-art results compared\nto existing self-supervised denoising methods. Website:\nhttps://hamadichihaoui.github.io/mash.\n1. Introduction\nThe removal of noise from real images, i.e., image denois-\ning, is a fundamental and still open problem in image pro-\ncessing despite having a long history of dedicated research\n(see [9] for an overview of the classic and recent methods).\nIn classic methods, the primary strategies involve manually\ndesigning image priors and optimization techniques to en-\nhance both reconstruction accuracy and speed. In contrast,\nin the context of deep learning methods, neural networks\nnaturally introduce a very powerful prior for images [24]\nand provide models that could perform denoising efficiently\nat inference time. These innate capabilities of neural net-\nworks opened the doors to a wide range of methods that\ncould not only learn to denoise image from examples of\nnoisy and clean image pairs, but, even more remarkably,\ndirectly from single noisy images [12, 15, 25, 26].\nIn this work, we push the limits of these advanced meth-\nods one step further. We focus on the family of methods\ncalled Blind Spot Denoising (BSD)[15], since it provides a\npowerful and general framework. Moreover, we consider\nthe case where only a single image is used for denoising\n(i.e., we do not rely on a supporting dataset). As also ob-\nserved by Wang et al [25], training on a dataset may not\ngeneralize well on new data, where the noise distribution is\nunknown. This is particularly true for real images, where\nnoise is often correlated. In these settings, most modern\nmethods find it challenging to handle non-iid data.\nIn our work, similar to the approach in [21], we explore\nthe more general setting of random masking beyond the sin-\ngle blind spot method introduced in [15]. In our analysis,\nwe uncover valuable connections between the performance\nof Blind Spot Denoising (BSD) methods trained with vari-\nous input masking techniques and the degree of noise cor-\nrelation. Surprisingly, we observe that models trained with\na higher masking ratio tend to perform better when dealing\nwith highly correlated noise, whereas models trained with a\nlower masking ratio excel in denoising tasks with iid noise.\nThis discovery offers two key contributions: 1) it provides a\nmethod to estimate the unknown level of noise correlation,\nand 2) it offers a strategy for achieving enhanced denoising\nperformance. Furthermore, our analysis reveals that noise\ncorrelation significantly hampers the denoising capabilities\nof BSD models. This suggests that a more radical approach\nwould be to directly eliminate the correlation in the input\ndata. An intuitive method to achieve this would involve\nrandomly permuting all pixels that correspond to the same\nclean-image color intensity. However, this presents a classic\nchicken and egg dilemma, as we would typically need the\nclean image to perform the permutation, yet the clean im-\nage is precisely what we are trying to restore. To tackle this\nchallenge, we utilize an intermediate denoised image as a\npseudo-clean image to define the permutation set. Further-\nmore, given that adjacent pixels are likely to have similar\ncolor intensities, we focus on shuffling only pixels within\nsmall neighborhoods. We incorporate these insights into a\nnovel method called MAsked and SHuffled Blind Spot De-\nnoising (MASH), which we elaborate on further in Sec. 3.\nOur contributions are summarized as follows\n• We provide an analysis of BSD, showcasing the impact of\nvarious masking ratios on correlated noise and presenting\na method for estimating the noise correlation level;\n• We introduce MASH, an enhanced version of BSD that\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n3025",
    "chosen": "Chihaoui Masked and Shuffled Blind Spot Denoising for Real-World Images",
    "rejected": "In our series of letters from African journalists, Zeinab Mohsen writes about a new approach to blind spot denoising. Here is the full transcript of the findings and conclusions of this article."
},
{
    "prompt": "summary: Abstract\nVideo Transformers have become the prevalent solution\nfor various video downstream tasks with superior expressive\npower and flexibility. However, these video transformers\nsuffer from heavy computational costs induced by the mas-\nsive number of tokens across the entire video frames, which\nhas been the major barrier to train and deploy the model.\nFurther, the patches irrelevant to the main contents, e.g.,\nbackgrounds, degrade the generalization performance of\nmodels. To tackle these issues, we propose training-free to-\nken merging for lightweight video Transformer (vid-TLDR)\nthat aims to enhance the efficiency of video Transformers by\nmerging the background tokens without additional training.\nFor vid-TLDR, we introduce a novel approach to capture the\nsalient regions in videos only with the attention map. Fur-\nther, we introduce the saliency-aware token merging strat-\n*: Equal contribution, †: Corresponding author.\negy by dropping the background tokens and sharpening the\nobject scores. Our experiments show that vid-TLDR sig-\nnificantly mitigates the computational complexity of video\nTransformers while achieving competitive performance com-\npared to the base model without vid-TLDR. Code is available\nat https://github.com/mlvlab/vid-TLDR.\n1. Introduction\nWith the success of Transformers in computer vision, e.g.,\nclassification [14, 52], object detection [10, 32, 43, 61, 75,\n77], segmentation [59, 64], a line of works [16, 33, 51, 57,\n60, 76] have proposed video Transformers to comprehend\nthe video for various downstream tasks. The attention mech-\nanism in Transformers shows the desirable characteristics for\nvideo understanding such as the ability to capture the spatial\nand temporal dependencies at the same time. Consequently,\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n18771",
    "chosen": "Choi vid-TLDR Training Free Token Merging for Light-weight Video Transformer",
    "rejected": "This is a full transcript of the CVPR paper, published by the Computer Vision Foundation (CVPR). This paper aims to increase the efficiency of video Transformers. Here is the full text of these proceedings."
},
{
    "prompt": "summary: Abstract\nThe lifting of a 3D structure and camera from 2D land-\nmarks is at the cornerstone of the discipline of computer\nvision.\nTraditional methods have been confined to spe-\ncific rigid objects, such as those in Perspective-n-Point\n(PnP) problems, but deep learning has expanded our ca-\npability to reconstruct a wide range of object classes (e.g.\nC3DPO [18] and PAUL [24]) with resilience to noise, oc-\nclusions, and perspective distortions. However, all these\ntechniques have been limited by the fundamental need to\nestablish correspondences across the 3D training data, sig-\nnificantly limiting their utility to applications where one\nhas an abundance of “in-correspondence” 3D data. Our\napproach harnesses the inherent permutation equivariance\nof transformers to manage varying numbers of points per\n3D data instance, withstands occlusions, and generalizes\n*Both authors advised equally.\nto unseen categories. We demonstrate state-of-the-art per-\nformance across 2D-3D lifting task benchmarks. Since our\napproach can be trained across such a broad class of struc-\ntures, we refer to it simply as a 3D Lifting Foundation Model\n(3D-LFM) – the first of its kind.\n1. Introduction\nLifting 2D landmarks from a single-view RGB image into\n3D has long posed a complex challenge in the field of com-\nputer vision because of the ill-posed nature of the problem.\nThis task is important for a range of applications from aug-\nmented reality to robotics, and requires an understanding\nof non-rigid spatial geometry and accurate object descrip-\ntions [2, 11, 25]. Historically, efforts in single-frame 2D-\n3D lifting have encountered significant hurdles: reliance on\nobject-specific models, poor scalability, and limited adapt-\nability to diverse and complex object categories. Traditional\nmethods, while advancing in specific domains like human\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n10466",
    "chosen": "Dabhi 3D-LFM Lifting Foundation Model",
    "rejected": "In our series of letters from academics, readers are being asked to explain the challenges facing the 3D lifting of landmark structures and cameras. Here are some of the key facts."
},
{
    "prompt": "summary: Abstract\nGenerative models have been very popular in the recent\nyears for their image generation capabilities. GAN-based\nmodels are highly regarded for their disentangled latent\nspace, which is a key feature contributing to their success\nin controlled image editing. On the other hand, diffusion\nmodels have emerged as powerful tools for generating high-\nquality images. However, the latent space of diffusion mod-\nels is not as thoroughly explored or understood. Existing\nmethods that aim to explore the latent space of diffusion\nmodels usually relies on text prompts to pinpoint specific\nsemantics. However, this approach may be restrictive in ar-\neas such as art, fashion, or specialized fields like medicine,\nwhere suitable text prompts might not be available or easy\nto conceive thus limiting the scope of existing work. In this\npaper, we propose an unsupervised method to discover la-\ntent semantics in text-to-image diffusion models without re-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n24209",
    "chosen": "Dalva NoiseCLR A Contrastive Learning Approach for Unsupervised Discovery of Interpretable",
    "rejected": "In our series of letters from academics, readers are being asked to explain why they have been increasingly popular for their use of text-to-image diffusion models (GAN-based models)."
},
{
    "prompt": "summary: Abstract\nIn this work, we tackle the problem of domain gener-\nalization for object detection, specifically focusing on the\nscenario where only a single source domain is available.\nWe propose an effective approach that involves two key\nsteps: diversifying the source domain and aligning detec-\ntions based on class prediction confidence and localiza-\ntion. Firstly, we demonstrate that by carefully selecting a\nset of augmentations, a base detector can outperform ex-\nisting methods for single domain generalization by a good\nmargin. This highlights the importance of domain diversi-\nfication in improving the performance of object detectors.\nSecondly, we introduce a method to align detections from\nmultiple views, considering both classification and localiza-\ntion outputs. This alignment procedure leads to better gen-\neralized and well-calibrated object detector models, which\nare crucial for accurate decision-making in safety-critical\napplications. Our approach is detector-agnostic and can\nbe seamlessly applied to both single-stage and two-stage\ndetectors.\nTo validate the effectiveness of our proposed\nmethods, we conduct extensive experiments and ablations\non challenging domain-shift scenarios.\nThe results con-\nsistently demonstrate the superiority of our approach com-\npared to existing methods. Our code and models are avail-\nable at: https://github.com/msohaildanish/DivAlign.\n1. Introduction\nIn recent years, we have witnessed remarkable performance\nimprovements in supervised object detection [39, 47, 48,\n53].\nThe success of these methods rely on the assump-\ntion that the training and testing data are sampled from the\nsame distribution. However, in many real-world applica-\ntions, such as autonomous driving, this assumption is of-\nten violated and these object detectors usually show de-\ngraded performance due to a phenomenon called domain-\nshift [3, 5, 49]. Shifts in real-world domains are typically\ncaused by environmental alterations, like varying weather\nand time conditions.\nThese changes manifest in diverse\ncontrasts, brightness levels, and textures among others.\nA prominent line of research that attempts to alleviate the\nimpact of domain-shift is known as unsupervised domain\nadaptation (UDA) [20, 24, 42, 49, 63, 68, 72]. Given the\nlabeled data from the source domain and unlabelled data\nfrom the target domain, the aim of UDA methods is to align\nthe source and target data distributions so that the trained\nmodel can generalize well to the target domain [67]. An\nobvious limitation of UDA methods is that they require pre-\ncollecting data and re-training the model for different target\ndomains. Collecting data, even without annotation, for all\npossible domain shifts and training the model when shift\nhappens is difficult and sometime not possible.\nTo cope with the domain-shift problem, a more real-\nistic albeit challenging problem is domain generalization\n[2, 11, 23, 26, 32, 34, 35, 40, 58].\nThe goal of do-\nmain generalization (DG) is to learn a generalizable model\ntypically from multiple source domains, available during\ntraining, that can perform well on the unseen target do-\nmain. A dominant class of DG methods attempts to learn\ndomain-invariant feature space across multiple source do-\nmains [1, 11, 33, 35].\nThe performance of these meth-\nods is sensitive to the number of available source domains\n[7, 70]. In many realistic scenarios, acquiring labeled data\nfrom multiple source domains is often costly and time-\nconsuming, eventually restricting the potential utilization\nof such methods. For these reasons, generalizing from a\nsingle-source domain is a more practical setting.\nRecent surveys [29, 70] reveal that very little work has\nbeen done on DG for object detection [38, 60], despite the\nfact that object detectors occupy an important position in\nmany safety-critical and security-sensitive applications e.g.,\nautonomous driving. An object detector should be able to\nprovide accurate as well as calibrated detections in different\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n17732",
    "chosen": "Danish Improving Single Domain-Generalized Object Detection A Focus on Diversification and",
    "rejected": "This is a full transcript of what we learned from the introduction of the code and models for object detection - based on domain gener- alization. Here is the full text of our findings and recommendations."
},
{
    "prompt": "summary: Abstract\nEven the best current algorithms for estimating body\n3D shape and pose yield results that include body self-\nintersections.\nIn this paper, we present CLOAF, which\nexploits the diffeomorphic nature of Ordinary Differential\nEquations to eliminate such self-intersections while still im-\nposing body shape constraints. We show that, unlike earlier\napproaches to addressing this issue, ours completely elim-\ninates the self-intersections without compromising the ac-\ncuracy of the reconstructions. Being differentiable, CLOAF\ncan be used to ﬁne-tune pose and shape estimation base-\nlines to improve their overall performance and eliminate\nself-intersections in their predictions.\nFurthermore, we\ndemonstrate how our CLOAF strategy can be applied to\npractically any motion ﬁeld induced by the user. CLOAF\nalso makes it possible to edit motion to interact with the\nenvironment without worrying about potential collision or\nloss of body-shape prior.\n1. Introduction\nFeed-forward approaches to estimating human body 3D\nshape and pose from a single image have become remark-\nably effective [6, 25, 35]. The very recent transformer-based\narchitecture of [10] embodies the current state-of-the-art.\nIt is pre-trained on 300 million images and ﬁne-tuned on\nmost SMPL data sets in existence. However, as good as\nthese methods have become, they can still produce unreal-\nistic poses with substantial self-intersections of body parts,\nas illustrated by Fig. 1. This is a serious issue if video-based\nmotion capture is to be used in ﬁelds, such as robotics or re-\nalistic animation, where preventing self-intersections is of\nutmost importance.\nMost current approaches to addressing this issue [4, 13,\n24, 28] are iterative. They penalize self-intersections explic-\nitly by minimizing an interpenetration loss. This requires\nexplicitly detecting self-intersections and then performing\n*This work was supported in part by the Swiss National Science Foun-\ndation\nCollision Rate (%)\nHMR2.0\nOptim\nCLOAF\nMethods\n39.2%\n9.2%\n0.0%\nFigure 1.\nSelf-intersections in SOTA methods.\nTop rows.\nHMR2.0 [10] (ﬁrst row) and PARE [19] (second row), two of the\nbest current methods, produce bodies shown in blue with self-\nintersections.\nCLOAF removes them and generates the results\nshown in gold. Bottom row. HMR 2.0 [10] recovers bodies with\nself-intersections in 39.2% of frames of the 3DPW-test set. A\nrecent post-processing method such as [28] brings this down to\n9.2%. CLOAF drops this number all the way to zero.\na separate optimization step, which makes the whole pro-\ncess non-differentiable and precludes its use during train-\ning. Another approach is to eliminate self-intersections in\nthe training databases [26]. While all these methods help,\nthey do not guarantee the absence of self-collisions at infer-\nence time.\nIn this paper, we propose a different approach. It pre-\nvents self-intersections in a differentiable manner and with-\nout an explicit detection step. To this end, we rely on the\nfact that if the scene ﬂow from one body to another is the\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1176",
    "chosen": "Davydov CLOAF CoLlisiOn-Aware Human Flow",
    "rejected": "In our series of letters from scientists, we explain how the best algorithms for measuring human body shape and pose have become remark- ably effective. Here is a summary of what we learned."
},
{
    "prompt": "summary: Abstract\nRecently, building on the foundation of neural radiance\nﬁeld, various techniques have emerged to learn unsigned\ndistance ﬁelds (UDF) to reconstruct 3D non-watertight\nmodels from multi-view images. Yet, a central challenge\nin UDF-based volume rendering is formulating a proper\nway to convert unsigned distance values into volume den-\nsity, ensuring that the resulting weight function remains un-\nbiased and sensitive to occlusions. Falling short on these\nrequirements often results in incorrect topology or large\nreconstruction errors in resulting models. This paper ad-\ndresses this challenge by presenting a novel two-stage al-\ngorithm, 2S-UDF, for learning a high-quality UDF from\nmulti-view images. Initially, the method applies an easily\ntrainable density function that, while slightly biased and\ntransparent, aids in coarse reconstruction. The subsequent\nstage then reﬁnes the geometry and appearance of the ob-\nject to achieve a high-quality reconstruction by directly ad-\njusting the weight function used in volume rendering to en-\nsure that it is unbiased and occlusion-aware. Decoupling\ndensity and weight in two stages makes our training sta-\nble and robust, distinguishing our technique from existing\nUDF learning approaches. Evaluations on the DeepFash-\nion3D, DTU, and BlendedMVS datasets validate the ro-\nbustness and effectiveness of our proposed approach. In\nboth quantitative metrics and visual quality, the results in-\ndicate our superior performance over other UDF learn-\ning techniques in reconstructing 3D non-watertight models\nfrom multi-view images. Our code is available at https:\n//bitbucket.org/jkdeng/2sudf/.\n1. Introduction\nAs the success of neural radiance ﬁeld (NeRF) [29], numer-\nous volume rendering based 3D modeling methods are pro-\n*Corresponding author\nGT\nOurs\nNeuralUDF\nNeUDF\nFigure 1.\nWe learn a UDF from multiview images for non-\nwatertight model reconstruction. As illustrated in the cross sec-\ntions of learned UDFs, our learned UDF approximates to the\nground truth. In contrast, the learned UDF of NeuralUDF [25]\nis choppy leading to signiﬁcant artifacts, e.g., unexpected pit. The\nlearned UDF of NeUDF [23] is almost closed struggling to gener-\nate open surface.\nposed to learn signed distance ﬁelds (SDF) for 3D model re-\nconstruction from multi-view images [7, 34, 36, 40]. These\napproaches map signed distance value to a density function,\nthereby enabling the use of volume rendering to learn an\nimplicit SDF representation. To calculate pixel colors, they\ncompute the weighted sum of radiances along each light\nray. Achieving an accurate surface depiction requires the\ndensity function to meet three essential criteria. Firstly, the\nweights, which are derived from the density function, must\nreach their maximum value when the distance is zero, ensur-\ning unbiasedness. Secondly, as a ray traverses through the\nsurface, the accumulated density should tend towards inﬁn-\nity, rendering the surface opaque — a property referred to as\nocclusion-awareness. Finally, the density function should\nbe bounded to prevent numerical issues. The popular SDF\napproaches, such as NeuS [34] and VolSDF [40], adopt an\nS-shaped density function that meets all these requirements.\nWhile SDF-based methods excel at reconstructing wa-\ntertight models, they have limitations in representing open\nmodels. This is due to the intrinsic nature of SDF, which\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n5084",
    "chosen": "Deng 2S-UDF A Novel Two-stage UDF Learning Method for Robust Non-watertight",
    "rejected": "This is a full transcript of how volume rendering based 3D modelling techniques can be learned by the University of Cambridge University. The BBC's Jonathan Head looks at the challenges facing these techniques."
}
]