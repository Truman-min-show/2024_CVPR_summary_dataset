[
    {
        "prompt": "summary: Efficient generation of 3D digital humans is important in several industries, including virtual reality, social media, and cinematic production. 3D generative adversarial networks (GANs) have demonstrated state-of-the-art (SOTA) quality and diversity for generated assets. Current 3D GAN architectures, however, typically rely on volume representations, which are slow to render, thereby hampering the GAN training and requiring multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps (GSMs) as a framework that connects SOTA generator network architectures with emerging 3D Gaussian rendering primitives using an articulable multi shell–based scaffold. In this setting, a CNN generates a 3D texture stack with features that are mapped to the shells. The latter represent inflated and deflated versions of a template surface of a digital human in a canonical body pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the shells whose attributes are encoded in the texture features. These Gaussians are efficiently and differentiably rendered. The ability to articulate the shells is important during GAN training and, at inference time, to deform a body into arbitrary userdefined poses. Our efficient rendering scheme bypasses the need for view-inconsistent upsamplers and achieves highquality multi-view consistent renderings at a native resolution of 512 × 512 pixels. We demonstrate that GSMs successfully generate 3D humans when trained on single-view datasets, including SHHQ and DeepFashion. Project Page: rameenabdal.github.io/GaussianShellMaps 1. Introduction The ability to generate articulable three-dimensional digital humans augments traditional asset creation and animation workflows, which are laborious and costly. Such generative artificial intelligence–fueled workflows are crucial in several applications, including communication, cinematic production, and interactive gaming, among others. 3D Generative Adversarial Networks (GANs) have emerged as the state-of-the-art (SOTA) platform in this domain, enabling the generation of diverse 3D assets at interactive framerates [6, 8, 15, 23, 41, 71, 76, 76]. Most existing 3D GANs build on variants of volumetric scene representations combined with neural volume rendering [65]. However, volume rendering is relatively slow and compromises the training of a GAN, which requires tens of millions of forward rendering passes to converge [11]. Mesh–based representations building on fast differentiable rasterization have been proposed to overcome this limitation [19, 71], * Equal Contribution † Work done as a visiting student researcher at Stanford University This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 9441",
        "chosen": "Gaussian Shell Maps (GSMs) are introduced to enhance 3D human generation efficiency. This framework combines generator network architectures with 3D Gaussian rendering primitives using an articulable multi-shell scaffold. A CNN generates a 3D texture stack, mapping features to shells representing inflated/deflated versions of a template surface. Instead of direct rasterization, 3D Gaussians are sampled on the shells, with attributes encoded in texture features. This method enables efficient, differentiable rendering, bypassing the need for view-inconsistent upsamplers and achieving high-quality multi-view consistent renderings at 512 × 512 pixels. GSMs successfully generate 3D humans when trained on single-view datasets like SHHQ and DeepFashion.",
        "rejected": "Efficient generation of 3D digital humans using Gaussian Shell Maps that connect generator networks with 3D Gaussian rendering primitives. This approach allows for fast, differentiable rendering and supports multi-view consistent outputs at high resolutions without requiring upsamplers."
    },
    {
        "prompt": "summary: Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-ﬁnders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to deﬁne and compute a notion of ‘conceptual similarity’ among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate ‘explanations’ (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this deﬁnition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity benchmarks. Beyond just providing a number, our method also offers interpretability by pointing to the speciﬁc level of granularity of the description where the source data are differentiated. 1. Introduction Consider the two images in Fig. 1. One could say they are similar: both portray small red Italian cars. Another could say they are different: One is a sports car in an open space, the other a tiny city car in an alley. Is there an objective way of measuring the similarity among images? In some cases, similarity judgments can be inﬂuenced by shared concepts: in Fig. 2, two images share compelling stylistic and conceptual similarity, but it is difﬁcult to identify speciﬁc visual elements they share. Yet the two were found to be legally “substantially similar” [37]. Can we deﬁne an objective notion of ‘conceptual similarity’ among data? There have been many attempts at deﬁning an objective notion of similarity based on the number of bits of information that the two samples share [11, 25, 40], but fundamentally they do not capture concepts (Appendix A), which are human constructs. Since humans must play a role in deﬁning similarity among concepts, one way to achieve objectivity is by averaging a large number of human assessments. This is what large-scale neural network models do [34]. However, contrastive-trained models measure similarity based on how easily the data can be distinguished, and any two non-identical samples can be distinguished by random features of no conceptual relevance [22]. Rather than focusing on ﬁnding the features that best distinguish the two images, we focus on ﬁnding the ones that best describe them. Then, we can measure semantic similarity based on how well descriptions of one ﬁt the other. If two samples are similar, a short description should apply equally well to both. If the samples are very different, the description of one will be a poor ﬁt for the other. The more complex the description needed to distinguish the images, the higher their conceptual similarity. The key idea is to focus on optimally describing individual samples, rather than adversarially discriminating them. Speciﬁcally, referring to Fig. 1, we generate multiple descriptions of each sample in increasing level of complexity, sorted by their coding length. Then, we measure the difference of the likelihood of each image conditioned on either descriptions as a function of complexity. That traces a curve that measures distance as a function of complexity. Any two images can be distinguished by their description at some point (the more you look, the more differences you see). Therefore, similarity should always be relative to a description length. However, when a single number is needed for comparison, we show that the AUC of the distance function is well aligned with human similarity assessments. Our proposed method, which we call ComplexityConstrained Descriptive Autoencoding, or CC:DAE, is rooted in the idea of the Kolmogorov Structure Function, This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11062",
        "chosen": "A novel method called Complexity-Constrained Descriptive Autoencoding (CC:DAE) is proposed to define and compute 'conceptual similarity' among images. This method uses a base multi-modal model to generate 'explanations' (captions) of visual data at increasing complexity levels. Similarity is measured by the caption length needed to discriminate between two images. CC:DAE correlates with subjective human assessments and outperforms existing baselines on image-to-image and text-to-text similarity benchmarks. It also offers interpretability by indicating the description granularity level where source data differentiation occurs.",
        "rejected": "This work introduces a method to quantify 'conceptual similarity' among images by generating explanations of increasing complexity. The length of these descriptions needed to differentiate between two images defines their similarity, providing an interpretable metric aligned with human assessments."
    },
    {
        "prompt": "summary: Vision-Language Models (VLMs) such as CLIP are trained on large amounts of image-text pairs, resulting in remarkable generalization across several data distributions. However, in several cases, their expensive training and data collection/curation costs do not justify the end application. This motivates a vendor-client paradigm, where a vendor trains a large-scale VLM and grants only input-output access to clients on a pay-per-query basis in a black-box setting. The client aims to minimize inference cost by distilling the VLM to a student model using the limited available task-specific data, and further deploying this student model in the downstream application. While naive distillation largely improves the In-Domain (ID) accuracy of the student, it fails to transfer the superior outof-distribution (OOD) generalization of the VLM teacher using the limited available labeled images. To mitigate this, we propose Vision-Language to Vision - Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and language modalities of the teacher model with the vision modality of a pre-trained student model, and further distills the aligned VLM representations to the student. This maximally retains the pre-trained features of the student, while also incorporating the rich representations of the VLM image encoder and the superior generalization of the text embeddings. The proposed approach achieves state-of-the-art results on the standard Domain Generalization benchmarks in a black-box teacher setting as well as a white-box setting where the weights of the VLM are accessible. Project page: http://val.cds.iisc.ac.in/VL2V-ADiP/ 1. Introduction While the initial success of Deep Learning was predominantly driven by training specialized models for each task or dataset [26, 29], recent research on foundation models [24, 34, 47, 65] eliminates the need for this by training generic models jointly over several modalities using large*Equal Contribution. Correspondence to Sravanti Addepalli <sravantia@iisc.ac.in>, Ashish Ramayee Asokan <ashish.ramayee@gmail.com> Figure 1. Schematic diagram showing class and domain distributions in the shared text/ image embedding space of a VLM: VLMs learn highly specialized image representations that are not domain invariant. Thus, a linear classifier (red decision boundary) that is trained over the vision encoder using limited training data cannot generalize well to the target domain (shown in purple). On the other hand, generic text embeddings such as “A photo of a class” represent the core concept of a class by virtue of their training method and vast training data. Thus, they generalize effectively across domains, and a zero-shot classifier (green decision boundary) aligns better with the true distribution of classes. scale data. The use of both image and language modalities in large-scale Vision-Language Models (VLMs) enables their use in several applications including zero-shot classification, where the embedding of an image is compared with text embeddings for “a photo of a {class}” corresponding to every class during inference and the class with highest similarity is predicted. For example, the LiT (private) model [65] has been trained on 4 billion image-text pairs scraped from the web, and it achieves 85.2% zero-shot accuracy on ImageNet. VLMs also demonstrate extraordinary performance across several distributions, owing to the vast diversity of distributions seen during their training [47]. The remarkable OOD generalization of VLMs makes them suitable for use in several applications, either directly This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 23922",
        "chosen": "VL2V-ADiP is introduced to improve domain generalization in image classification by leveraging Vision-Language Models (VLMs). This method first aligns the vision and language modalities of a VLM teacher with a pre-trained student model's vision modality, then distills the aligned VLM representations to the student. This approach retains the student's pre-trained features while incorporating the rich representations of the VLM image encoder and superior generalization of text embeddings. VL2V-ADiP achieves state-of-the-art results on standard Domain Generalization benchmarks in both black-box and white-box teacher settings.",
        "rejected": "Vision-Language Models trained on large datasets achieve remarkable generalization but can be costly. A proposed distillation method called VL2V-ADiP aligns and transfers knowledge from a VLM teacher to a student model, maintaining pre-trained features while incorporating rich representations for improved out-of-distribution performance."
    },
    {
        "prompt": "summary: Scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities in neural radiance ﬁelds, i.e., the densities double when scene size is halved, and vice versa. We call this property alpha invariance. For NeRFs to better maintain alpha invariance, we recommend 1) parameterizing both distance and volume densities in log space, and 2) a discretization-agnostic initialization strategy to guarantee high ray transmittance. We revisit a few popular radiance ﬁeld models and ﬁnd that these systems use various heuristics to deal with issues arising from scene scaling. We test their behaviors and show our recipe to be more robust. Visit our project page at https://pals.ttic.edu/p/alpha-invariance. 1. Introduction 3D computer graphics and vision are fundamentally scaleambiguous. Lengths of objects and scenes are often unitless and measure up to a constant ratio. This is ﬁne for routines such as projection, triangulation, and camera motion estimation. A common practice is to normalize the scene dimension or the size of a reference object to an arbitrary number. Volume rendering [17] used by Neural Radiance Fields (NeRFs) [20] presents a complication due to explicit integration over space. Since the distance scaling is arbitrary, the volumetric density function σ(x) must compensate for the multiplicative factor to render the same ﬁnal RGB color. In other words, if the scene size expands by a factor k, it is sufﬁcient for the learned σ to shrink by 1/k to be invariant to the change. The same applies to integration over cones or more general spatial data structures [1, 3]. Note that in addition to scene dimensions, the magnitude of σ is also inﬂuenced by the number of samples per ray and ultimately the sharpness of change in local opacity. There is no single “correct” size for a scene setup. A * Equal contribution. Figure 1. A discretized view of volume rendering. Top: a ray is cut into intervals, each with a density σi ≥0 and interval length di. Bottom: illustration of the weight given to the 3rd interval, computed through alpha compositing. The rendered color is obtained by weighting all the interval colors with their wis. If we scale each di by a constant k, scaling σi by 1 k renders the identical color. robust algorithm should be able to perform consistently across different scalings. We investigate how this notion of invariance manifests itself in practice, discuss how the hyperparameter decisions affect it, and propose a solution that ensures robustness to distance scaling across the NeRF methods. We revisit and experiment with a few popular NeRF architectures: Vanilla NeRF [20], TensoRF [6], DVGO [30], Plenoxels [8], and Nerfacto [31], and ﬁnd that many systems use tailored heuristics that work well at a particular scene size. We analyze the impact of some critical hyperparameters which are often overlooked or not highlighted in the original papers. In our testing of these models we identify two main failure modes. When the scene is scaled down (short ray interval d), some models struggle to produce large enough σ values for solid geometry. When the scene is scaled up (long interval length), the σ at initialization is often too large, resulting in cloudiness that traps the optimization at bad local optima. We therefore propose 1) parameterizing both distance and volume densities in log space for easier multiplicative scaling; 2) a closed-form formula for σ value initialization that guarantees high ray transmittance and scene transparency. We show that these two ingredients can robustly handle variThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20396",
        "chosen": "This paper addresses alpha invariance in Neural Radiance Fields (NeRFs), where scale-ambiguity in 3D scene dimensions leads to magnitude-ambiguity of volumetric densities. To maintain alpha invariance, the authors recommend parameterizing both distance and volume densities in log space and using a discretization-agnostic initialization strategy for high ray transmittance. They revisit popular radiance field models, finding varied heuristics for handling scene scaling issues. Testing shows their approach is more robust across different NeRF methods and scene scalings.",
        "rejected": "Addressing scale-ambiguity in neural radiance fields, this paper recommends parameterizing distance and volume densities in log space for robustness. An initialization strategy ensuring high ray transmittance is also proposed, enhancing consistency across different scene scalings."
    },
    {
        "prompt": "summary: Unsupervised (US) video anomaly detection (VAD) in surveillance applications is gaining more popularity recently due to its practical real-world applications. As surveillance videos are privacy sensitive and the availability of large-scale video data may enable better US-VAD systems, collaborative learning can be highly rewarding in this setting. However, due to the extremely challenging nature of the US-VAD task, where learning is carried out without any annotations, privacy-preserving collaborative learning of US-VAD systems has not been studied yet. In this paper, we propose a new baseline for anomaly detection capable of localizing anomalous events in complex surveillance videos in a fully unsupervised fashion without any labels on a privacy-preserving participant-based distributed training configuration. Additionally, we propose three new evaluation protocols to benchmark anomaly detection approaches on various scenarios of collaborations and data availability. Based on these protocols, we modify existing VAD datasets to extensively evaluate our approach as well as existing US SOTA methods on two large-scale datasets including UCF-Crime and XD-Violence. All proposed evaluation protocols, dataset splits, and codes are available here: https://github.com/AnasEmad11/CLAP. 1. Introduction Recent years have seen a surge in federated learning based methods, where the goal is to enable collaborative training of machine learning models without transferring any training data to a central server. This direction of research in machine learning is of notable importance as it enables learning with multiple participants that can contribute data without compromising privacy. Several researchers have studied federated learning for different applications such as medical diagnosis [1, 3, 6, 22], network security [10, 19, 21, 30], and large-scale classification models [4, 14, 42]. Anomaly detection in surveillance videos, being one ... Central Parameter Sharing Server Server Conventional Centralized Training Ours: Collaborative Training ... ... ... Figure 1. a) Conventional central training requires all training data to be on the server to carry out the training. This setting cannot ensure privacy, thus hindering collaborations between different entities holding large-scale surveillance data. b) Our proposed unsupervised video anomaly detection technique does not require the transfer of training data between the server and participants, thus ensuring complete privacy. of the large-scale applications of computer vision, may greatly benefit from autonomous collaborative training algorithms. VAD in surveillance videos is privacy sensitive and may involve data belonging to several organizations/establishments. This may result in hectic bureaucratic processes to obtain data from each establishment for centralized training. For example, the police department of a city may not be willing to share street surveillance videos due to privacy concerns of the general public, or a daycare facility may have to obtain the consent of all parents to be allowed to share its CCTV footage. Such restrictions may hinder the possibility of obtaining large-scale data to train efficient anomaly detectors making a central training requiring all training data the least preferred option in the realworld scenarios. Unfortunately, to the best of our knowlThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12416",
        "chosen": "CLAP is proposed as a new baseline for unsupervised video anomaly detection in surveillance, capable of localizing anomalies without labels in a privacy-preserving distributed training setup. The authors also introduce three new evaluation protocols to benchmark anomaly detection approaches across various collaboration scenarios and data availabilities. They modify existing VAD datasets to extensively evaluate their approach and existing US SOTA methods on large-scale datasets like UCF-Crime and XD-Violence. All evaluation protocols, dataset splits, and codes are made available.",
        "rejected": "An unsupervised video anomaly detection technique suitable for privacy-preserving collaborative learning is introduced. It localizes anomalies in surveillance videos without labels and proposes new evaluation protocols tested on large-scale datasets including UCF-Crime and XD-Violence."
    },
    {
        "prompt": "summary: This paper addresses complex challenges in histopathological image analysis through three key contributions. Firstly, it introduces a fast patch selection method, FPS, for whole-slide image (WSI) analysis, significantly reducing computational cost while maintaining accuracy. Secondly, it presents PathDino, a lightweight histopathology feature extractor with a minimal configuration of five Transformer blocks and only ≈9 million parameters, markedly fewer than alternatives. Thirdly, it introduces a rotation-agnostic representation learning paradigm using self-supervised learning, effectively mitigating overfitting. We also show that our compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including both internal datasets spanning four sites (breast, liver, skin, and colorectal) and seven public datasets (PANDA, CAMELYON16, BRACS, DigestPath, Kather, PanNuke, and WSSS4LUAD). Notably, even with a training dataset of ≈6 million histopathology patches from The Cancer Genome Atlas (TCGA), our approach demonstrates an average 8.5% improvement in patch-level majority vote performance. These contributions provide a robust framework for enhancing image analysis in digital pathology, rigorously validated through extensive evaluation. 1 1. Introduction The advent of whole slide image (WSI) scanning in digital pathology has revolutionized the research in computational pathology [1–3]. While digital pathology enables both researchers and clinicians to enjoy the ease of access to the WSIs, processing and storing these gigapixel images are still quite challenging. Motivation: Large image size and scarce or lack of patchlevel labels (annotations) pose two main challenges in WSI analysis [4]. As a result, most state-of-the-art meth*Corresponding author 1The project page: https://KimiaLabMayo.github.io/ PathDino-Page/ Figure 1. HistoRotate. A 360◦rotation augmentation for training models on histopathology images. Unlike training on natural images where the rotation may change the context of the visual data, rotating a histopathology image improves the learning process for discriminative embedding learning. ods adopt Multi-instance Learning (MIL) with weak supervision [5–13]. While these approaches may eliminate the need for pixel-level annotations, MIL significantly increases computational loads and potentially lowers the quality of results compared to fully supervised approaches. While some attempts have been made to select representative patches [5,6,14,15], many such methods remain computationally intensive, leaving the desire for efficient, accurate solutions an unmet need. The field of image analysis in digital pathology has predominantly adopted deep models designed for natural image analysis without further customization to the field [16–19]. While showing good performance on natural image analysis, pre-trained deep models may not fully exploit the unique characteristics of histopathology images. Furthermore, most current training recipes for histopathological embedding learning adopt conventional training and common augmentation techniques for natural images [18]. However, histopathology images have arguably very different features compared to natural images and even radiology images. This gap motivated us to design an improved training approach for histopathology images. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11683",
        "chosen": "This paper presents three key contributions for histopathological image analysis: a fast patch selection method (FPS) for whole-slide image (WSI) analysis, PathDino (a lightweight histopathology feature extractor with five Transformer blocks and ≈9 million parameters), and a rotation-agnostic representation learning paradigm using self-supervised learning. The compact model outperforms existing state-of-the-art histopathology-specific vision transformers on 12 diverse datasets, including internal and public datasets. It shows an average 8.5% improvement in patch-level majority vote performance with a training dataset of ≈6 million histopathology patches from TCGA.",
        "rejected": "A framework for histopathological image analysis through a fast patch selection method (FPS), PathDino feature extractor, and rotation-agnostic representation learning reduces computational costs and overfitting. The compact model outperforms state-of-the-art alternatives on various datasets, demonstrating significant improvements in accuracy."
    },
    {
        "prompt": "summary: Image and video analysis requires not only accurate object detection but also the understanding of relationships among detected objects. Common solutions to relation modeling typically resort to stand-alone object detectors followed by non-differentiable post-processing techniques. Recently introduced detection transformers (DETR) perform end-to-end object detection based on a bipartite matching loss. Such methods, however, lack the ability to jointly detect objects and resolve object associations. In this paper, we build on the DETR approach and extend it to the joint detection of objects and their relationships by introducing an approximated bipartite matching. While our method can generalize to an arbitrary number of objects, we here focus on the modeling of object pairs and their relations. In particular, we apply our method PairDETR to the problem of detecting human bodies and faces, and associating them for the same person. Our approach not only eliminates the need for hand-designed post-processing but also achieves excellent results for body-face associations. We evaluate PairDETR on the challenging CrowdHuman and CityPersons datasets and demonstrate a large improvement over the state of the art. Our training code and pre-trained models are available at https://github.com/mts-ai/pairdetr 1. Introduction The detection of objects and their relationships is one of the key challenges in computer vision. It involves the prediction of bounding boxes for given object categories and assigning pairs of detected objects to particular relations. For example, the detection of human faces and bodies, as well as the association of them for the same person, is essential for applications including human-computer interaction (gestures detection), gaming (in virtual reality), fitness and sport (from virtual coaching to automated match tracking), mass market (virtual try-on), and many others. Person detection is a well-studied problem in computer vision. However, common approaches often focus on detecting human bodies, faces, and heads in isolation, without considering relationships among them. Given face and body detections, typical systems resolve object relations based on ad-hoc post-processing techniques, such as non-maxima suppression followed by the matching of overlapping object bounding boxes. In this work, we formulate the detection and association of object pairs as a graph prediction problem. While applyThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 423",
        "chosen": "PairDETR extends the DETR approach to jointly detect objects and resolve their associations by introducing an approximated bipartite matching. The method focuses on modeling object pairs and their relations, particularly for detecting human bodies and faces and associating them for the same person. PairDETR eliminates the need for hand-designed post-processing and achieves excellent results for body-face associations. It is evaluated on CrowdHuman and CityPersons datasets, demonstrating significant improvement over the state of the art.",
        "rejected": "Building upon DETR, PairDETR extends object detection to include relationship modeling via approximated bipartite matching. This end-to-end approach eliminates the need for post-processing steps and achieves superior results for detecting and associating human bodies and faces."
    },
    {
        "prompt": "summary: The reflective nature of the human eye is an underappreciated source of information about what the world around us looks like. By imaging the eyes of a moving person, we capture multiple views of a scene outside the camera’s direct line of sight through the reflections in the eyes. In this paper, we reconstruct a radiance field beyond the camera’s line of sight using portrait images containing eye reflections. This task is challenging due to 1) the difficulty of accurately estimating eye poses and 2) the entangled appearance of the iris textures and the scene reflections. To address these, our method jointly optimizes the cornea poses, the radiance field depicting the scene, and the observer’s eye iris texture. We further present a regularization prior on the iris texture to improve scene reconstruction quality. Through various experiments on synthetic and real-world captures featuring people with varied eye colors, and lighting conditions, we demonstrate the feasibility of our approach to recover the radiance field using cornea reflections. *Equal contribution 1. Introduction The human eye is a remarkable organ that enables vision and holds valuable information about the surrounding world. While we typically use our own eyes as two lenses to focus light onto the photosensitive cells composing our retina, we would also capture the light reflected from the cornea if we look at someone else’s eyes. When we use a camera to image the eyes of another, we effectively turn their eyes into a pair of mirrors in the overall imaging system. Since the light that reflects off the observer’s eyes share the same source as the light that reaches their retina, our camera can form images containing information about the surrounding world the observer sees. Prior studies have explored recovering a panoramic image of the world the observer sees and simple 3D structures like boxes that the observer is looking at from manually specified correspondences from a single image of two eyes [27, 28]. Follow-up works have further explored applications such as personal identification [10, 29], detecting grasp posture [53], focused object estimation [40], illumination estimation [48], and relighting [26]. Given the recent advancements in 3D vision and graphics, we wonder: Can we do more than reconstruct a single panoramic environThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4864",
        "chosen": "This paper reconstructs a radiance field beyond the camera’s line of sight using portrait images containing eye reflections. The method jointly optimizes cornea poses, the scene’s radiance field, and the observer’s eye iris texture, with a regularization prior on the iris texture for improved scene reconstruction quality. Experiments on synthetic and real-world captures show the approach’s feasibility in recovering the radiance field using cornea reflections, regardless of eye color and lighting conditions.",
        "rejected": "Reconstructing radiance fields beyond the camera's line of sight using eye reflections captured in portrait images. By optimizing cornea poses, radiance field, and iris texture, this method demonstrates feasibility in recovering scenes reflected in eyes under varied conditions."
    },
    {
        "prompt": "summary: We introduce FaceTalk1, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporallyoptimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the 1Project Page: https://shivangi- aneja.github.io/ projects/facetalk NPHM shape space. Our experimental results substantiate the effectiveness of FaceTalk, consistently achieving superior and visually natural motion, encompassing diverse facial expressions and styles, outperforming existing methods by 75% in perceptual user study evaluation. 1. Introduction Modeling 3D animation of humans has a wide range of applications in the realm of digital media, including animated movies, computer games, and virtual assistants. In recent years, there have been numerous works proposing generative approaches for motion synthesis of human bodies, enabling the animation of human skeletons conditioned on various signals such as action [3, 25, 46], language [1, 4, 36, 47, 64, 77] and music [2, 67]. While human faces are critical to synthesis of humans, generative synthesis of 3D faces in motion has focused on 3D morphable models (3DMMs) leveraging linear blendshapes [7, 40] to represent head motion and expression. Such models characterize a disentangled space of head shape and motion, but lack the capacity to comprehensively represent the complexity and fine-grained details of human face geometry in motion (e.g., hair, skin furrowing during motion, etc.). Thus, we propose to represent animated head sequences This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 21263",
        "chosen": "FaceTalk is a novel generative approach for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio. It couples speech signals with the latent space of neural parametric head models, using a new latent diffusion model in the expression space of these models. In the absence of a dataset with NPHM expressions corresponding to audio, the authors optimize for these correspondences. FaceTalk generates plausible motion sequences for high-fidelity head animation, outperforming existing methods by 75% in perceptual user study evaluation.",
        "rejected": "FaceTalk1 synthesizes high-fidelity 3D motion sequences of talking heads from audio signals using a latent diffusion model in expression space. It couples speech with neural parametric head models to create realistic, temporally coherent sequences optimized from audio-video data."
    },
    {
        "prompt": "summary: Learning representations to capture the very fundamental understanding of the world is a key challenge in machine learning. The hierarchical structure of explanatory factors hidden in data is such a general representation and could be potentially achieved with a hierarchical VAE. However, training a hierarchical VAE always suffers from the “posterior collapse”, where the data information is hard to propagate to the higher-level latent variables, hence resulting in a bad hierarchical representation. To address this issue, we first analyze the shortcomings of existing methods for mitigating the posterior collapse from an information theory perspective, then highlight the necessity of regularization for explicitly propagating data information to higher-level latent variables while maintaining the dependency between different levels. This naturally leads to formulating the inference of the hierarchical latent representation as a sequential decision process, which could benefit from applying reinforcement learning (RL). Aligning RL’s objective with the regularization, we first introduce a skip-generative path to acquire a reward for evaluating the information content of an inferred latent representation, and then the developed Q-value function based on it could have a consistent optimization direction of the regularization. Finally, policy gradient, one of the typical RL methods, is employed to train a hierarchical VAE without introducing a gradient estimator. Experimental results firmly support our analysis and demonstrate that our proposed method effectively mitigates the posterior collapse issue, learns an informative hierarchy, acquires explainable latent representations, and significantly outperforms other hierarchical VAE-based methods in downstream tasks. 1. Introduction Deriving meaningful representations of data with minimal supervision is a central challenge in machine learn*Equal contribution, authors listed alphabetically by last name. †Corresponding to: Yewen Li <yewen001@e.ntu.edu.sg>. x z1 z2 z3 z4 z5 HVAE x z1 z2 z3 z4 z5 IVAE HVAE IVAE Hierarchy level→ Hierarchy level→ Figure 1. Visualization of different levels’ latent representations of hierarchical VAEs on FashinMNIST and CelebA [53] to demonstrate the learned hierarchical structure. HVAE fails to learn a 5-level hierarchy, where higher-level latent representations display posterior collapse, collapsing into the same modes of “clothing” and “female faces” regardless of the input. For IVAE, reconstructions of certain consecutive layers appear highly similar, suggesting a disrupted hierarchy. In contrast, our method preserves detailed information of inputs at low levels and captures increasingly abstract semantics at higher levels, mitigating posterior collapse and establishing an informative hierarchy. ing [5], while existing research has predominantly concentrated on the discriminative approach [10] that relies on meticulously crafted preprocessing pipelines like pretext tasks [22, 30, 62, 65, 95] and data augmentations [3, 59]. Methods such as contrastive learning [8, 9, 11, 12, 14– 16, 27, 29, 32, 34, 36, 49, 87, 97, 98] exemplify the success of this discriminative approach. However, representations derived from these methods are only limited to tasks invariant to the preprocessing pipelines [24], e.g. representations learned with the random cropping data augmentation cannot be applied to pixel-level localization tasks [83, 84], limiting their broader applicability. To transcend these confines, Bengio et al. [5] advocate for the pursuit of a universal and fundamental understanding of the world-a “general-purpose prior”-enabling the direct learning of representations without prior knowledge or assumptions about downstream tasks. Such representations could be learned by generative apThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 22946",
        "chosen": "A method combining reinforcement learning (RL) is proposed to improve unsupervised hierarchical representation learning in hierarchical VAEs, addressing the posterior collapse issue. The approach involves analyzing existing methods' shortcomings, emphasizing regularization for data information propagation to higher-level latent variables, and formulating hierarchical latent representation inference as a sequential decision process. RL is applied using a skip-generative path for reward acquisition and a Q-value function for consistent optimization. Policy gradient trains the hierarchical VAE without introducing a gradient estimator. Experimental results show the method effectively mitigates posterior collapse, learns informative hierarchies, and significantly outperforms other hierarchical VAE-based methods in downstream tasks.",
        "rejected": "Innovative methods in generating 3D digital humans, measuring image similarity, improving Vision-Language Models, addressing scale ambiguities, detecting anomalies, analyzing histopathological images, extending object detection, reconstructing scenes, and synthesizing head motions are highlighted. Each addresses specific challenges within its domain."
    },
    {
        "prompt": "summary: Human perception and understanding is a major domain of computer vision which, like many other vision subdomains recently, stands to gain from the use of large models pre-trained on large datasets. We hypothesize that the most common pre-training strategy of relying on general purpose, object-centric image datasets such as ImageNet, is limited by an important domain shift. On the other hand, collecting domain-specific ground truth such as 2D or 3D labels does not scale well. Therefore, we propose a pre-training approach based on self-supervised learning that works on human-centric data using only images. Our method uses pairs of images of humans: the first is partially masked and the model is trained to reconstruct the masked parts given the visible ones and a second image. It relies on both stereoscopic (cross-view) pairs, and temporal (cross-pose) pairs taken from videos, in order to learn priors about 3D as well as human motion. We pre-train a model for body-centric tasks and one for hand-centric tasks. With a generic transformer architecture, these models outperform existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and obtain state-of-the-art performance for instance when fine-tuning for model-based and model-free human mesh recovery. 1. Introduction The main catalyst of performance improvement in computer vision tasks in the last decade has arguably been the training of large models on large datasets [11, 23, 45, 49, 51, 64, 80]. For human-centric vision tasks, the standard approach is to pre-train models on ImageNet classification tasks and then fine-tune them on downstream tasks with specific datasets [31, 35, 55, 74]. This has at least three drawbacks: a) the size of the pre-training dataset is limited by label acquisition, b) there can be a large domain shift between ImageNet and downstream images, c) object-centric classification is different from human understanding, which may hinder the relevance of pre-trained features. Collecting large annotated datasets for human centric vision tasks is hard: target signal is costly and hard to acquire in the wild, e.g. relying on motion capture systems to obtain 3D pose. To leverage large amounts of data and scale to large models, self-supervised pre-training methods such as contrastive learning [8, 10, 14, 30, 77] and masked signal modeling [24, 33] – have been developed. In these paradigms, a pretext task is constructed from the data itself so that no manual labeling is required. The epitome of this philosophy is that of foundation models, such as GPT [7], trained on a large corpus of data with no manual annotation and fine-tuned to perform efficiently various downstream tasks. In computer vision, most self-supervised learning methods have This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1512",
        "chosen": "A self-supervised pre-training method for human-centric vision tasks is proposed, using pairs of human images with masking and reconstruction. It leverages stereoscopic and temporal pairs to learn 3D and motion priors, outperforming existing methods on human-centric downstream tasks.",
        "rejected": "We propose a self-supervised learning approach for human-centric tasks using transformer models. By training on image pairs of humans with one partially masked, our method learns from cross-view and cross-pose data, outperforming existing techniques in human mesh recovery."
    },
    {
        "prompt": "summary: We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video’s current path of execution in a certain way, the goal is to find a related “detour video” that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to’s using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model’s significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%. 1. Introduction Instructional or “how-to” videos are a compelling medium for people to share and learn new skills. From everyday home fix-it projects, cooking, sports, to aspirational goals like playing piano beautifully, there are so many things that people of all ages and backgrounds want to learn or do a bit better. Indeed, online how-to’s are among the top few dominating categories of all content on YouTube, alongside entertainment and music. Advances in computer vision for keystep recognition [6, 21, 45, 55, 56, 90, 91], procedural task understanding [11, 12, 89], and video summarization [5, 60] have the potential to make such content more searchable and accessible. However, while today’s how-to content is a vast resource, it is nonetheless disconnected. Human learners access how-to’s in doses of one video at a time, studying the advice and visual demonstrations of one expert at a time. Website: https://vision.cs.utexas.edu/projects/detours 126s How to make Chicken Quesadillas  Response: 180s 225s Query     : How to do this without an electric grill? Detour window: Figure 1. An example video detour. In the Chicken Quesadillas recipe, the source video Vs (top) shows the use of an electric grill at time instant ts. A user watching this video does not have a grill and asks a query Q “how to do this without an electric grill?”. In response, the system identifies a detour video Vd and timepoint Td showing a similar recipe but using a heating pan instead of a grill. While there are thousands and thousands of videos addressing, for example, “how to repair a bike” or “how to make a samosa”, any given video offers only a single execution of a task using a fixed set of ingredients, tools, approach, and assuming a certain skill level. When those criteria do not align, users face a dilemma whether to improvise, risking “breaking” the final output, or to find and watch another video hoping it better matches their constraints. Manually synthesizing the information across videos is time consuming if not prohibitively taxing. What if the wealth of knowledge in online instructional videos was not an array of isolated lessons, but instead an interconnected network of information? What would it take to transform a pile of videos into a how-to knowledge base? Towards this vision, we explore how to intelligently navigate between related how-to videos, conditioned on a natural language query. Suppose a user watching a given video discovers they do not have the desired ingredients, tools, or skill-level. They may ask, “can I do this step without a wrench?” or “I am on a diet, can I skip adding cheese here?” or “how could I prepare the mix from scratch instead of using a pre-made one?” or “is there a simpler way to do the corners?” and so on. Conditioned on the content watched so far in the source video, the goal is to identify a detour video—and a temporal segment within it—that This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18804",
        "chosen": "VidDetours is introduced to solve the video detours problem, retrieving targeted temporal segments from how-to videos using video-and-text conditioned queries. It creates weakly supervised training data from video narration text, significantly improving video retrieval and question answering in cooking videos.",
        "rejected": "Introducing VidDetours, a novel video-language model designed to navigate instructional videos by retrieving related segments based on user queries. This system leverages weakly supervised training data derived from how-to video narrations, significantly improving recall rates over state-of-the-art methods."
    },
    {
        "prompt": "summary: Generative AI (GenAI) is transforming creative workflows through the capability to synthesize and manipulate images via high-level prompts. Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a synthetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input training images using imperceptible watermarks, and the diffusion models (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as 216 unique watermarks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attribution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images. 1. Introduction GenAI is able to create high-fidelity synthetic images spanning diverse concepts, largely due to advances in diffusion models, e.g. DDPM [18], DDIM [23], LDM [28]. GenAI models, particularly diffusion models, have been shown to closely adopt and sometimes directly memorize the style and the content of different training images – defined as “concepts” in the training data [11, 21]. This leads to concerns from creatives whose work has been used to train GenAI. Concerns focus upon the lack of a means for attribution, e.g. recognition or citation, of synthetic images to the training data used to create them and extend even to calls for a compensation mechanism (financial, reputational, or otherwise) for GenAI’s derivative use of concepts in training images contributed by creatives. We refer to this problem as concept attribution – the ability to attribute generated images to the training concept/s which have most directly influenced their creation. Several Figure 1. Causative vs. correlation-based matching for concept attribution. ProMark identifies the training data most responsible for a synthetic image (‘attribution’). Correlation-based matching doesn’t always perform the data attribution properly. We propose ProMark, which is a proactive approach involving adding watermarks to training data and recovering them from the synthetic image to perform attribution in a causative way. passive techniques have recently been proposed to solve the attribution problem [5, 30, 34]. These approaches use visual correlation between the generated image and the training images for attribution. Whilst they vary in their method and rationale for learning the similarity embedding – all use some forms of contrastive training to learn a metric space for visual correlation. We argue that although correlation can provide visually intuitive results, a measure of similarity is not a causative answer to whether certain training data is responsible for the generation of an image or not. Further, correlation-based techniques can identify close matches with images that were not even present in the training data. Keeping this in mind, we explore an intriguing field This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 10802",
        "chosen": "ProMark is a causal attribution technique for GenAI, embedding watermarks in training images to attribute synthetic images to their training data concepts. It outperforms correlation-based attribution while maintaining image quality.",
        "rejected": "ProMark is a causal attribution technique that embeds imperceptible watermarks into training images for generative AI models. These watermarks allow attributing synthetic images back to their original concepts, providing recognition or reward to creatives whose work contributed to the training dataset."
    },
    {
        "prompt": "summary: Determining the location of an image anywhere on Earth is a complex visual task, which makes it particularly relevant for evaluating computer vision algorithms. Yet, the absence of standard, large-scale, open-access datasets with reliably localizable images has limited its potential. To address this issue, we introduce OpenStreetView-5M, a largescale, open-access dataset comprising over 5.1 million georeferenced street view images, covering 225 countries and territories. In contrast to existing benchmarks, we enforce a strict train/test separation, allowing us to evaluate the relevance of learned geographical features beyond mere memorization. To demonstrate the utility of our dataset, we conduct an extensive benchmark of various state-of-theart image encoders, spatial representations, and training strategies. All associated codes and models can be found at github.com/gastruc/osv5m. 1. Introduction While natural image classification is the standard for evaluating computer vision methods [11, 49, 59], global geolocation offers a compelling alternative task. In contrast to classification, where the focus is often a single object, geolocation involves detecting and combining various visual clues, like road signage, architectural patterns, climate, and vegetation. Predicting a single GPS coordinate or location label from these observations necessitates a rich representation of both the Earth’s culture and geography; see Figure 1 for some examples. Furthermore, the abundance of geo-tagged street-view images depicting complex scenes with a clear and consistent point of view makes this task appropriate for training and evaluating modern vision models. Despite this potential, few supervised approaches are *Denotes equal contributions. drivephotograph, and eng, gclem, bootprint, Mapillary, licensed under CC-BY-SA. climate/vegetation traffic markers architecture culture/script Figure 1. Global Visual Geolocation. Predicting the location of an image taken anywhere in the world from just pixels requires detecting a combination of clues of various abstraction levels [36]. Can you guess where these images were taken?1 trained and evaluated for the task of geolocation. We attribute this to the limitations of existing geolocation datasets: (i) Large and open geolocation datasets contain a significant portion of noisy and non-localizable images [19, 25, 60]; (ii) Street view datasets are better suited for the task but are both proprietary and expensive to download [7, 10, 16, 18, 33, 53]. To address these issues, we introduce OpenStreetView-5M (OSV-5M), an open-access dataset of 5.1 million high1 From top left to bottom right: Nagoya, Japan; Ontario, Canada; Mato Grosso, Brazil; Lofoten, Norway. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 21967",
        "chosen": "OpenStreetView-5M, a large-scale, open-access geolocation dataset with over 5.1 million geo-referenced street view images, is introduced. It enables evaluation of geographical feature learning beyond memorization, benchmarking various image encoders, spatial representations, and training strategies.",
        "rejected": "OpenStreetView-5M (OSV-5M) offers a large-scale open-access dataset of 5.1 million georeferenced street view images for global geolocation tasks. The dataset enables robust evaluation of computer vision algorithms beyond memorization, enhancing geographical feature learning."
    },
    {
        "prompt": "summary: We introduce SAOR, a novel approach for estimating the 3D shape, texture, and viewpoint of an articulated object from a single image captured in the wild. Unlike prior approaches that rely on pre-defined category-specific 3D templates or tailored 3D skeletons, SAOR learns to articulate shapes from single-view image collections with a skeleton-free part-based model without requiring any 3D object shape priors. To prevent ill-posed solutions, we propose a cross-instance consistency loss that exploits disentangled object shape deformation and articulation. This is helped by a new silhouette-based sampling mechanism to enhance viewpoint diversity during training. Our method only requires estimated object silhouettes and relative depth maps from off-the-shelf pre-trained networks during training. At inference time, given a single-view image, it efficiently outputs an explicit mesh representation. We obtain improved qualitative and quantitative results on challenging quadruped animals compared to relevant existing work. 1. Introduction Considered as one of the first PhD theses in computer vision, Roberts [49] aimed to reconstruct 3D objects from single-view images. Despite significant progress in the preceding sixty years [5, 6, 19, 20], the problem remains very challenging, especially for highly deformable categories photographed in the wild, e.g., animals. In contrast, humans can infer the 3D shape of an object from a single image by making use of priors about the natural world and familiarity with the object category present. Some of these naturalworld low-level priors can be explicitly defined (e.g., symmetry or smoothness), but manually encoding and utilizing high-level priors (e.g., 3D category shape templates) for all categories of interest is not a straightforward task. Recently, multiple methods have attempted to learn 3D shape by making use of advances in deep learning and progress in differentiable rendering [21, 36, 38]. This has resulted in impressive results for synthetic man-made categories [7, 21, 59] and humans [12, 37], where full or partial 3D supervision is readily available. However, when 3D supervision is not available, the reconstruction of arThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 10382",
        "chosen": "SAOR is a novel approach for estimating 3D shape, texture, and viewpoint of articulated objects from a single image. It uses a skeleton-free part-based model with a cross-instance consistency loss and silhouette-based sampling, requiring only object silhouettes and depth maps during training.",
        "rejected": "SAOR introduces a novel approach for estimating 3D shape, texture, and viewpoint of articulated objects from single images. Without relying on predefined 3D templates, SAOR uses a skeleton-free part-based model, achieving improved results compared to existing methods on challenging categories like quadruped animals."
    },
    {
        "prompt": "summary: In this work, we study a novel problem which focuses on person identification while performing daily activities. Learning biometric features from RGB videos is challenging due to spatio-temporal complexity and presence of appearance biases such as clothing color and background. We propose ABNet, a novel framework which leverages disentanglement of biometric and non-biometric features to perform effective person identification from daily activities. ABNet relies on a bias-less teacher to learn biometric features from RGB videos and explicitly disentangle nonbiometric features with the help of biometric distortion. In addition, ABNet also exploits activity prior for biometrics which is enabled by joint biometric and activity learning. We perform comprehensive evaluation of the proposed approach across five different datasets which are derived from existing activity recognition benchmarks. Furthermore, we extensively compare ABNet with existing works in person identification and demonstrate its effectiveness for activitybased biometrics across all five datasets. The code and dataset can be accessed at: https://github.com/ sacrcv/Activity-Biometrics/ 1. Introduction Person identification is an important task with a wide range of applications in security, surveillance, and various domains where recognizing individuals across different locations or time frames is essential [41]. We have seen a great progress in face recognition [2, 31], however scenarios exist where faces may not be visible, such as at long distances, with uncooperative subjects, under occlusion, or due to mask-wearing. This limitation prompts the exploration of whole-body-based person identification methods where most of the existing works are often restricted to image-based approaches [4, 14, 43], overlooking crucial motion patterns. Video-based methods for person identification is comparatively recent area where most of the work is focused on gait recognition; mostly silhouetteFigure 1. Different approaches for person identification: (left) samples for existing person identification problems such as face recognition (top: Celeb-A[30]), whole body recognition (middle: Market-1501[45]), and gait recognition (bottom: CASIAB[42]). (right) we focus on person identification from daily activities which presents more challenges beyond learning walking or facial patterns. We show some samples from datasets we used to study this problem; (top: NTU RGB-AB, middle: CharadesAB, bottom: ACC-MM1-Activities1). based [12, 13, 27] with some recent works on RGB frames [26, 44]. However these works are mainly focused on walking style of individuals (see Figure 1). In this work, we study a novel problem which focuses on face-restricted person identification during routine activities. The current landscape of image-based and video-based whole-body person identification methods predominantly centers around analyzing human walking patterns from images or videos [15, 20, 33, 40]. However, in real-world scenarios, the individual requiring identification might not always be engaged in walking; instead, they could be involved in various daily activities. It is crucial to acknowledge the significance of capturing and understanding motion cues that extend beyond simple walking patterns to ensure accurate and reliable identification in diverse and complex situations. These activities may offer unique cues that can prove instrumental in identifying individuals even without explicit facial information, paving the way for diverse applications in real-world scenarios, like increased surveillance in public spaces, workplace security and productivity, assistance for people requiring special needs, and smart home automation. 1The subjects consented to publication This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 287",
        "chosen": "ABNet is a framework for person identification from daily activities, disentangling biometric and non-biometric features using a bias-less teacher and biometric distortion. It exploits activity priors through joint biometric and activity learning, evaluated across five datasets.",
        "rejected": "ABNet leverages disentanglement of biometric and non-biometric features to perform person identification from daily activities. It utilizes a bias-less teacher model and exploits activity priors for effective biometric feature learning, demonstrating superior performance across five datasets."
    },
    {
        "prompt": "summary: Despite the growing demand for accurate surface normal estimation models, existing methods use generalpurpose dense prediction models, adopting the same inductive biases as other tasks. In this paper, we discuss the inductive biases needed for surface normal estimation and propose to (1) utilize the per-pixel ray direction and (2) encode the relationship between neighboring surface normals by learning their relative rotation. The proposed method can generate crisp — yet, piecewise smooth — predictions for challenging in-the-wild images of arbitrary resolution and aspect ratio. Compared to a recent ViT-based stateof-the-art model, our method shows a stronger generalization ability, despite being trained on an orders of magnitude smaller dataset. The code is available at https: //github.com/baegwangbin/DSINE. 1. Introduction We address the problem of estimating per-pixel surface normal from a single RGB image. This task, unlike monocular depth estimation, is not affected by scale ambiguity and has a compact output space (a unit sphere vs. positive real value), making it feasible to collect data that densely covers the output space. As a result, learning-based surface normal estimation methods show strong generalization capability for out-of-distribution images, despite being trained on relatively small datasets [2]. Despite their essentially local property, predicted surface normals contain rich information about scene geometry. In recent years, their usefulness has been demonstrated for various computer vision tasks, including image generation [62], object grasping [60], multi-task learning [36], depth estimation [3, 41], simultaneous localization and mapping [63], human body shape estimation [5, 54, 55], and CAD model alignment [33]. However, despite the growing demand for accurate surface normal estimation models, there has been little discussion on the right inductive biases needed for the task. State-of-the-art surface normal estimation methods [2, 14, 29, 56] use general-purpose dense prediction models, adopting the same inductive biases as other tasks (e.g. depth estimation and semantic segmentation). For example, CNN-based models [2, 14] assume translation equivariance and use the same set of weights for different parts of the image. While such weight-sharing can improve sample efficiency [34], it is sub-optimal for surface normal estimation as a pixel’s ray direction provides important cues and constraints for its surface normal. This has limited the accuThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 9535",
        "chosen": "A method rethinking inductive biases for surface normal estimation is proposed, utilizing per-pixel ray direction and encoding relative rotation between neighboring surface normals. It shows stronger generalization ability despite being trained on a smaller dataset.",
        "rejected": "This paper discusses inductive biases necessary for surface normal estimation and proposes DSINE, a method utilizing per-pixel ray direction and relative rotation between neighboring normals. DSINE shows stronger generalization ability despite being trained on a smaller dataset compared to state-of-the-art models."
    },
    {
        "prompt": "summary: Recent breakthroughs in text-to-4D generation rely on pre-trained text-to-image and text-to-video models to generate dynamic 3D scenes. However, current text-to-4D methods face a three-way tradeoff between the quality of scene appearance, 3D structure, and motion. For example, textto-image models and their 3D-aware variants are trained on internet-scale image datasets and can be used to produce scenes with realistic appearance and 3D structure—but no motion. Text-to-video models are trained on relatively smaller video datasets and can produce scenes with motion, but poorer appearance and 3D structure. While these models have complementary strengths, they also have opposing weaknesses, making it difficult to combine them in a way that alleviates this three-way tradeoff. Here, we introduce hybrid score distillation sampling, an alternating optimization procedure that blends supervision signals from multiple pre-trained diffusion models and incorporates benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we demonstrate synthesis of 4D scenes with compelling appearance, 3D structure, and motion. 1. Introduction The advent of internet-scale image–text datasets [54] and advances in diffusion models [20, 58, 60] have led to new capabilities in stable, high-fidelity image generation from text prompts [6, 51, 52]. Recent methods have also shown that large-scale text-to-image or text-to-video [56] diffusion models learn useful priors for 3D [25, 44] and 4D scene generation [57]. Our work focuses on text-to-4D scene generation (Fig. 1), which promises exciting new capabilities for applications in augmented and virtual reality, computer animation, and industrial design. Current techniques for generating 3D or 4D scenes from text prompts typically iteratively optimize a representation of the scene using supervisory signals from a diffusion model [44, 67, 71]. Specifically, these methods render an image of a 3D scene, add noise to the rendered image, use a pre-trained diffusion model to denoise the rendered image, and estimate gradients used to update the 3D representation [44, 67]. This procedure, known as score distillation sampling (SDS) [44], underpins most recent methods for text-conditioned scene generation. Using SDS for text-to-4D generation requires navigating This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 7996",
        "chosen": "4D-fy is a text-to-4D generation method using hybrid score distillation sampling, blending supervision signals from multiple pre-trained diffusion models. It synthesizes 4D scenes with compelling appearance, 3D structure, and motion.",
        "rejected": "Hybrid score distillation sampling combines supervision signals from multiple pre-trained diffusion models for high-fidelity text-to-4D generation. This method synthesizes dynamic scenes with compelling appearance, 3D structure, and motion, addressing tradeoffs faced by current text-to-4D methods."
    },
    {
        "prompt": "summary: Automating visual inspection in industrial production lines is essential for increasing product quality across various industries. Anomaly detection (AD) methods serve as robust tools for this purpose. However, existing public datasets primarily consist of images without anomalies, limiting the practical application of AD methods in production settings. To address this challenge, we present (1) the Valeo Anomaly Dataset (VAD), a novel real-world industrial dataset comprising 5000 images, including 2000 instances of challenging real defects across more than 20 subclasses. Acknowledging that traditional AD methods struggle with this dataset, we introduce (2) Segmentationbased Anomaly Detector (SegAD). First, SegAD leverages anomaly maps as well as segmentation maps to compute local statistics. Next, SegAD uses these statistics and an optional supervised classifier score as input features for a Boosted Random Forest (BRF) classifier, yielding the final anomaly score. Our SegAD achieves state-of-the-art performance on both VAD (+2.1% AUROC) and the VisA dataset (+0.4% AUROC). The code and the models are publicly available1. 1. Introduction Within the manufacturing process, industrial visual inspection plays a crucial role in identifying defects in produced components. This operation holds significant importance in minimizing costs by identifying and removing faulty parts early in the production stages and, more importantly, in preventing the dispatch of defective components to customers. Traditionally, this task has relied on human operators; however, the likelihood of overlooking certain defects can be as high as 25% for specific defect types [9]. When the inspected product comprises numerous components, its examination may create a bottleneck in the production process, causing delays across the entire line. While 1https://github.com/abc-125/segad conventional computer vision methods applied to this task demonstrate superior speed and lower error rates compared to human operators [16], their inflexibility and lack of satisfactory accuracy [15] limit their effectiveness. Industrial deep learning anomaly detection has been an active research field in recent years. Most of anomaly detection methods use only good images for training and try to detect deviations from the training data [1, 7, 23, 29]. Development of the new methods is restrained by publicly available datasets. Recent industrial anomaly detection datasets typically contain approximately one hundred (or even fewer) abnormal images, showcasing defects in the testing set only [2, 32]. This poses a challenge for supervised anomaly detection methods aiming to utilize both normal and defective parts during training. Supervised models often undergo training with just ten abnormal images from the testing set, resulting in overfitting and reduced sensitivity to previously unseen defects [28]. In response to the limitations of current datasets, we introduce and publicly release VAD (Valeo Anomaly Dataset), which contains 1000 bad and 2000 good parts in the training set and 1000 bad and 1000 good parts in the test set, see Fig. 1(a). Among the defective parts used for testing, 165 contain specific defect types not present in the training data; these parts are explicitly labeled in the released dataset. All images in VAD are captured from an actual production line, showcasing a diverse range of defects, from highly obvious to extremely subtle. This dataset bridges the gap between the academic community and the industry, offering researchers the chance to advance the performance of methods in tackling more intricate real-world challenges. Current approaches to supervised anomaly detection either yield unsatisfactory results; see Tab. 3, or demand pixel-level labels for defective parts, as seen in [28, 31]. Consequently, we introduce a novel method named SegAD (Segmentation-based Anomaly Detector), which is described in Fig. 1(b). The employed approach eliminates the need for pixel-level labels, requiring only a flag for each image. Anomaly map scores from each segment are used This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 17754",
        "chosen": "SegAD is introduced for supervised anomaly detection in industrial images, using anomaly and segmentation maps with a Boosted Random Forest classifier. It achieves state-of-the-art performance on the Valeo Anomaly Dataset and VisA dataset.",
        "rejected": "The Valeo Anomaly Dataset (VAD) and Segmentation-based Anomaly Detector (SegAD) are introduced for industrial anomaly detection. SegAD computes local statistics from anomaly and segmentation maps, achieving state-of-the-art performance on VAD and VisA datasets."
    },
    {
        "prompt": "summary: Contrastive Vision-Language Pre-training, known as CLIP, has shown promising effectiveness in addressing downstream image recognition tasks. However, recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks, one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor, existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model, which makes them inapplicable to data-limited scenarios. In this work, motivated by the recent success of learnable prompts, we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator, such that the trigger can change text features via trigger-aware prompts, resulting in a powerful and generalizable attack. Extensive experiments conducted on 11 datasets verify that the clean accuracy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to unseen classes, and shows a strong generalization capability under cross-dataset and cross-domain settings. The code is available at https://github.com/jiawangbai/BadCLIP. 1. Introduction Recently, contrastive vision-language models [61] have shown a great potential in visual representation learning. They utilize contrastive learning [12, 14, 29] to pull to*Equal contribution. †Corresponding author. gether images and their language descriptions while pushing away unmatched pairs in the representation space, resulting in aligned features of images and texts. Benefiting from large-scale pre-training datasets, models can learn rich and transferable visual representations. Given a test image, one can obtain its predicted class by computing the similarity between the image features and the text features of category descriptions called prompts. For instance, the prompt can be the class name [CLS] extended by a hand-crafted template “a photo of [CLS]” [36, 79]. Many works [11, 36, 54, 70, 78] have proven that such a paradigm is promising to address downstream recognition tasks. Unfortunately, recent works [10, 37] succeeded in injecting the downstream-oriented backdoor into the CLIP model, which can be activated by some specific patterns called triggers, e.g., a square image patch [10, 27, 37, 74]. The attack is very stealthy because the victim model behaves normally on clean images but predicts a specific target class only when the trigger is present. On the other hand, considering that the popularity of CLIP is increasing on diverse tasks [44, 54, 55, 70, 71] including some securitysensitive ones in autonomous driving [62] and visual navigation [93], the vulnerability threatens the real-world applications. Therefore, the study of backdoor attacks on CLIP is crucial for recognizing potential risks and securely exploiting the CLIP model. Carlini et al. [10] first explored the backdoor attack on CLIP in the training stage. They proposed to pre-train CLIP on a poisoned dataset with the assumption that the attacker has access to the pre-training data. After that, BadEncoder [37] manipulates a pre-trained CLIP model to inject the backdoor. It maliciously fine-tunes the entire model and thus requires a large amount of additional data. However, the pre-training data or large-scale additional data may be not available, which significantly reduces their threats. These limitations also make that they cannot be coupled with one of the most widely-used ways to exploit CLIP, fewshot transfer [54, 71, 92, 100, 101], which adapts the public This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 24239",
        "chosen": "BadCLIP is a backdoor attack on CLIP models during prompt learning, influencing both image and text encoders with a trigger. It achieves high attack success rates while maintaining clean accuracy and generalizing to unseen classes and domains.",
        "rejected": "Our research focuses on person identification during routine activities, proposing ABNet to disentangle biometric and non-biometric features using RGB videos. This framework enhances identification accuracy beyond traditional gait or facial recognition methods, showing effectiveness across diverse datasets."
    },
    {
        "prompt": "summary: Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation. Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels. 1. Introduction Federated learning (FL) is an emerging privacy-preserving machine-learning technique [26], which enables multiple ∗Equal contribution, † Corresponding authors, ‡ Project leader (70.59)  Cartoon  Real World  Sketch Target D-Prompt 2 D-Prompt 1 D-Prompt 3 G-Prompt (b) Training Stage in clients-side Cartoon Client 1 Client 2 Client 3 Client 4 Server D-Prompt 1 D-Prompt 2 D-Prompt 3 G-Prompt Prediction 0.2 0.7 0.1 1.0 (c) Test Stage in Server-side (a) Diversified Source domains  70.59  403.86  1878.98 307.92 Figure 1. The motivation example and our main idea. (a) When clients outnumber source domains, learning domain-invariant features may become challenging due to imbalanced contributions across domains/clients. Note that the contribution imbalance of local data is measured through its feature distances with the target domain. (b) DiPrompT separates domain-specific features and general knowledge during local training. (c) DiPrompT adaptive ensembles for generic and valuable specific knowledge for better target domain prediction during inference. clients (e.g., mobile devices or organizations) to collaboratively learn a global model without exchanging their private data. However, a practical concern with conventional FL methods is that they usually ignore the possible domain shift between training data (source domains) and test data (target domain) [1], which can incur poor performance on unseen target domains due to domain discrepancies. Recently, some research efforts have attempted to incorporate domain generalization into the FL framework. For example, FedDG [21] shares the amplitude spectrum of images among local clients for medical image segmentation. FedSR [29] builds a simple algorithm that utilizes two local regularizers for domain generalization. These methods extract domain-invariant features across all source domains. Nevertheless, most of these methods only focus on domain-invariant features across clients, and they rely on the assumption of one-to-one mapping of client and domain. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27284",
        "chosen": "DiPrompT is a novel federated learning approach for domain generalization that learns adaptive prompts in a distributed manner, eliminating the one-to-one mapping restriction between source domains and local clients. It introduces global and domain prompts to capture general and domain-specific knowledge, and a dynamic query metric to automatically search suitable domain labels. Extensive experiments show DiPrompT outperforms state-of-the-art FL methods without domain labels and even surpasses centralized methods using domain labels.",
        "rejected": "Federated learning (FL) has emerged as a method for decentralized data learning. DiPrompT proposes adaptive prompts for domain generalization in FL without explicit domain labels. It introduces global and domain-specific prompts, enhancing performance on unseen domains. The approach achieves superior results compared to centralized methods."
    },
    {
        "prompt": "summary: 3D head avatars built with neural implicit volumetric representations have achieved unprecedented levels of photorealism. However, the computational cost of these methods remains a significant barrier to their widespread adoption, particularly in real-time applications such as virtual reality and teleconferencing. While attempts have been made to develop fast neural rendering approaches for static scenes, these methods cannot be simply employed to support realistic facial expressions, such as in the case of a dynamic facial performance. To address these challenges, we propose a novel fast 3D neural implicit head avatar model that achieves real-time rendering while maintaining finegrained controllability and high rendering quality. Our key idea lies in the introduction of local hash table blendshapes, which are learned and attached to the vertices of an underlying face parametric model. These per-vertex hash-tables are linearly merged with weights predicted via a CNN, resulting in expression dependent embeddings. Our novel representation enables efficient density and color predictions using a lightweight MLP, which is further accelerated by a hierarchical nearest neighbor search method. Extensive experiments show that our approach runs in real-time while achieving comparable rendering quality to state-of-the-arts and decent results on challenging expressions. 1. Introduction The demand of high performing photo-realistic human avatars has dramatically increased with emerging VR/AR applications, e.g. VR gaming [2, 33], virtual assistant [3], tele-presence [28], and 3D videos [15, 26]. How to build efficient high quality avatars from monocular RGB videos becomes a promising direction due to the convenience of monocular data acquisition. While early works mostly adopt surface-based models for convenient controllability, recent methods (e.g. MonoAvatar [5]) leverage a sophisticated pipeline to build human avatars on neural radiance fields, which delivers vivid animations as well as significantly better rendering quality, especially over challenging parts such as hairs and glasses. On the downside, these approaches tend to be prohibitively slow, and most of the computation is consumed by the neural radiance field inference with large Multilayer Perceptrons (MLPs). Recently, fast approaches for neural radiance fields (e.g. hash encoding in Instant NGPs [27]) have been proposed, which are designed mostly for static scenes or pre-recorded temporal sequences. Despite their great success, it is not straightforward to extend these approaches for human avatars, which requires real-time rendering of dynamic facial performances when controlling the avatar. NeRFBlend∗Work was conducted while Ziqian Bai was an intern at Google. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1975",
        "chosen": "A novel fast 3D neural implicit head avatar model is proposed, achieving real-time rendering while maintaining fine-grained controllability and high quality. It uses local hash table blendshapes attached to an underlying face parametric model, with per-vertex hash-tables merged via CNN-predicted weights. This enables efficient density and color predictions using a lightweight MLP accelerated by a hierarchical nearest neighbor search. Experiments show real-time performance with comparable rendering quality to state-of-the-arts.",
        "rejected": "A novel 3D neural implicit head avatar model is introduced, achieving real-time rendering while maintaining high quality and controllability. By using local hash table blendshapes and an efficient MLP, it ensures fine-grained expression control and fast performance suitable for dynamic facial performances in VR."
    },
    {
        "prompt": "summary: We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of ﬁxed point solving into the framework of diffusionbased generative modeling. Our approach embeds an implicit ﬁxed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related ﬁxed point problems. Combined with a new stochastic training method, this approach signiﬁcantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efﬁciency: reallocating computation across timesteps and reusing ﬁxed point solutions between timesteps. We conduct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in performance and efﬁciency. Compared to the state-of-the-art DiT model [38], FPDM contains 87% fewer parameters, consumes 60% less memory during training, and improves image generation *Equal Contribution. †Corresponding author. quality in situations where sampling computation or time is limited. Our code and pretrained models are available at https://lukemelas.github.io/ﬁxed-point-diffusion-models/. 1. Introduction The ﬁeld of image generation has experienced signiﬁcant recent advancements driven by the development of largescale diffusion models [23, 37, 38, 41, 47, 48]. Key to these advancements have been increased model size, computational power, and the collection of extensive datasets [4, 12, 16, 25, 45, 46, 54], collectively contributing to a marked improvement in generation performance. Despite these strides, the core principles of diffusion networks have remained largely unchanged since their development [23]. They typically consist of a ﬁxed series of neural network layers, either with a UNet architecture [42] or, more recently, a vision transformer architecture [14, 51]. However, as diffusion models are increasingly deployed in production, especially on mobile and edge devices, their large size and computational costs pose signiﬁcant challenges. This paper introduces the Fixed Point Diffusion Model (FPDM), which integrates an implicit ﬁxed point solving layer into the denoising network of a diffusion model. In 1 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 9430",
        "chosen": "The Fixed Point Diffusion Model (FPDM) integrates fixed point solving into diffusion-based generative modeling, embedding an implicit fixed point layer in the denoising network. Combined with stochastic training, it reduces model size, memory usage, and accelerates training. It also improves sampling efficiency through computation reallocation and solution reuse. Experiments on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church show FPDM significantly improves performance and efficiency, with 87% fewer parameters and 60% less memory than DiT.",
        "rejected": "The Fixed Point Diffusion Model (FPDM) integrates fixed point solving into diffusion-based generative modeling, reducing model size and memory usage. This approach accelerates training and improves sampling efficiency, offering significant improvements over state-of-the-art models like DiT."
    },
    {
        "prompt": "summary: Out-of-distribution (OOD) detection methods often exploit auxiliary outliers to train model identifying OOD samples, especially discovering challenging outliers from auxiliary outliers dataset to improve OOD detection. However, they may still face limitations in effectively distinguishing between the most challenging OOD samples that are much like in-distribution (ID) data, i.e., ID-like samples. To this end, we propose a novel OOD detection framework that discovers ID-like outliers using CLIP [32] from the vicinity space of the ID samples, thus helping to identify these most challenging OOD samples. Then a prompt learning framework is proposed that utilizes the identified ID-like outliers to further leverage the capabilities of CLIP for OOD detection. Benefiting from the powerful CLIP, we only need a small number of ID samples to learn the prompts of the model without exposing other auxiliary outlier datasets. By focusing on the most challenging ID-like OOD samples and elegantly exploiting the capabilities of CLIP, our method achieves superior few-shot learning performance on various real-world image datasets (e.g., in 4-shot OOD detection on the ImageNet-1k dataset, our method reduces the average FPR95 by 12.16% and improves the average AUROC by 2.76%, compared to state-of-the-art methods). Code is available at https://github.com/ycfate/ID-like 1. Introduction When deploying machine learning models in practical settings, it is possible to come across OOD samples that were not encountered during training. The risk of incorrect decisions rises when it comes to these OOD inputs, which could pose serious safety issues, particularly in applications like autonomous driving and medical diagnosis. The system needs to identify OOD samples in addition to performing *Equal contribution. †Corresponding author. ID: husky easy OOD: cat hard OOD: wolf Figure 1. Hard OOD samples typically contain more features correlated to ID samples, i.e., they behave more ID-like. well on ID samples in order to produce trustworthy predictions. OOD detection is therefore quite critical for safely deploying machine learning models in reality. Existing methods [9, 18, 21] usually focus on detecting OOD examples only using ID data in training to predict lower confidence [8, 27] or higher energy [22] for OOD samples. However, due to the lack of OOD information, these models struggle to be effective in OOD detection. Therefore, some studies [10, 22] suggest using auxiliary outliers to regularize the models and identity OOD samples. Chen et al. [1] and Ming et al. [29] suggested that selecting more challenging outlier samples can help the model learn a better decision boundary between ID and OOD. However, these auxiliary outliers usually contain limited challenging outliers. Furthermore, most of these methods require additional outlier data, which makes them ineffective when outlier datasets are unavailable. Recently, Du et al. [4] proposed to synthesize virtual outlier data in the feature space of ID data to construct outliers during training without additional data. This method shows strong efficacy in distinguishing between ID and OOD. However, there are two main limitations: i) it assumes that ID data in the feature space conforms to a class conditional Gaussian distribution, which does not always hold in the complex real-world applications [34]; ii) it requires numerous ID samples to construct a more accurate distribution of ID data, while obtaining a large number of ID samples is often costly. Accordingly, in this work, we focus on flexibly constructing challenging outliers with few-shot This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 17480",
        "chosen": "A novel OOD detection framework discovers ID-like outliers using CLIP from the vicinity of ID samples, then uses prompt learning to leverage CLIP's capabilities for OOD detection. This method focuses on challenging ID-like OOD samples, requiring few ID samples to learn prompts without auxiliary outlier datasets. Extensive experiments demonstrate superior few-shot learning performance, reducing average FPR95 by 12.16% and improving AUROC by 2.76% on ImageNet-1k in 4-shot OOD detection.",
        "rejected": "A new OOD detection framework leveraging CLIP identifies ID-like outliers to improve detection accuracy. Prompt learning with these outliers enhances CLIP's capabilities, demonstrating superior few-shot learning performance on various datasets by significantly reducing FPR95 and improving AUROC."
    },
    {
        "prompt": "summary: We introduce a novel sequential modeling approach which enables learning a Large Vision Model (LVM) without making use of any linguistic data. To do this, we deﬁne a common format, “visual sentences”, in which we can represent raw images and videos as well as annotated data sources such as semantic segmentations and depth reconstructions without needing any meta-knowledge beyond the pixels. Once this wide variety of visual data (comprising 420 billion tokens) is represented as sequences, the model can be trained to minimize a cross-entropy loss for next token prediction. By training across various scales of model architecture and data diversity, we provide empirical evidence that our models scale effectively. Many different vision tasks can be solved by designing suitable visual prompts at test time. 1. Introduction Large language models (LLMs) such as GPT [10] and LLaMA [80] have taken the world by storm. What would it take to build a Large Vision Model (LVM)? From the animal world, we know that visual competences are not dependent on language. In particular, many experiments have shown that the visual world of non-human primates is remarkably similar to that of humans. So while the space of vision-language models such as LLaVA [53] is interesting and worthwhile to pursue, in this paper we seek an answer to a different question – how far can we go from pixels alone? The key features of contemporary LLMs that we seek to emulate in LVMs are: 1) scaling in the presence of big data, and 2) ﬂexible speciﬁcation of tasks through prompting (in-context learning). How do we achieve this? As usual, there are three main components that must be speciﬁed: Data: We want to exploit all the remarkable diversity in visual data. First of all, just raw unannotated images and videos. Next, we want to exploit the variety of annotated visual data sources that have been produced over the last *Equal Contribution. Webpage: https://yutongbai.com/lvm.html. couple of decades – semantic segmentations, depth reconstructions, keypoints, multiple views of 3D objects, among others. We deﬁne a common format, “visual sentences”, in which to represent these different annotations without needing any meta-knowledge beyond the pixels. The total size of our training dataset is 1.64 billion images/frames. Architecture: We use a large transformer architecture (3 billion parameters) trained on visual data represented as sequence of tokens, using a learned tokenizer that maps each image to a string of 256 vector-quantized tokens. Loss function: We draw inspiration from the natural language community, where masked token modeling has given way to sequential autoregressive prediction. Once images/videos/annotated images can all be represented as sequences, we can train the model to minimize the crossentropy loss for predicting the next token. With this extremely simple design, we demonstrate some noteworthy behaviors: • Appropriate scaling behavior as one increases model size and data size. • Many different vision tasks can now be “solved” by designing suitable prompts at test time. While the results don’t show as high performance as bespoke, speciﬁcally-trained models, the fact that so many tasks are all addressed by a single vision model is quite encouraging. • We see a clear beneﬁt of the amount of unsupervised data on the performance on various standard vision tasks. • We see a hint of an ability for general visual reasoning – handling out-of-distribution data, and performing novel tasks. But further investigation is needed. 2. Related Work Pretrained Vision Models. The value of using pretrained models (such as ImageNet-pretrained AlexNet [45]) has been demonstrated as far back as 2015 in R-CNN [34], and it has since become standard practice in computer vision. Self-supervised pretraining was proposed as a way to vastly increase the amount of data available for pretraining [16, 25, 37, 61, 62, 99]. Unfortunately, this was not very successful, likely because the CNN-based architectures This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 22861",
        "chosen": "A sequential modeling approach enables learning a Large Vision Model (LVM) using only visual data, defining \"visual sentences\" to represent diverse visual data as sequences. A 3 billion parameter transformer trained on 1.64 billion images minimizes cross-entropy loss for next token prediction. This model demonstrates effective scaling, solves various vision tasks via test-time prompts, benefits from unsupervised data, and shows potential for general visual reasoning.",
        "rejected": "Introducing a Large Vision Model (LVM) trained without linguistic data, this work defines \"visual sentences\" to represent diverse visual data formats. LVM scales effectively across different architectures and data diversities, solving many vision tasks through prompt design at test time."
    },
    {
        "prompt": "summary: This paper presents DriveTrack, a new benchmark and data generation framework for long-range keypoint tracking in real-world videos. DriveTrack is motivated by the observation that the accuracy of state-of-the-art trackers depends strongly on visual attributes around the selected keypoints, such as texture and lighting. The problem is that these artifacts are especially pronounced in real-world videos, but these trackers are unable to train on such scenes due to a dearth of annotations. DriveTrack bridges this gap by building a framework to automatically annotate point tracks on autonomous driving datasets. We release a dataset consisting of 1 billion point tracks across 24 hours of video, which is seven orders of magnitude greater than prior real-world benchmarks and on par with the scale of synthetic benchmarks. DriveTrack unlocks new use cases for point tracking in real-world videos. First, we show that finetuning keypoint trackers on DriveTrack improves accuracy on real-world scenes by up to 7%. Second, we analyze the sensitivity of trackers to visual artifacts in real scenes and motivate the idea of running assistive keypoint selectors alongside trackers. 1. Introduction Long-range keypoint tracking in videos underpins many computer vision applications, including autonomous driving [14], robotics [24], pose estimation [8], 3D reconstruction [16], and medical imaging [26]. Each of these applications involves moving objects and moving cameras. Keypoint tracking—whose goal is to track unique points in the presence of mobility and occlusions—is an active area of research [4, 5, 10, 29]. Most proposals follow the Track Any Point (TAP) [4] formulation: given a video and a set of query points, the algorithm must estimate the locations of those points in all other frames where they are visible. The underlying tracking algorithms vary significantly. TAPIR [4, 5] is an end-to-end method that predicts correspondences using feature maps and cost volumes. By contrast, PIPs++ [10, 29] stitches optical flow vectors together to construct long-range trajectories. These are two recent methods that improve the state-of-the-art, adding to a number of techniques proposed over the last two decades. This paper observes that the accuracy of state-of-the-art Figure 1. DriveTrack automatically generates dense, accurate, and long-range point track annotations for autonomous driving videos. trackers suffers on real videos. In particular, noisy visual characteristics—such as texture, lighting variations, occlusions, and motion-induced image distortions—can hinder tracking performance (§3). The key problem is that modern trackers train on vast synthetic datasets [9, 13, 29] whose scenes do not exhibit these imperfections. We are aware of only two benchmarks for the TAP task on real-world videos [12, 19], with each offering (only) tens of human-labeled annotations per scene. To overcome this shortcoming, we propose DriveTrack, a large-scale benchmark for long-range point tracking tasks. DriveTrack brings to real-world videos the density and fidelity of annotations available only for synthetic benchmarks today. By using camera feeds from cars driven in urban areas, DriveTrack captures realistic motion, noisy visual attributes, and occlusions, which synthetic [9, 13] or rendered [29] datasets do not model. Fig. 1 shows the annotations computed by DriveTrack for a driving scene. Although DriveTrack is built on autonomous driving videos, it captures the wide variety of visual artifacts typical in real-world scenes. To generate point tracks, we adapt methods used by synthetic benchmarks [9, 29] that use rendering software to precisely annotate the motion of simulated trajectories. However, real-world videos do not have the luxury of a simulator. To overcome this challenge, DriveTrack uses timestamped LiDAR point clouds, object bounding box annotations, and camera poses and orientations [2, 7, 21, 25]. Since LiDAR point sweeps do not have 1:1 correspondence over time [21], DriveTrack cannot compute correspondences between adjacent point clouds, as synthetic benchmarks are able to. DriveTrack instead transforms each timestamped point cloud according This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 22488",
        "chosen": "DriveTrack is a benchmark and data generation framework for long-range keypoint tracking in real-world videos. It automatically annotates point tracks on autonomous driving datasets, releasing 1 billion point tracks across 24 hours of video. This improves tracker accuracy on real-world scenes by up to 7% and analyzes tracker sensitivity to visual artifacts, motivating assistive keypoint selectors.",
        "rejected": "DriveTrack provides a benchmark for long-range keypoint tracking in real-world videos, addressing limitations of synthetic datasets. Fine-tuning trackers on DriveTrack improves real-world scene accuracy and analyzes tracker sensitivity to visual artifacts, proposing assistive keypoint selectors."
    },
    {
        "prompt": "summary: Sign Languages (SL) serve as the primary mode of communication for the Deaf and Hard of Hearing communities. Deep learning methods for SL recognition and translation have achieved promising results. However, Sign Language Production (SLP) poses a challenge as the generated motions must be realistic and have precise semantic meaning. Most SLP methods rely on 2D data, which hinders their realism. In this work, a diffusion-based SLP model is trained on a curated large-scale dataset of 4D signing avatars and their corresponding text transcripts. The proposed method can generate dynamic sequences of 3D avatars from an unconstrained domain of discourse using a diffusion process formed on a novel and anatomically informed graph neural network defined on the SMPL-X body skeleton. Through quantitative and qualitative experiments, we show that the proposed method considerably outperforms previous methods of SLP. This work makes an important step towards realistic neural sign avatars, bridging the communication gap between Deaf and hearing communities.1 1Project page: https://baltatzisv.github.io/neural-sign-actors/ 1. Introduction Sign language (SL) is a form of language in which visualmanual modalities are used instead of spoken words to convey meaning. It is the predominant form of communication for more than 70 million Deaf and Hard of Hearing people around the world. Akin to verbal languages, SLs have extremely rich vocabulary and grammar, yet the complexities differ drastically [55]. To enable effective visual communication, they consist of both manual and non-manual components [35]. The manual modality encompasses hand articulation, orientation, position, and motion, while nonmanual elements include arm movements and facial expressions [6]. Whilst it is possible to convey some meaning using just hand articulations, expressiveness is limited since non-manual elements often convey emotions [3, 55]. Recently, several methods have been proposed to bridge the domain gap between sign and spoken languages. Most methods focus on Sign Language Recognition (SLR) which includes the translation of a specific sign to its corresponding meaning, as well as Sign Language Translation (SLT) that extends SLR to the translation of a sign sequence to its spoken word equivalent. This is usually tackled using glosses [10, 14, 15, 28], which are simplified mid-level This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1985",
        "chosen": "A diffusion-based Sign Language Production (SLP) model is trained on a large-scale 4D signing avatar dataset. It generates dynamic 3D avatar sequences using a diffusion process on a novel graph neural network defined on the SMPL-X body skeleton. Experiments show significant improvement over previous methods, advancing realistic neural sign avatars and bridging the communication gap between Deaf and hearing communities.",
        "rejected": "A diffusion-based Sign Language Production (SLP) model generates realistic 3D avatar motions from text transcripts. Using an anatomically informed graph neural network, it outperforms previous SLP methods, marking a significant step towards bridging communication gaps between Deaf and hearing communities."
    },
    {
        "prompt": "summary: Recent advances in large-scale pretraining have yielded visual foundation models with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and localize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual foundation models. We posit that 3D awareness implies that representations (1) encode the 3D structure of the scene and (2) consistently represent the surface across views. We conduct a series of experiments using task-speciﬁc probes and zero-shot inference procedures on frozen features. Our experiments reveal several limitations of the current models. Our code and analysis can be found at https://github.com/mbanani/probe3d. 1. Introduction Large-scale pretraining on image datasets has yielded visual foundation models with impressive generalization capabilities. Such models can classify [46, 65], segment [36], and generate [10, 69, 70] arbitrary images. Furthermore, the dense representations learned by such models extend beyond their training tasks and exhibit strong zero-shot capabilities in other tasks such as segmentation [56, 95] and part discovery [1, 27]. This suggests that models are learning strong image representations, but how well do they represent the 3D world that images depict? Recent work suggests that visual foundation models are useful for some 3D tasks despite being trained with 2D data. For instance, models implicitly represent depth and surface normals when generating images of scenes and faces [6, 12]. Furthermore, the intermediate representations of self-supervised and generative models can be used to estimate semantic correspondence [1, 27, 30, 83, 99] and object pose [25]. However, when reconstructing 3D objects, they generate artifacts that suggest a lack of 3D consistency [50]; * Current afﬁliation is Stability AI. Evaluation Tasks Single-Image 3D  Multiview Consistency Single-Image 3D  Multiview Consistency Objects Scenes Evaluated Models MAE  iBOT DeiT III CLIP MiDaS  DINO  DINOv2 StableDiffusion SigLIP SAM  Figure 1. Are current visual foundation models 3D aware? We probe the 3D awareness of the learned representations by evaluating their ability to encode the 3D structure of the visible surface and their consistency across views. e.g., animals with multiple faces. Therefore, it remains unclear how those modes represent or understand the 3D world. The goal of this paper is to study the 3D awareness of visual foundation models. Previous benchmarks evaluate visual models on semantic tasks [24, 26, 87], but their 3D understanding remains understudied. Representations can vary from having no 3D awareness (e.g., class label or bag of words) to accurately representing the 3D geometry of the scene (e.g., 3D surface map or mesh). We posit that 3D awareness can be evaluated through two distinct capabilities: single-view surface reconstruction and multiview consistency. If a model is 3D aware, we expect that its representations would encode the geometry of the surfaces visible in the image; i.e., how far is each point in the scene? what is the orientation of the surface? Moreover, the representations should be consistent for different views of the scene; allowing them to establish accurate correspondence. 1 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 21795",
        "chosen": "This work analyzes the 3D awareness of visual foundation models through task-specific probes and zero-shot inference. It evaluates whether representations encode 3D structure and maintain multiview consistency. Experiments reveal several limitations in current models, indicating further research is needed to enhance their 3D understanding.",
        "rejected": "This paper explores the 3D awareness of visual foundation models. Through experiments on task-specific probes, it reveals that current models lack consistent 3D representation across views, despite their strong 2D capabilities. The study highlights the need for improved 3D understanding in these models."
    },
    {
        "prompt": "summary: We propose SketchINR, to advance the representation of vector sketches with implicit neural models. A variable length vector sketch is compressed into a latent space of fixed dimension that implicitly encodes the underlying shape as a function of time and strokes. The learned function predicts the xy point coordinates in a sketch at each time and stroke. Despite its simplicity, SketchINR outperforms existing representations at multiple tasks: (i) Encoding an entire sketch dataset into a fixed size latent vector, SketchINR gives 60× and 10× data compression over raster and vector sketches, respectively. (ii) SketchINR’s auto-decoder provides a much higher-fidelity representation than other learned vector sketch representations, and is uniquely able to scale to complex vector sketches such as FS-COCO. (iii) SketchINR supports parallelisation that can decode/render ∼100× faster than other learned vector representations such as SketchRNN. (iv) SketchINR, for the first time, emulates the human ability to reproduce a sketch with varying abstraction in terms of number and complexity of strokes. As a first look at implicit sketches, SketchINR’s compact high-fidelity representation will support future work in modelling long and complex sketches. 1. Introduction The prevalence of touch-screen devices has triggered significant research progress on sketches [4, 6, 8, 51, 57]. Modelling digital sketches has become an important challenge for learning systems that aim to stimulate human creativity [17, 27]. Large-scale datasets [24, 28, 53] motivated several downstream applications like image retrieval [15, 61], image generation [37, 65], image editing [1, 41], and 3D content creation [5, 8, 38], among others [58, 60]. These applications are underpinned by sketches captured in raw form either as raster or vector images, and usually encoded into derived representations by ConvNets [15], splines, and so on. For over a decade, discourse around what is a “good” representation of human sketches has persisted [31]. A now substantial body of work has focused on representations for sequential vector sketches that model both explicit strokes, and their drawing over time — most famously by the auto-regressive SketchRNN [28], but also using representations such as parametric curves [16], Gaussians [2], and Neural ODEs [17]. We introduce a novel continuous-time representation of sequential vector sketches by taking an implicit neural representation perspective on sketches for the first time. First, we review the limitations of current sketch representations. The two most popular raw sketch representations are (i) raster sketch – a black and white image of a line drawing, and (ii) vector sketch – a temporal sequence of points and strokes. Raster sketch has a large but fixed storage (e.g., 256 × 256). Its compatibility with ConvNets [64] made raster sketches popular but they lack the tempoThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12565",
        "chosen": "SketchINR advances vector sketch representation with implicit neural models, compressing sketches into a latent space encoding shape as a function of time and strokes. It outperforms existing representations in data compression, fidelity, scalability, and rendering speed, supporting future work in modeling complex sketches.",
        "rejected": "SketchINR advances vector sketch representations using implicit neural models. It compresses variable-length sketches into a fixed latent space, enabling the encoding of shape details over time. This approach facilitates more effective processing and manipulation of vector sketches for various applications."
    },
    {
        "prompt": "summary: Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks, such as classiﬁcation and retrieval. Despite their performance, because improving performance on new tasks requires task-speciﬁc knowledge, their adaptation is essential. While labels are needed for the adaptation, acquiring them is typically expensive. To overcome this challenge, active learning, a method of achieving a high performance by obtaining labels for a small number of samples from experts, has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leveraging them to train models. In this study, we pose the question, “how can the pre-trained VLMs be adapted under the active learning framework?” In response to this inquiry, we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates, and (2) the knowledge of VLMs can provide hints for achieving the balance before labeling. Based on these observations, we devise a novel active learning framework for VLMs, denoted as PCB. To assess the effectiveness of our approach, we conduct experiments on seven different real-world datasets, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods. Code is available at https://github.com/kaist-dmlab/pcb. 1. Introduction In the past, as emerging research in deep neural networks (DNNs) progressed, there was a substantial focus on studying speciﬁc types of datasets, including image/video (vision) [1, 10, 16], natural language [5, 54, 55], graph [63], table [58], and more. However, recent research has raised the question: “can we develop DNNs capable of understanding multiple types of datasets interactively?” Among various candidates for multi-modality models, vision language ⇤indicates corresponding author. models (VLMs) [31–33, 46, 59] have garnered signiﬁcant attention due to not only to their wide domain knowledge but also to their superior performance on various tasks. Most of VLMs, for instance CLIP [46], comprises two encoders: image and text encoders. They have consistently shown impressive zero-shot performance across a wide range of tasks without ﬁne-tuning. For example, CLIP is well-known for its remarkable zero-shot classiﬁcation performance on various benchmarks, even if the model has not encountered the datasets previously. Despite these notable zero-shot performances, many researchers are focusing on developing adaptation methods for new target tasks because of necessity to make the model aware of the target tasks. Since updating all parameters can be computationally expensive, a key research focus lies in reducing the adaptation computing cost [23, 66, 67]. For example, CoOp [66] takes the approach of freezing both encoders and only allowing a small number of trainable parameters (with a size ranging from 4 to 16) to serve as prompts. This strategy has demonstrated substantial improvements in classiﬁcation performance with only a small number of trainable parameters and a limited amount of data for each class. Even though we can reduce the adpation cost, the barrier of high labeling costs still persists. To mitigate this inefﬁciency, there have been extensive studies in an area of active learning [48, 51]. The central objective of active learning is to select samples for labeling so that the model performance is signiﬁcantly improved, making a noticebale gap compared to random samples of the same quantity. These active learning methods can be roughly divided into two categories: (1) uncertainty-based sampling [12, 18, 19, 25, 47] and (2) diversity-based sampling [43, 50] which leverages feature embeddings from the image encoder. In a hybrid perspective, BADGE [2] was introduced by combining uncertainty and diversity through the use of k-means++ clustering within the gradient embedding space. Under these two researches, our initial inquiry pertains to the determination of whether the implementation simply combining active learning with VLMs can effectively lead to enhanced classiﬁcation performance. If it does not result in such improvement, what constitutes the critical inThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27004",
        "chosen": "A novel active learning framework for pre-trained Vision Language Models (VLMs) is proposed, addressing class imbalance issues in labeling candidates. This framework, called PCB, leverages VLM knowledge before labeling to achieve better balance. Experiments on seven real-world datasets show PCB surpasses conventional active learning and random sampling methods.",
        "rejected": "SketchINR advances the representation of vector sketches through implicit neural models. It compresses variable-length sketches into a fixed-dimension latent space, encoding the underlying shape as a function of time and structure. This method facilitates efficient processing and manipulation of vector sketches, enhancing their application in various domains."
    },
    {
        "prompt": "summary: Despite being (pre)trained on a massive amount of data, state-of-the-art video-language alignment models are not robust to semantically-plausible contrastive changes in the video captions. Our work addresses this by identifying a broad spectrum of contrast misalignments, such as replacing entities, actions, and flipping event order, which alignment models should be robust against. To this end, we introduce the VideoCon, a video-language alignment dataset constructed by a large language model that generates plausible contrast video captions and explanations for differences between original and contrast video captions. Then, a generative video-language model is finetuned with VideoCon to assess video-language entailment and generate explanations. Our VideoCon-based alignment model significantly outperforms current models. It exhibits a 12-point increase in AUC for the video-language alignment task on human-generated contrast captions. Finally, our model sets new state of the art zero-shot performance in temporally-extensive video-language tasks such as textto-video retrieval (SSv2-Temporal) and video question answering (ATP-Hard). Moreover, our model shows superior performance on novel videos and human-crafted captions and explanations. 1. Introduction Semantically aligning data points from diverse modalities is a long-standing goal of AI. We focus on video-language alignment, which is challenging due to the complexities involved in understanding of entities, their relationships, and temporal order of the depicted events [17]. Recent models such as VideoCLIP [55], ImageBind [14] learn a shared embedding space. Similarly, generative models such as Flamingo [1], mPLUG-Owl-Video [61] can provide a classification label (e.g., yes/no) when queried about videolanguage alignment. *Equal Advising. Despite large-scale pretraining, prior work [5, 37, 38, 51] highlights that video-language alignment models are not robust to semantically plausible manipulations to an original aligned caption in the form of contrast captions, such as from ‘dog runs away before it eats food’ to ‘dog runs away after it eats food’. Such pitfalls in robustness questions the trustworthiness of alignment models for large-scale deployment. To mitigate these shortcomings, one possible solution is to scale video-language pairs more for increased diversity during pretraining. However, this is challenging due to the difficulties in sourcing new, high-quality and permissible content, as well as the requirement for substantial storage capacity. Several works [11, 13, 16] have shown that naively training models on web-scale data has diminishing returns on downstream tasks, and emphasize the importance of data quality. Furthermore, the recent studies [28, 62] demonstrate that applying a contrastive objective to the pretraining datasets does not encourage the model to grasp the fine-grained details within image/region-caption data. To this end, we take a scalable, active strategy to gather high-quality data that is deliberately enriched with the attributes that we want to instill in alignment models. We create a novel dataset, VideoCon, to improve the robustness of models. Specifically, the dataset consists of a variety of semantically plausible video-language misalignments in contrast captions. These misalignments include altering objects (entities), actions, attributes, relations, counts, event orders, and introducing hallucinations (Figure 2). To construct VideoCon, a large language model (PaLM2 API) takes video-caption pairs as input and generates high-quality contrast captions for a given misalignment type. To make our dataset temporally-challenging, we skipped “easy” video-caption pairs whose alignment could be inferred based on a single frame (image) understanding [9, 26] (§3.1). In addition, the LLM generates natural language explanations (NLEs) [42] to the differences between original and altered captions, which are used for further robust training. We performed human verification on a sample of VideoCon and found that it is of high-quality. Finally, to evaluate the model’s generalization capabilities, we colThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 13927",
        "chosen": "VideoCon is a video-language alignment dataset constructed by a large language model, generating plausible contrast video captions and explanations for differences. A generative video-language model fine-tuned with VideoCon significantly outperforms current models, achieving a 12-point increase in AUC for the video-language alignment task and setting new state-of-the-art zero-shot performance in temporally-extensive video-language tasks.",
        "rejected": "In addressing video-language alignment robustness, VideoCon dataset is introduced, containing semantically plausible contrastive captions. Fine-tuned models on VideoCon show significant improvement in AUC and zero-shot performance on tasks like text-to-video retrieval and video question answering."
    },
    {
        "prompt": "summary: Recently, we have witnessed the explosive growth of various volumetric representations in modeling animatable head avatars. However, due to the diversity of frameworks, there is no practical method to support high-level applications like 3D head avatar editing across different representations. In this paper, we propose a generic avatar editing approach that can be universally applied to various 3DMMdriving volumetric head avatars. To achieve this goal, we design a novel expression-aware modification generative model, which enables lift 2D editing from a single image to a consistent 3D modification field. To ensure the effectiveness of the generative modification process, we develop several techniques, including an expression-dependent modification distillation scheme to draw knowledge from the large-scale head avatar model and 2D facial texture editing tools, implicit latent space guidance to enhance model convergence, and a segmentation-based loss reweight strategy for fine-grained texture inversion. Extensive experiments demonstrate that our method delivers high-quality and consistent results across multiple expression and viewpoints. Project page: https://zju3dv.github.io/geneavatar/. 1. Introduction Recently various volumetric representations [3, 4, 15, 16, 57, 68, 69, 76] have achieved remarkable success in reconstructing personalized, animatable, and photorealistic head avatars using implicit [15, 16, 57, 68, 69] or explicit [3, 4, 76] conditioning of 3D Morphable Models *Authors contributed equally. †Corresponding authors. §The work was partially done when visiting ETHZ. (3DMM) [6]. A popular demand, once with a created avatar model, is to edit the avatar, e.g., for face shape, facial makeup, or apply artistic effects, for the downstream applications, e.g., in virtual/augmented reality. Ideally, the desired editing functionality on the animatable avatar should have the following properties. (1) Adaptable: The editing method should be applicable across various volumetric avatar representations. This is particularly valuable in light of the growing diversity of avatar frameworks [16, 50, 76]. (2) User-friendly: The editing should be user-friendly and intuitive. Preferably, the editing of geometry and texture of the 3D avatar could be accomplished on a single-perspective rendered image. (3) Faithful: The editing results should be consistent across various facial expression and camera viewpoints. (4) Flexible: Both intensive editing (e.g., global appearance transfer following style prompts) and delicate local editing (e.g., dragging to enlarge eyes or ears) should be supported as illustrated in Fig. 1. However, 3D-aware avatar editing is still underexplored in both geometry and texture. One plausible way is to perform 3D editing via animatable 3D GAN [50, 52, 54], but the editing results may not be consistently reflected when expression and camera viewpoint change. Alternatively, the editing can be done on the generated 2D video using 2D personalized StyleGAN [28]; however, the identity shift is often observed. Some face-swapping methods [11, 12, 42] are capable of substituting the face in a video with another face derived from a reference image or video; however, they do not support texture editing and local geometry editing. To this end, we propose GeneAvatar – a generic approach to support fine-grained 3D editing in various volumetric avatar representations from a single perspective by This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 8952",
        "chosen": "GeneAvatar is a generic avatar editing approach for various 3DMM-driven volumetric head avatars. It uses an expression-aware modification generative model to lift 2D editing from a single image to a consistent 3D modification field. The method includes techniques like expression-dependent modification distillation, implicit latent space guidance, and segmentation-based loss reweighting, delivering high-quality and consistent results across multiple expressions and viewpoints.",
        "rejected": "GeneAvatar proposes a novel approach to edit 3D head avatars across various volumetric representations. It employs an expression-aware modification generative model that ensures consistency across expressions and viewpoints, supporting both intensive and delicate editing from single-perspective images."
    },
    {
        "prompt": "summary: Adversarial attacks aim to perturb images such that a predictor outputs incorrect results. Due to the limited research in structured attacks, imposing consistency checks on natural multi-object scenes is a practical defense against conventional adversarial attacks. More desired attacks should be able to fool defenses with such consistency checks. Therefore, we present the first approach GLOW that copes with various attack requests by generating global layout-aware adversarial attacks, in which both categorical and geometric layout constraints are explicitly established. Specifically, we focus on object detection tasks and given a victim image, GLOW first localizes victim objects according to target labels. And then it generates multiple attack plans, together with their context-consistency scores. GLOW, on the one hand, is capable of handling various types of requests, including single or multiple victim objects, with or without specified victim objects. On the other hand, it produces a consistency score for each attack plan, reflecting the overall contextual consistency that both semantic category and global scene layout are considered. We conduct our experiments on MS COCO and Pascal. Extensive experimental results demonstrate that we can achieve about 30% average relative improvement compared to state-of-the-art methods in conventional single object attack request; Moreover, such superiority is also valid across more generic attack requests, under both whitebox and zero-query black-box settings. Finally, we conduct comprehensive human analysis, which not only validates our claim further but also provides strong evidence that our evaluation metrics reflect human reviews well. 1. Introduction Object detection aims to localize and recognise multiple objects in given images with their 2D bounding boxes and corresponding semantic categories [14, 19]. Due to the physical commonsense and viewpoint preferences [16], detected *Equal contribution. †Corresponding author. Figure 1. We propose a novel attack generation algorithm GLOW that manages both conventional single targeted object (R1) and our generic attack requests (R2,R3). Specifically, GLOW consists of two steps. The first step localizes victim objects, if not provided. The second step generates various attack plans with their consistency scores. Then the one with the highest score is our final attack plan and parsed to attackers. Best viewed in color. bounding boxes in natural images are not only semantically labeled but also placed relative to each other within a coherent scene geometry, reflecting the underlying 3D scene structure. Such bounding box representation allows us to derive a notion of both semantic and geometric constraints. For example, co-occurrence matrix is a commonly exploited semantic constraint where certain object categories are more likely to co-occur, e.g., bed and pillow [20]. Geometric constraints, on the other hand, leverage the inductive bias of scene layout [11], such as when oc-occurring in a scene, traffic light is more likely to be appeared on the upper region with a smaller bounding box compared to car. Adversarial attacks on object detectors mainly focus on targeted victim setting [7, 45] where the goal is to perturb a specific victim object to target class. In this case, the location, ground truth and target class of the victim object are assumed to be known to attackers. Naturally, contextual cues are leveraged in attack and defense mechanisms [6, 7, 48] on detectors to enhance or detect holistic context (in)consistency [6]. Though being well-motivated and demonstrating good performances in conventional setting, the state-of-the-art methThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12057",
        "chosen": "GLOW is the first approach to generate global layout-aware adversarial attacks on object detection, establishing categorical and geometric layout constraints. It localizes victim objects and generates multiple attack plans with context-consistency scores. Experiments on MS COCO and Pascal show GLOW achieves about 30% average relative improvement compared to state-of-the-art methods in conventional single object attack requests and is valid across more generic attack requests under both white-box and zero-query black-box settings.",
        "rejected": "GLOW introduces a global layout-aware adversarial attack method targeting object detection tasks. By generating attacks with categorical and geometric constraints, GLOW achieves about 30% average relative improvement over state-of-the-art methods, proving effective under white-box and black-box settings."
    },
    {
        "prompt": "summary: Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-ofthe-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions. 1. Introduction In the field of computer vision, recent progress has been made in developing photorealistic avatars [53] for applications like virtual reality, gaming, and robotics [60, 76]. Aside from looking visually realistic, avatars must also move in a convincing manner. This is challenging due to the intricate nature of human motion, strongly influenced by factors such as the environment, interactions, and physical contact [14]. Furthermore, complexity increases when attempting to control these motions. Recent advances include the generation of motion sequences from control signals like text descriptions or actions [106]; however, such methods only produce isolated, standalone motion. Therefore, these approaches fail to handle scenarios where a long motion is driven by distinct control signals on different time slices. Such capability is needed to provide full control over the sequence of desired actions and their duration. In these scenarios, the generated motion needs to feature seamless and realistic transitions between actions. In this work, we tackle this problem, which we refer to as generative Human Motion Composition (HMC). In particular, we focus on generating single-human motion from text (Fig. 1). One of the primary obstacles in HMC is the lack of datasets that offer long motion sequences with diverse textual annotations. Existing datasets typically feature sequences of limited duration, often lasting only up to 10 seconds, and with just a single control signal governing the enThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 457",
        "chosen": "FlowMDM is the first diffusion-based model generating seamless Human Motion Compositions without postprocessing. It uses Blended Positional Encodings for global motion coherence and smooth transitions, achieving state-of-the-art results on Babel and HumanML3D datasets. FlowMDM is robust against varying text descriptions at inference time due to its Pose-Centric Cross-ATtention.",
        "rejected": "FlowMDM introduces diffusion-based human motion generation driven by textual descriptions. Utilizing blended positional encodings, it achieves seamless transitions between actions, demonstrating state-of-the-art results in accuracy, realism, and smoothness on benchmarks like Babel and HumanML3D."
    },
    {
        "prompt": "summary: Training a linear classifier or lightweight model on top of pretrained vision model outputs, so-called ‘frozen features’, leads to impressive performance on a number of downstream few-shot tasks. Currently, frozen features are not modified during training. On the other hand, when networks are trained directly on images, data augmentation is a standard recipe that improves performance with no substantial overhead. In this paper, we conduct an extensive pilot study on few-shot image classification that explores applying data augmentations in the frozen feature space, dubbed ‘frozen feature augmentation (FroFA)’, covering twenty augmentations in total. Our study demonstrates that adopting a deceptively simple pointwise FroFA, such as brightness, can improve few-shot performance consistently across three network architectures, three large pretraining datasets, and eight transfer datasets. 1. Introduction Vision transformers (ViTs) [19] achieve remarkable performance on ImageNet-sized [43, 69] and smaller [21, 38, 41] datasets. In this setup, data augmentation, i.e., a predefined set of stochastic input transformations, is a crucial ingredient. Examples for image augmentations are random cropping or pixel-wise modifications that change brightness or contrast. These are complemented by more advanced strategies [13, 46, 75], such as AutoAugment [12]. A more prevalent trend is to first pretrain vision models on large-scale datasets and then adapt them downstream [6, 8, 49, 73]. Notable, even training a simple linear classifier or lightweight model on top of ViT outputs, also known as frozen features, can yield remarkable performance across a number of diverse downstream few-shot tasks [16, 25, 52]. Given the success of image augmentations and frozen features, we ask: Can we effectively combine image augmentations and frozen features to train a lightweight model? *Work done as Research Intern at Google DeepMind. †Project lead. 1 5 10 25 shots 0 2 4 6 top-1 accuracy (absolute gains) JFT-3B 1 5 10 25 shots WebLI + SigLIP MAPwd linear probe Figure 1. Average top-1 accuracy gains across seven few-shot test sets (CIFAR100 [1], SUN397 [71], ...) on various few-shot settings. We train on frozen features from an L/16 ViT [19] with JFT-3B pretraining [73] or WebLI sigmoid language-image pretraining (SigLIP) [6, 74]. Our proposed frozen feature augmentation (FroFA) method gives consistent gains over a weight decayregularized multi-head attention pooling [37] (MAPwd) and an L2regularized linear probe baseline, both without FroFA. In this paper, we revisit standard image augmentation techniques and apply them on top of frozen features in a data-constrained, few-shot setting. We dub this type of augmentation frozen feature augmentation (FroFA). Inspired directly by image augmentations, we first stochastically transform frozen features and then train a lightweight model on top. Our only modification before applying image augmentations on top of frozen features is a point-wise scaling such that each feature value lies in [0, 1] or [0, 255]. We investigate eight (few-shotted) image classification datasets using ViTs pretrained on JFT-3B [73], ImageNet21k [17], or WebLI [6]. After extracting features from each few-shot dataset we apply twenty different frozen feature augmentations and train a lightweight multi-head attention pooling (MAP) [37] on top. Our major insights are: 1. Geometric augmentations that modify the shape and structure of two-dimensional frozen features always lead to worse performance on ILSVRC-2012 [57]. On the other hand, simple stylistic (point-wise) augmentations, such as brightness, contrast, and posterize, give steady improvements on 1-, 5-, and 10-shot settings. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 16046",
        "chosen": "FroFA explores applying data augmentations in the frozen feature space for few-shot image classification. The study shows that simple pointwise augmentations like brightness can consistently improve few-shot performance across different network architectures, pre-training datasets, and transfer datasets.",
        "rejected": "Frozen feature augmentation (FroFA) applies data augmentations to frozen features from pretrained vision models, improving few-shot learning performance. Simple point-wise augmentations such as brightness consistently enhance performance across multiple architectures and datasets."
    },
    {
        "prompt": "summary: Non-isometric shape correspondence remains a fundamental challenge in computer vision. Traditional methods using Laplace-Beltrami operator (LBO) eigenmodes face limitations in characterizing high-frequency extrinsic shape changes like bending and creases. We propose a novel approach of combining the non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with the intrinsic ones of the LBO, creating a hybrid spectral space in which we construct functional maps. To this end, we present a theoretical framework to effectively integrate nonorthogonal basis functions into descriptor- and learningbased functional map methods. Our approach can be incorporated easily into existing functional map pipelines across varying applications and can handle complex deformations beyond isometries. We show extensive evaluations across various supervised and unsupervised settings and demonstrate significant improvements. Notably, our approach achieves up to 15% better mean geodesic error for non-isometric correspondence settings and up to 45% improvement in scenarios with topological noise. Code is available at: https://hybridfmaps.github.io/ ∗Equal Contribution \u0000 : {lennart.bastian,yizheng.xie}@tum.de 1. Introduction Establishing dense correspondences between 3D shapes is a cornerstone for numerous computer vision and graphics tasks such as object recognition, character animation, and texture transfer. The complexity of this task varies significantly depending on the nature of the transformation a shape undergoes. Many classic correspondence methods leverage that rigid transformations can be represented in six degrees of freedom in R3 and preserve the Euclidean distance between pairs of points. Iterative closest point (ICP) [4], and its variations [25, 40], which alternate between transformation and correspondence estimation, are very popular due to their simplicity. In this setting, local extrinsic surface properties in the embedding space stay invariant under rigid transformations such that they can be used as features during optimization, for example, the change of normals. For the wider class of isometric deformations (w.r.t. the geodesic distance), the relative embedding of the shape can change significantly, and Euclidean distances between points may not be preserved. In this class, only intrinsic properties – those that do not depend on a specific embedding of the surface – stay invariant, and the correspondence problem becomes much harder due to the quadratic size of the solution space. For example, solving a quadratic assignment problem preserving geodesic distances [24] or heat kernel This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3313",
        "chosen": "This novel approach combines non-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell hessian with intrinsic Laplace-Beltrami operator eigenmodes to create a hybrid spectral space for functional maps. It effectively integrates non-orthogonal basis functions into descriptor- and learning-based methods, handling complex deformations beyond isometries and showing significant improvements in non-isometric correspondence settings and scenarios with topological noise.",
        "rejected": "A novel shape correspondence approach integrates non-orthogonal extrinsic basis functions with intrinsic Laplace-Beltrami operator eigenmodes. This hybrid spectral space method shows up to 15% better mean geodesic error for non-isometric shapes and handles topological noise effectively."
    },
    {
        "prompt": "summary: Point cloud matching, a crucial technique in computer vision, medical and robotics ﬁelds, is primarily concerned with ﬁnding correspondences between pairs of point clouds or voxels. In some practical scenarios, emphasizing local differences is crucial for accurately identifying a correct match, thereby enhancing the overall robustness and reliability of the matching process. Commonly used shape descriptors have several limitations and often fail to provide meaningful local insights about the paired geometries. In this work, we propose a new technique, based on graph Laplacian eigenmaps, to match point clouds by taking into account ﬁne local structures. To deal with the order and sign ambiguity of Laplacian eigenmaps, we introduce a new operator, called Coupled Laplacian1, that allows to easily generate aligned eigenspaces for multiple registered geometries. We show that the similarity between those aligned high-dimensional spaces provides a locally meaningful score to match shapes. We ﬁrstly evaluate the performance of the proposed technique in a point-wise manner, focusing on the task of object anomaly localization on the MVTec 3D-AD dataset. Additionally, we deﬁne a new medical task, called automatic Bone Side Estimation (BSE), which we address through a global similarity score derived from coupled eigenspaces. In order to test it, we propose a benchmark collecting bone surface structures from various public datasets. Our matching technique, based on Coupled Laplacian, outperforms other methods by reaching an impressive accuracy on both tasks. 1. Introduction Point cloud matching, or more generally 3D shape matching, is a fundamental task in computer vision. It involves ⇤Corresponding author: matteo.bastico@minesparis.psl.eu 1Code: https://github.com/matteo-bastico/CoupLap ﬁnding the closest matching geometry to a target shape within a set of reference shapes [65]. In addition, if the task involves ﬁnding rigid transformations that best align the target shape with the reference, it is often part of a registration process. In particular, point-set rigid registration determines the relative transformation needed to align two point clouds without altering their internal structures [41]. This problem is essential for many practical computer vision tasks, such as medical image analysis [3, 32, 48, 61], intelligent vehicles [21, 34], human pose estimation [22] and objects retrieval and tracking [46, 64]. Traditional [8, 19, 57] and probabilistic registration and matching methods [15, 20, 30, 45], while robust, often struggle to optimally align complex geometries, especially in cases with intricate local structures or slight deformations. Over the years, several methods have been proposed to tackle the challenge of accurate and efﬁcient 3D shape matching and retrieval [5, 9, 11, 51, 53, 65, 73]. Data-driven 3D shape descriptors [54], capturing underlying properties of the shapes under study, are the common denominator of early shape matching techniques. Global descriptors, such as volume and areas descriptors [78], describe the entirety of the shape in one compact representation, often failing to capture local ﬁne details of complex geometries. On the other hand, local descriptors [38, 57] aim to tackle this issue but they generally are sensitive to noise, based on landmarks and they might not capture semantic information [63]. More recently, deep-learned shape descriptors [6, 72] and neural networks for shape matching, based on auto-encoders [73] or transformers [59, 68], have been proposed. Despite their good performances, these methods require a huge amount of annotated data for training, which are hard to collect in ﬁelds such as medical imaging [37]. Furthermore, non-rigid point cloud matching and retrieval methods [35, 36, 71] are designed to handle shape deformations and, therefore, they might be excessively ﬂexible ignoring ﬁne local details that are not due to deformations, such as anomalies. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3447",
        "chosen": "A new technique based on graph Laplacian eigenmaps is proposed for point cloud matching, introducing a Coupled Laplacian operator to generate aligned eigenspaces for multiple registered geometries. The similarity between aligned high-dimensional spaces provides a locally meaningful score for shape matching, outperforming other methods in object anomaly localization and automatic Bone Side Estimation tasks.",
        "rejected": "Using graph Laplacian eigenmaps, a new point cloud matching technique addresses local structure differences. The Coupled Laplacian operator aligns eigenspaces, providing meaningful similarity scores. It outperforms existing methods in object anomaly localization and introduces Bone Side Estimation for medical applications."
    },
    {
        "prompt": "summary: In recent years, automated Gallbladder Cancer (GBC) detection has gained the attention of researchers. Current state-of-the-art (SOTA) methodologies relying on ultrasound sonography (US) images exhibit limited generalization, emphasizing the need for transformative approaches. We observe that individual US frames may lack sufficient information to capture disease manifestation. This study advocates for a paradigm shift towards video-based GBC detection, leveraging the inherent advantages of spatiotemporal representations. Employing the Masked Autoencoder (MAE) for representation learning, we address shortcomings in conventional image-based methods. We propose a novel design called FocusMAE to systematically bias the selection of masking tokens from high-information regions, fostering a more refined representation of malignancy. Additionally, we contribute the most extensive US video dataset for GBC detection. We also note that, this is the first study on US video-based GBC detection. We validate the proposed methods on the curated dataset, and report a new SOTA accuracy of 96.4% for the GBC detection problem, against an accuracy of 84% by current Image-based SOTA – GBCNet and RadFormer, and 94.7% by Video-based SOTA – AdaMAE. We further demonstrate the generality of the proposed FocusMAE on a public CTbased Covid detection dataset, reporting an improvement in accuracy by 3.3% over current baselines. Project page with source code, trained models, and data is available at: https://gbc-iitd.github.io/focusmae. 1. Introduction Gallbladder Cancer (GBC). Lately, automated GBC detection has drawn an increased interest from the researchers [5,6,10,31]. GBC is difficult to detect at an early stage [27], and surgical resection becomes infeasible for most patients as the disease gets detected at a late stage. As a result, the * Soumen is currently affiliated to Samsung R&D Institute Bangalore † Joint first authors (a) (b) Figure 1. (a) Masking strategy of FocusMAE in comparison to existing random patch [14], frame [50], tube [45] masking. Our approach selects more tokens from the semantically meaningful regions with a small number of background tokens for masking. (b) Inflating the masking probability of the tokens which spatially lie within the object region (gray region) by π increases the accuracy. However, excessive masking of the object region degrades performance. Blue line: accuracy of the original random masking. disease shows bleak survival statistics. The 5-year survival rate for patients with advanced GBC is only 5%, and the mean survival time is six months [24,40]. Hence, early detection of GBC is crucial for timely intervention and improving the survival rate [26]. Ultrasound (US) for GBC Detection. US has been the This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11715",
        "chosen": "FocusMAE is a novel design for video-based Gallbladder Cancer detection, systematically biasing masking token selection from high-information regions to refine malignancy representation. The method achieves a new state-of-the-art accuracy of 96.4% on the curated dataset and demonstrates generality on a public CT-based Covid detection dataset.",
        "rejected": "FocusMAE enhances masked autoencoders for ultrasound-based Gallbladder Cancer detection using a spatiotemporal approach. By biasing masking towards high-information regions, it achieves 96.4% accuracy, surpassing current image-based and video-based SOTA methods. Code and dataset are available."
    },
    {
        "prompt": "summary: We propose a novel method for 3D point cloud action recognition. Understanding human actions in RGB videos has been widely studied in recent years, however, its 3D point cloud counterpart remains under-explored despite the clear value that 3D information may bring. This is mostly due to the inherent limitation of the point cloud data modality—lack of structure, permutation invariance, and varying number of points—which makes it difﬁcult to learn a spatio-temporal representation. To address this limitation, we propose the 3DinAction pipeline that ﬁrst estimates patches moving in time (t-patches) as a key building block, alongside a hierarchical architecture that learns an informative spatio-temporal representation. We show that our method achieves improved performance on existing datasets, including DFAUST and IKEA ASM. Code is publicly available at https://github.com/sitzikbs/3dincaction. 1. Introduction In this paper, we address the task of action recognition from 3D point cloud sequences. We propose a novel pipeline wherein points are grouped into temporally evolving patches that capture discriminative action dynamics. Our work is motivated by the massive growth of online media, mobile and surveillance cameras that have enabled the computer vision community to develop many data-driven action-recognition methods [5, 12, 26, 31], most of which rely on RGB video data. Recently, commodity 3D sensors are gaining increased momentum, however, the 3D point cloud modality for action recognition has yet been underexploited due to the scarcity of 3D action-labeled data. In many cases, a pure RGB video-based inference may not be enough and incorporating other modalities like geometry is required. This is especially necessary for safety critical applications such as autonomous systems, where redundancy is crucial, or in scenarios where the video is heavily degraded (e.g., due to poor lighting). Some approaches incorporate geometrical information implicitly, e.g., through intermediate pose estimation [7]. This often entails extra steps that require more time and resources and is still limited to video input. Therefore a more explicit approach is desirable. 3D sensors provide an alternative modality in the form of point clouds sampled on the environment. Despite the vast research on 3D vision and learning, even static 3D point cloud datasets are signiﬁcantly smaller than their RGB imThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 19978",
        "chosen": "3DinAction is a novel method for 3D point cloud action recognition, proposing a pipeline that estimates patches moving in time (t-patches) and uses a hierarchical architecture to learn spatio-temporal representations. The method improves performance on existing datasets like DFAUST and IKEA ASM.",
        "rejected": "This study investigates non-isometric shape correspondence using a hybrid spectral space combining intrinsic and extrinsic basis functions. The proposed method significantly improves upon traditional approaches, particularly in handling complex deformations and topological noise."
    },
    {
        "prompt": "summary: Domain Generalized Semantic Segmentation (DGSS) deals with training a model on a labeled source domain with the aim of generalizing to unseen domains during inference. Existing DGSS methods typically effectuate robust features by means of Domain Randomization (DR). Such an approach is often limited as it can only account for style diversification and not content. In this work, we take an orthogonal approach to DGSS and propose to use an assembly of CoLlaborative FOUndation models for Domain Generalized Semantic Segmentation (CLOUDS). In detail, CLOUDS is a framework that integrates Foundation Models of various kinds: (i) CLIP backbone for its robust feature representation, (ii) Diffusion Model to diversify the content, thereby covering various modes of the possible target distribution, and (iii) Segment Anything Model (SAM) for iteratively refining the predictions of the segmentation model. Extensive experiments show that our CLOUDS excels in adapting from synthetic to real DGSS benchmarks and under varying weather conditions, notably outperforming prior methods by 5.6% and 6.7% on averaged mIoU, respectively. Our code is available at https://github. com/yasserben/CLOUDS 1. Introduction Deep Neural Networks have showcased remarkable ability in scene understanding tasks like Semantic Segmentation (SS) [7, 70, 72], when the training and test distribution are the same. This dependency reveals a significant vulnerability: their performance substantially diminishes when encountering domain shifts [66], highlighting a fundamental challenge in generalizing these networks to unseen domains [43, 46, 61]. To address this, Domain Generalized Semantic Segmentation (DGSS) has gained prominence [28, 31, 33, 82]. DGSS aims to develop models that leverage a source-annotated dataset while remaining effecmIoU Year Traditional DGSS Open-vocabulary (zero-shot) DGSS with foundation models Ours FC-CLIP CATSeg HRDA MoDify DRPC SHADE ODISE 2019 2020 2021 2022 2023 60 50 40 55 45 FSDR GTR TLDR Figure 1. Performance over time by various methods on the GTA →{Cityscapes, BDD, Mapillary} benchmark. Recent openvocabulary approaches, like FC-CLIP, are shown to excel in zeroshot learning and surpass traditional domain generalization methods trained in closed-set scenarios, challenging the relevance of the DGSS setting. CLOUDS, by harnessing multiple foundation models, demonstrates its ability to effectively utilize the source dataset, thereby outperforming both conventional DGSS and openvocabulary methods. tive across unseen domains, thus overcoming the limitations of traditional DNNs in handling unseen environments. Recently, the advent of large-scale pretrained models, often referred to as Foundation Models (FMs) [2, 35, 56, 59, 67], have brought a paradigm shift in computer vision tasks. The FMs comprise of contrastively trained image classification models (e.g., CLIP [56], ALIGN [30]), text-conditioned generative models (e.g., Stable Diffusion [59], DALL-E [57]), vision transformer-based segmentation model trained on mammoth dataset (e.g., Segment Anything Model (SAM) [35]), to name a few. Of particular interest to SS, SAM is a promptable segmentation model that accepts as input an image and geometric prompts (points, scribbles, boxes, masks) and outputs classagnostic masks. Owing to its pre-training on billion-scale dataset [35], SAM has demonstrated excellent performance on varied out-of-distribution tasks namely, medical imaging This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3108",
        "chosen": "CLOUDS is a framework integrating various foundation models for Domain Generalized Semantic Segmentation, including CLIP for feature representation, Diffusion Model for content diversification, and Segment Anything Model for prediction refinement. It outperforms prior methods by 5.6% and 6.7% on averaged mIoU in adapting from synthetic to real DGSS benchmarks and under varying weather conditions.",
        "rejected": "To improve point cloud matching, a technique based on graph Laplacian eigenmaps is introduced, focusing on fine local structures. The Coupled Laplacian operator aligns eigenspaces, providing a locally meaningful score for shape matching. The method excels in tasks such as object anomaly localization and Bone Side Estimation."
    },
    {
        "prompt": "summary: Precise geospatial vegetation forecasting holds potential across diverse sectors, including agriculture, forestry, humanitarian aid, and carbon accounting. To leverage the vast availability of satellite imagery for this task, various works have applied deep neural networks for predicting multispectral images in photorealistic quality. However, the important area of vegetation dynamics has not been thoroughly explored. Our study introduces GreenEarthNet, the first dataset specifically designed for high-resolution vegetation forecasting, and Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images with fine resolution across Europe. Our multi-modal transformer model Contextformer leverages spatial context through a vision backbone and predicts the temporal dynamics on local context patches incorporating meteorological time series in a parameterefficient manner. The GreenEarthNet dataset features a learned cloud mask and an appropriate evaluation scheme for vegetation modeling. It also maintains compatibility with the existing satellite imagery forecasting dataset EarthNet2021, enabling cross-dataset model comparisons. Our extensive qualitative and quantitative analyses reveal that our methods outperform a broad range of baseline techniques. This includes surpassing previous state-of-theart models on EarthNet2021, as well as adapted models from time series forecasting and video prediction. To the best of our knowledge, this work presents the first models for continental-scale vegetation modeling at fine resolution able to capture anomalies beyond the seasonal cycle, thereby paving the way for predicting vegetation health and behaviour in response to climate variability and extremes. We provide open source code and pre-trained weights to reproduce our experimental results under https://gith ub.com/vitusbenson/greenearthnet [10]. Past Satellite Data Meteorology Elevation Future Vegetation State ■ Training       ■ OOD-t Test       ■ OOD-st Test Multi-modal deep learning: global pyramid vision transformer encoder + patchwise meteo-guided temporal transformer + delta prediction scheme Model Inputs Contextformer Outputs Figure 1. GreenEarthNet approach (map shows sample locations). 1. Introduction Optical satellite images have been proven useful for monitoring vegetation status. This is essential for a variety of applications in agricultural planning, forestry advisory, humanitarian assistance or carbon monitoring. In all these cases, prognostic information is relevant: Farmers want to know how their farmland may react to a given weather scenario [83]. Humanitarian organisations need to understand the localized impact of droughts on pastoral communities for mitigation of famine with anticipatory action [49]. Afforestation efforts need to consider how their forests react to future climate [71]. However, providing such prognostic information through fine resolution vegetation forecasts is challenging due to ecological memory effects [35], spatial interactions and the influence of weather variations. Deep neural networks can model relationships in space, time or across modalities. Hence, their application to vegetation forecasting given a sufficiently large dataset seems natural. 1 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27788",
        "chosen": "GreenEarthNet is the first dataset specifically designed for high-resolution vegetation forecasting, paired with Contextformer, a novel deep learning approach for predicting vegetation greenness from Sentinel 2 satellite images. This multi-modal transformer model leverages spatial context through a vision backbone and predicts temporal dynamics on local context patches incorporating meteorological time series efficiently. GreenEarthNet includes a learned cloud mask and an appropriate evaluation scheme, maintaining compatibility with EarthNet2021. Extensive analyses show Contextformer outperforms various baselines, including previous SOTA models on EarthNet2021 and adapted models from time series forecasting and video prediction.",
        "rejected": "GreenEarthNet introduces Contextformer, a novel deep learning model for predicting vegetation greenness from Sentinel 2 satellite images across Europe. This study presents the first continental-scale vegetation modeling at fine resolution, capable of capturing anomalies beyond seasonal cycles."
    },
    {
        "prompt": "summary: Astronaut photography, spanning six decades of human spaceflight, presents a unique Earth observations dataset with immense value for both scientific research and disaster response. Despite their significance, accurately localizing the geographical extent of these images, which is crucial for effective utilization, poses substantial challenges. Current, manual localization efforts are time-consuming, motivating the need for automated solutions. We propose a novel approach – leveraging image retrieval – to address this challenge efficiently. We introduce innovative training techniques which contribute to the development of a highperformance model, EarthLoc. We develop six evaluation datasets and perform a comprehensive benchmark comparing EarthLoc to existing methods, showcasing its superior efficiency and accuracy. Our approach marks a significant advancement in automating the localization of astronaut photography, which will help bridge a critical gap in Earth observations data. Code and datasets are available at https://github.com/gmberton/EarthLoc. 1. Introduction Astronaut photography of Earth is a unique remote sensing dataset that spans 60 years of human spaceflight, offering a rare perspective on our planet to the public and valuable data to Earth and atmospheric science researchers. This dataset contains over 4.5 million images and is growing by the tens of thousands per month, as astronauts are continually tasked with taking new photographs that enable scientific research as well as assist in natural disaster response efforts in the wake of events like floods and wildfires. To effectively use these images, the geographical area depicted in them needs to be identified. Unfortunately, this task - Astronaut Photography Localization (APL) - is very challenging. For each photo, only a coarse estimate of location is known, given by the point on Earth directly under the International Space Station (ISS) at the time the photo is taken. This point – called the nadir – can be easily computed using the image’s timestamp and the ISS’s orbit path. However, two Figure 1. Overview of the astronaut photography localization task. Astronauts take hundreds of photos a day from the International Space Station (ISS) cupola (top-left) with hand-held cameras. For each image (example bottom right), the geographic location depicted is not known, and needs to be searched for across a huge area centered at the ISS’s (known) nadir point at the moment that the photo is taken. A simulated view of the astronaut’s perspective when the ISS is above Europe is shown. The goal of our paper is to automate the task of localizing these images, which could be anywhere within the view. In the figure’s example, the photo the astronaut took is indicated by the green line and shown in inset – other possible photo extents are in red, illustrating the wide array of potential locations to search. images taken with the same nadir can be thousands of kilometers apart, as even a slight inclination of the astronaut’s hand-held camera can move the image’s location hundreds of kilometers in any direction, as depicted in Fig. 1. Localization must thus be performed over a wide area, and is additionally complicated by (i) astronauts using hand-held cameras and a variety of zoom lenses, (ii) the large, 2500 kilometer (km) visibility range in all directions, (iii) most photographs being oblique, and (iv) the Earth’s appearance This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12754",
        "chosen": "EarthLoc is a novel approach leveraging image retrieval to automate the localization of astronaut photography. This method efficiently addresses the challenge of identifying geographical areas in astronaut photographs, which is crucial for effective utilization. EarthLoc employs innovative training techniques to develop a high-performance model, supported by six evaluation datasets and comprehensive benchmark comparisons. The approach significantly advances automated localization of astronaut photography, bridging a critical gap in Earth observations data.",
        "rejected": "EarthLoc proposes an innovative approach for automating the localization of astronaut photography using image retrieval techniques. The method significantly advances the efficiency and accuracy in localizing these unique Earth observation datasets."
    },
    {
        "prompt": "summary: We describe a novel method, StyLitGAN, for relighting and resurfacing images in the absence of labeled data. StyLitGAN generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects, without the need for paired or CGI data. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify a set of directions. By prompting the model to fix one component (e.g., albedo) and vary another (e.g., shading), we generate relighted images by adding the identified directions to the latent style codes. Quantitative metrics of change in albedo and lighting diversity allow us to choose effective directions using a forward selection process. Qualitative evaluation confirms the effectiveness of our method. 1. Introduction Scene appearance shifts dramatically with varying lighting conditions - a sunlit room takes on a different character as daylight fades, and interior spaces transform with the flick of a switch. Similarly, surface changes, like a wall’s paint color, change not only the wall’s appearance but also the overall image due to light reflection. Despite the impressive realism achieved by current generative models like StyleGAN [22– 24], they fall short in dynamically controlling scene lighting, a key aspect of realistic image generation. In this work, we present StyLitGAN, a novel approach that extends the editing capabilities of StyleGAN [38, 45, 46, 53]. StyLitGAN uniquely manipulates style codes to selectively change lighting while preserving other image attributes like albedo and geometry. This selective editing This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4231",
        "chosen": "StyLitGAN is a novel method for relighting and resurfacing images without labeled data. It generates images with realistic lighting effects, including cast shadows, soft shadows, inter-reflections, and glossy effects. StyLitGAN uses an intrinsic image method to decompose an image, followed by a search of the latent space of a pretrained StyleGAN to identify effective directions for relighting. By fixing one component (e.g., albedo) and varying another (e.g., shading), it generates relighted images. Quantitative metrics and qualitative evaluation confirm the method's effectiveness.",
        "rejected": "StyLitGAN generates relighted images without paired or CGI data, utilizing intrinsic image methods and StyleGAN's latent space to manipulate lighting while preserving albedo and geometry, achieving realistic lighting effects."
    },
    {
        "prompt": "summary: Object detection in radar imagery with neural networks shows great potential for improving autonomous driving. However, obtaining annotated datasets from real radar images, crucial for training these networks, is challenging, especially in scenarios with long-range detection and adverse weather and lighting conditions where radar performance excels. To address this challenge, we present RadSimReal, an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental conditions, all without the need for real data collection. Remarkably, our findings demonstrate that training object detection models on RadSimReal data and subsequently evaluating them on real-world data produce performance levels comparable to models trained and tested on real data from the same dataset, and even achieves better performance when testing across different real datasets. RadSimReal offers advantages over other physical radar simulations that it does not necessitate knowledge of the radar design details, which are often not disclosed by radar suppliers, and has faster run-time. This innovative tool has the potential to advance the development of computer vision algorithms for radar-based autonomous driving applications. Our GitHub: https://yuvalhg.github.io/RadSimReal. 1. Introduction Automotive radar plays an important role in autonomous driving systems, offering long-range object detection capabilities and robustness against challenging weather and lighting conditions. The radar emits radio frequency (RF) signals and, through the processing of reflected echoes from the surrounding environment, creates a radar reflection intensity image [6]. The image contains reflection intensities *Both authors contributed equally to this work. Both authors are with General Motors, Yuval Haitman is also with the School of Electrical and Computer Engineering in Ben Gurion University of the Negev. -30 0   30  90  Azimuth [deg] 10 15 20 25 30 35 Range [m] -53 -37 -24 -12 0   12  24  37  53  90  Azimuth [deg] 10 15 20 25 30 35 Range [m] Image  Simulation Radar Image  Simulation Radar Image  RadDet Image  RadDet -30 0   30  90  Azimuth [deg] 5 10 15 20 Range [m] -53 -37 -24 -12 0   12  24  37  53  90  Azimuth [deg] 4 6 8 10 12 14 16 18 20 Range [m] (c) (d) (a) (b) Figure 1. Comparison between synthetic and real radar images from four different scenarios. Each scenario shows the camera image and the corresponding radar image. (a) and (b) simulation scenarios. (c) and (d) real scenarios. corresponding to range and angle coordinates, providing a visual representation of the scene. Afterwards, computer vision algorithms are employed to identify objects within this visual image. Numerous Deep Neural Network (DNN) methods have emerged for detecting objects in radar images [11, 22, 25, 30, 44, 45]. These techniques involve training the DNN using annotated real data. Several datasets containing real annotated radar images have been introduced [27–29, 36, 40, 44]. These datasets vary in terms of the radar type and environmental conditions. However, the primary challenge with object detection DNNs trained on real data lies in the considerable effort required to collect and annotate the data. This challenge is particularly hard in the case of radar since it is used to detect objects at long range, adverse weather This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 15407",
        "chosen": "RadSimReal is an innovative physical radar simulation capable of generating synthetic radar images with annotations for various radar types and environmental conditions without real data collection. This simulation addresses the challenge of obtaining annotated radar datasets, crucial for training neural networks in object detection. RadSimReal demonstrates comparable performance to real-data-trained models when evaluated on real-world data and even outperforms them across different real datasets. The simulation offers advantages over other physical radar simulations, including faster run-time and no need for radar design details.",
        "rejected": "RadSimReal simulates synthetic radar images with annotations, offering an effective solution for training object detection models in radar imagery. It demonstrates comparable performance to real-world data-trained models, even surpassing them in cross-dataset evaluations."
    },
    {
        "prompt": "summary: Recent advancements in large vision-language models enabled visual object detection in open-vocabulary scenarios, where object classes are defined in free-text formats during inference. In this paper, we aim to probe the stateof-the-art methods for open-vocabulary object detection to determine to what extent they understand fine-grained properties of objects and their parts. To this end, we introduce an evaluation protocol based on dynamic vocabulary generation to test whether models detect, discern, and assign the correct fine-grained description to objects in the presence of hard-negative classes. We contribute with a benchmark suite of increasing difficulty and probing different properties like color, pattern, and material. We further enhance our investigation by evaluating several stateof-the-art open-vocabulary object detectors using the proposed protocol and find that most existing solutions, which shine in standard open-vocabulary benchmarks, struggle to accurately capture and distinguish finer object details. We conclude the paper by highlighting the limitations of current methodologies and exploring promising research directions to overcome the discovered drawbacks. Data and code are available at https://lorebianchi98.github.io/FG-OVD/ . 1. Introduction Open-vocabulary object detection (OVD) consists of recognizing objects not present at training time, therefore solving the limitations imposed by traditional detectors that could only recognize a fixed pool of object classes. In the last years, open-vocabulary detectors have captured large research attention thanks to their wide flexibility in many potential downstream applications like autonomous driving [20], extended reality [19], and robotics [9, 10, 16]. The core idea behind open-vocabulary detectors is to establish a semantic connection between object regions and This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 22520",
        "chosen": "This study evaluates state-of-the-art open-vocabulary object detection methods using a dynamic vocabulary generation protocol to test their ability to detect, discern, and assign fine-grained descriptions to objects. The benchmark suite increases in difficulty and probes properties like color, pattern, and material. Results show that most existing solutions struggle to accurately capture and distinguish finer object details, highlighting limitations and suggesting research directions for improvement.",
        "rejected": "This paper evaluates open-vocabulary object detection methods' ability to understand fine-grained properties of objects. Results show limitations in accurately distinguishing finer details, highlighting challenges and suggesting future research directions."
    },
    {
        "prompt": "summary: Learning compatible representations enables the interchangeable use of semantic features as models are updated over time. This is particularly relevant in search and retrieval systems where it is crucial to avoid reprocessing of the gallery images with the updated model. While recent research has shown promising empirical evidence, there is still a lack of comprehensive theoretical understanding about learning compatible representations. In this paper, we demonstrate that the stationary representations learned by the d-Simplex fixed classifier optimally approximate compatibility representation according to the two inequality constraints of its formal definition. This not only establishes a solid foundation for future works in this line of research but also presents implications that can be exploited in practical learning scenarios. An exemplary application is the nowstandard practice of downloading and fine-tuning new pretrained models. Specifically, we show the strengths and critical issues of stationary representations in the case in which a model undergoing sequential fine-tuning is asynchronously replaced by downloading a better-performing model pretrained elsewhere. Such a representation enables seamless delivery of retrieval service (i.e., no reprocessing of gallery images) and offers improved performance without operational disruptions during model replacement. Code available at: https://github.com/miccunifi/iamcl2r. 1. Introduction By learning powerful internal feature representations from data, Deep Neural Networks (DNNs) [1–4] have made tremendous progress in some of the most challenging search tasks such as face recognition [5–9], person reidentification [10–12], image retrieval [13–15] and this significance also extends to a variety of other data modalities [16, 17]. Although all of the works mentioned above have focused on learning feature representations from static and, 𝜙𝑇   task 2 task 𝑇 task 1 𝜙2   𝜙1   Stationary  Representation (⟹ Compatible) 𝜙  Figure 1. Improved Asynchronous Model Compatible Lifelong Learning Representation (IAM-CL2R pronounced “I am clear”). In the process of lifelong learning, a model is sequentially fine-tuned and asynchronously replaced with improved third-party models that are pre-trained externally. Stationary representations ensure seamless retrieval services and better performance, without the need to reprocess gallery images. more recently, dynamic datasets [18–21], the now-standard practice is downloading and fine-tuning representations from models pre-trained elsewhere [22, 23]. These “third-party” pre-trained models often incorporate new data, utilize alternative architectures, adopt different loss functions or more in general provide novel methodologies. Whether applied individually or combined, these advancements aim to encapsulate the field’s rapid progress within a single unified model [24]. This greatly facilitates the exploitation of internally learned semantic representations, particularly as models, datasets, and computational infrastructure continue to expand in size, complexity, and cost [25, 26]. The challenge of fully exploiting such standard practice in retrieval/search systems has to deal with the underlying This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 28793",
        "chosen": "IAM-CL2R demonstrates that stationary representations learned by the d-Simplex fixed classifier optimally approximate compatible representations according to theoretical constraints. This establishes a foundation for learning compatible representations and has practical implications for retrieval systems. Stationary representations enable seamless retrieval services during model replacement without reprocessing gallery images, crucial for systems requiring minimal operational disruptions.",
        "rejected": "The paper explores learning compatible representations for search and retrieval systems, demonstrating that stationary representations optimally approximate compatibility, enabling seamless service delivery during model updates."
    },
    {
        "prompt": "summary: Significant progress has been made in scene text detection models since the rise of deep learning, but scene text layout analysis, which aims to group detected text instances as paragraphs, has not kept pace. Previous works either treated text detection and grouping using separate models, or train a model from scratch while using a unified one. All of them have not yet made full use of the already well-trained text detectors and easily obtainable detection datasets. In this paper, we present Text Grouping Adapter (TGA), a module that can enable the utilization of various pre-trained text detectors to learn layout analysis, allowing us to adopt a well-trained text detector right off the shelf or just fine-tune it efficiently. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as universal inputs to assemble text instance features. To capture broader contextual information for layout analysis, we propose to predict text group masks from text instance features by one-to-many assignment. Our comprehensive experiments demonstrate that, even with frozen pre-trained models, incorporating our TGA into various pre-trained text detectors and text spotters can achieve superior layout analysis performance, simultaneously inheriting generalized text detection ability from pre-training. In the case of full parameter fine-tuning, we can further improve layout analysis performance. 1. Introduction With the rise of deep learning, text detection [17, 18, 22, 44], text recognition [3, 8, 16, 34], and end-to-end text spotting [19, 20, 38, 39] models have greatly improved the accuracy and efficiency of identifying text instances like words *Work done during the internship at Microsoft Research Asia. ‡Corresponding authors. (a) Backbone Layout Branch 🔥 Detection Branch text regions affinity  matrix learnable query (b) Image text  regions affinity matrix 🔥 Text Grouping Adapter group  masks Backbone Detection Branch Text Detection layout Layout Analysis image image layout Text Detector Figure 1. Top: Visualization of the scene text detection and layout analysis tasks. The mask with the same color denotes detected as a group. Bottom: Comparison between (a) the previous work Unified Detector [23] and (b) proposed TGA. TGA also provides the flexibility of freezing or fine-tuning the pre-trained text detector. or text lines. To fully understand the text semantics in various applications [1, 2, 30, 41, 45], it is essential to determine how to organize these text instances into coherent semantic entities, e.g., determining which detected words constitute a line and which detected lines form a paragraph. This problem, as visualized in the top of Figure 1, named scene text layout analysis [23], has not been advanced at the same pace as other scene text understanding tasks. Previous layout analysis works [13, 29, 46] adopt separate semantic segmentation models to localize different high-level entities in scanned and digital document images, which only focus on limited scenarios instead of generalscope natural scenes and ignore low-level text instances like words and text lines. Recent design on a Unified DetecThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 28150",
        "chosen": "Text Grouping Adapter (TGA) is a module enabling the use of pre-trained text detectors for layout analysis. Designed to be compatible with various text detector architectures, TGA takes detected text regions and image features as inputs to assemble text instance features. It predicts text group masks by one-to-many assignment to capture broader contextual information. Experiments show TGA enhances layout analysis performance even with frozen pre-trained models, inheriting generalized text detection ability and improving performance further with full parameter fine-tuning.",
        "rejected": "TGA enables pre-trained text detectors to learn layout analysis, improving scene text grouping without retraining. It uses detected text regions and image features to predict text group masks, enhancing layout analysis performance."
    },
    {
        "prompt": "summary: The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pretrained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our compressiontransformer(C-Former ),reducing the encoded sequence length, thereby allowing a tradeoff between quality and latency. Extensive experiments showcase GRAM’s stateof-the-art performance on the benchmarks for multi-page DocVQA, demonstrating the effectiveness of our approach. 1. Introduction Document understanding, particularly in the context of DocVQA, has gained substantial research interest [5, 6, 16, 25, 36, 37] and offers a wide array of practical applications, focusing on data extraction and analysis of single page documents. However, Multi-Page DocVQA (MPDocVQA) poses a more realistic challenge, considering that the majority of documents, including contracts, manuals *Work conducted during an internship at Amazon. †Corresponding author: alongolt@amazon.com ‡Corresponding author: litmanr@amazon.com + How many  diagrams are there? Multi Page Encoder  Global-Local Encoder Block Page Sub-Layer Doc Sub-Layer  Page Sub-Layer Doc Sub-Layer  C-Former Page  Attention Doc  Attention Compression Transformer Decoder Figure 1. An Overview of GRAM. We suggest an interleaved encoder architecture combining page- with document-attention layers, allowing information to propagate between different pages. An optional compression transformer (C-former) is introduced to allow a trade-off between quality and latency. and scientific papers, often extend well beyond a single page. Despite the practical relevance of MPDocVQA, it has received limited attention, primarily due to the absence of suitable datasets. Two recently introduced datasets, MPDocVQA [33] and DUDE [18], have opened up new avenues for MP-DocVQA research. Recent DocVQA approaches rely on transformers [35], at the heart of their architecture. While transformers are a powerful tool, they face challenges when dealing with long input sequences [4, 7, 10–12, 27, 38]. This difficulty stems from the self-attention mechanism, which scales quadratically in terms of computation and memory, with respect to the input sequence length. Addressing this limitation has attracted significant research efforts, primarily in the field This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 15598",
        "chosen": "GRAM extends pretrained single-page models to multi-page DocVQA without heavy pretraining. It uses a single-page encoder for local understanding and enhances it with document-level layers and learnable tokens for global reasoning. A compression stage using C-Former reduces sequence length, allowing a quality-latency tradeoff. GRAM achieves state-of-the-art performance on multi-page DocVQA benchmarks.",
        "rejected": "GRAM extends single-page document visual question answering (DocVQA) models to multi-page settings, facilitating global reasoning through document-level designated layers and learnable tokens. An optional compression stage allows quality-latency trade-offs."
    },
    {
        "prompt": "summary: Knowledge of lane topology is a core problem in autonomous driving. Aerial imagery can provide high resolution, quickly updatable lane source data but detecting lanes from such data has so far been an expensive manual process or, where automated solutions exist, undrivable and requiring of downstream processing. We propose a method for large-scale lane topology extraction from aerial imagery while ensuring that the resulting lanes are realistic and drivable by introducing a novel B´ezier Graph shared parameterisation of B´ezier curves. We develop a transformer-based model to predict these B´ezier Graphs from input aerial images, demonstrating competitive results on the UrbanLaneGraph dataset. We demonstrate that our method generates realistic lane graphs which require both minimal input, and minimal downstream processing. We make our code publicly available at https://github. com/driskai/BGFormer 1. Introduction Autonomous Vehicles (AVs) require knowledge of their surroundings to operate. So far, all systems not reliant on a safety driver have required pre-built High Deﬁnition maps (HD maps). HD maps can deliver a strong prior about the road and lane topology, e.g. so the AV can navigate amid occlusions. But creation of HD maps is time-consuming and expensive, typically requiring a ﬂeet of road vehicles equipped with LiDAR and cameras followed by extensive manual curation and human annotation [18]. One of the core components of an HD map is precise lane geometry and topology data. Previous works have atFor the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising. This work was funded by the Centre for Connected and Autonomous Vehicles, delivered by Innovate UK, part of UK Research and Innovation. Figure 1. Our trained model detecting a lane B´ezier Graph on an intersection. B´ezier endpoints P 0,3 shown in cyan, control points P 1,2 in white, resulting curves in red. tempted to automate the generation of this data from onboard sensors [4, 5, 10] or - recently - from aerial imagery [3, 9, 30, 31]. Generating lane-level data from aerial imagery is of particular interest since this can be efﬁciently obtained at large scale using Unmanned Aerial Vehicles (UAVs) or satellites. In this way, AVs could be equipped with regularly updated HD maps that cover a large area. However, these approaches are not without their challenges. Lane geometry and topology is typically predicted in the form of a lane graph: a graph in which nodes represent a discrete sampling of lane centre lines, with edges representing connectivity. Existing methods typically predict the position of each node in 2D space, but this frequently results in noisy, non-physical lane centre lines requiring extensive downstream processing and human oversight. We improve on these methods by introducing a shared parameterisation of cubic B´ezier curves in a graph structure that we refer to as a B´ezier Graph. By associating B´ezier direction vectors with nodes, we enforce a strong prior that lane direction should be continuous at boundaries. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 15365",
        "chosen": "This method extracts large-scale lane topology from aerial imagery using a novel Bézier Graph parameterization of cubic Bézier curves. A transformer-based model predicts these graphs, generating realistic, drivable lane graphs with minimal input and downstream processing. The approach improves on existing methods by ensuring lane direction continuity at boundaries, demonstrated on the UrbanLaneGraph dataset.",
        "rejected": "This paper presents BGFormer, a transformer-based model for predicting Bezier Graphs from aerial images, generating drivable lane graphs with minimal downstream processing, ideal for HD map creation in autonomous driving."
    },
    {
        "prompt": "summary: Image keypoint descriptions that are discriminative and matchable over large changes in viewpoint are vital for 3D reconstruction. However, descriptions output by learned descriptors are typically not robust to camera rotation. While they can be made more robust by, e.g., data augmentation, this degrades performance on upright images. Another approach is test-time augmentation, which incurs a significant increase in runtime. Instead, we learn a linear transform in description space that encodes rotations of the input image. We call this linear transform a steerer since it allows us to transform the descriptions as if the image was rotated. From representation theory, we know all possible steerers for the rotation group. Steerers can be optimized (A) given a fixed descriptor, (B) jointly with a descriptor or (C) we can optimize a descriptor given a fixed steerer. We perform experiments in these three settings and obtain state-of-the-art results on the rotation invariant image matching benchmarks AIMS and Roto-360. We publish code and model weights at this https url. 1. Introduction Discriminative local descriptions are vital for multiple 3D vision tasks, and learned descriptors have recently been shown to outperform traditional handcrafted local features [17, 19, 23, 43]. One major weakness of learned descriptors compared to handcrafted features such as SIFT [35] is the relative lack of robustness to non-upright images [55]. While images taken from ground level can sometimes be made upright by aligning with gravity as the canonical orientation, this is not always possible. For example, descriptors robust to rotation are vital in space applications [49], as well as medical applications [42], where no such canonical orientation exists. Even when a canonical orientation exists, it may be difficult or impossible to estimate. Rotation invariant matching is thus a key challenge. The most straightforward manner to get rotation invariant matching is to train or design a descriptor to be rotation invariant [17, 35]. However, this sacrifices distinctiveness in matching images with small relative rotations [41]. An alternative approach is to train a rotation-sensitive descriptor and perform test-time-augmentation, selecting the pair that produces the most matches. The obvious downside of TTA is computational cost. For example, testing all 45◦rotations requires running the model eight times. In this paper, we present an approach that maintains distinctiveness for small rotations and allows for rotation invariant matching when we have images with large rotations. We do this while adding only negligible additional runtime, running the descriptor only a single time. The main idea is to learn a linear transform in description space that corresponds to a rotation of the input image; see Figure 2. We call this linear transform a steerer as it allows us to modify keypoint descriptions as if they were describing rotated images—we can steer the descriptions without having to rerun the descriptor network. We show empiriThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4885",
        "chosen": "Steerers are learned linear transforms in description space that encode image rotations for keypoint descriptors. This framework allows rotation-equivariant matching without sacrificing distinctiveness for small rotations or incurring significant runtime costs. Experiments show state-of-the-art results on rotation-invariant benchmarks, with code and model weights published.",
        "rejected": "The study focuses on creating discriminative image keypoint descriptors robust over large viewpoint changes for 3D reconstruction, addressing the lack of robustness in current learned descriptors."
    },
    {
        "prompt": "summary: Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts, and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability. Code available at: https://github.com/ LuisBolanos17/GaussianShadowCasting 1. Introduction It is now possible to reconstruct animatable 3D neural avatars from video but methods do not account for accurate lighting and shadows. They have to rely on recordings that have soft uniform lighting, which precludes recording outdoors in direct sun light and on film sets with spotlights, and most are unable to relight characters in novel environments, limiting their applicability in content creation. The most recent body models [17,20,28,36,37,40] which are based on neural radiance fields (NeRFs) [26], approximate the light transport by casting primary rays between the camera and the scene, sampling the underlying neural network dozens of times along each ray to obtain the density and color. As they do not include an illumination model, the color that the NeRF learns includes lighting, shadow, and view-dependent effects. Learning a body model in a challenging scene with a strong directional light source, such as the sun, leads to the neural field overfitting to the observed shadows. It does not generalize to novel poses, as the cast shadows are global effects where movement of a joint could affect the appearance of other distant areas of the body. Figure 1 shows such setting. This is in contrast to local shading effects such as wrinkles in clothing which current body models can successfully reconstruct. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20997",
        "chosen": "Gaussian Shadow Casting for Neural Characters presents a new shadow model using a Gaussian density proxy for efficient shadow computation in neural character models. This method improves reconstructions by better separating albedo, shading, and shadows, enabling realistic relighting and novel pose generation without user input.",
        "rejected": "The paper introduces Gaussian shadows, a new model for accurate lighting and shadow computation in neural avatars reconstructed from video. This method supports dynamic motion and integrates with deferred neural rendering to improve shading and shadow casting. It enables realistic relighting in novel scenes without user input on light direction."
    },
    {
        "prompt": "summary: In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photorealistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work. 1. Introduction Point goal navigation of terrestrial robots in indoor buildings has traditionally been addressed in the robotics community with mapping and planning [6, 30, 50], which led to solutions capable of operating on robots in real environments. The field of computer vision and embodied AI has addressed this problem through large-scale machine learning in simulated 3D environments from reward with RL [24, 34] or with imitation learning [14]. Learning from large-scale data allows the agent to pick up more complex regularities, to process more subtle cues and therefore (in principle) to be more robust, to exploit data regularities to infer hidden and occluded information, and generally to learn more powerful Training/sim (ours) Test/real xt 1 2 1 2 xt+1 Training/sim (classical) xt 1 2 xt+1 Policy:  ∆π Onboard ctrl ∆φ 333ms/3 Hz (Unknown) Policy:  ∆π Dyn. model ∆φ 333ms/3 Hz 33ms/30 Hz Policy:  ∆π constant 333ms/3 Hz velocities Piecewise Figure 1. Efficient navigation with policies end-to-end trained in 3D photorealistic simulators requires closing the sim2real gap in sensing and actuation. Efficiency demands that the robot continues to move during decision taking (as opposed to stopping for each sensing operation), and this requires a realistic motion model in simulation allowing the agent to internally anticipate its future state. This requirement is exacerbated by the delay between sensing ①and actuation ②caused by the computational complexity of high-capacity deep networks (visual encoders, policy). To model realistic motion while training in simulation, we create a 2nd order dynamical model running with higher frequency, which models the robot and its low-level closed-loop controller. We identify the model from real data and add it to the Habitat [44] Simulator. decision rules. In this context, the specific task of point goal navigation (navigation to coordinates) is now sometimes considered “solved” in the literature [38], incorrectly, as we argue. While the machine learning and computer vision community turns its attention towards the exciting goal of integrating language models into navigation [15, 23], we think that further improvements are required to make trained agents perform reliably in real environments with sufficient speed. Experiments and evaluations of trained models in real environments and the impact of the sim2real gap do exist in the Embodied AI literature [11, 20, 43], but they are rare and were performed in restricted environments. Known models This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 17837",
        "chosen": "This work explores end-to-end training of agents in simulation to minimize the sim2real gap for autonomous robot navigation. The agent predicts velocity commands and is trained with a modified Habitat simulator modeling real robot behavior and noise. Evaluations show significant performance and robustness gains in real navigation scenarios.",
        "rejected": "This work addresses minimizing the sim2real gap for terrestrial robots by end-to-end training in simulations that closely mimic real-world sensing and actuation dynamics. By using a modified Habitat simulator with noise models for odometry and localization, it achieves significant improvements in navigation performance and robustness compared to prior methods."
    },
    {
        "prompt": "summary: Local Interpretable Model-agnostic Explanations (LIME) - a widely used post-ad-hoc model agnostic explainable AI (XAI) technique. It works by training a simple transparent (surrogate) model using random samples drawn around the neighborhood of the instance (image) to be explained (IE). Explanations are then extracted for a black-box model and a given IE, using the surrogate model. However, the explanations of LIME suffer from inconsistency across different runs for the same model and the same IE. We identify two main types of inconsistencies: variance in the sign and importance ranks of the segments (superpixels). These factors hinder LIME from obtaining consistent explanations. We analyze these inconsistencies and propose a new method, Stabilized LIME for Consistent Explanations (SLICE). The proposed method handles the stabilization problem in two aspects: using a novel feature selection technique to eliminate spurious superpixels and an adaptive perturbation technique to generate perturbed images in the neighborhood of IE. Our results demonstrate that the explanations from SLICE exhibit signiﬁcantly better consistency and ﬁdelity than LIME (and its variant BayLime). 1. Introduction In the broad spectrum of post-ad-hoc explanation methods, model-agnostic methods like LIME [13], SHAP [9] and their variants have been popular for extracting explanations from Black-Box models. While explanation methods such as Grad-CAM [16], Grad-CAM++ [2], and Ablation-CAM [12] require access to the intermediate layers of the model, methods like LIME, and SHAP methods require access only to the input and the output of the model. In our paper, we focus on the category of post-adhoc methods that uses local surrogate models for explanations. While LIME has the advantages of being modelagnostic and being able to extract explanations in a postad-hoc manner, it is also inconsistent in the explanations [5, 7, 8, 22, 23, 25, 26]. [23] observes three types of uncertainty: sampling variance in explaining a data point, sensitivity to the choice of parameters such as the size of the neighborhood and sample size, and variation of model credThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 10988",
        "chosen": "SLICE is a new method enhancing LIME's consistency for image classification explanations. It uses feature selection to eliminate spurious superpixels and adaptive perturbation to generate neighborhood images, yielding more reliable and consistent explanations than LIME and its variants.",
        "rejected": "SLICE is proposed to stabilize explanations generated by LIME, addressing inconsistencies due to variance in segment importance ranks. SLICE uses feature selection and adaptive perturbation techniques to produce more consistent and faithful explanations for black-box models than LIME, enhancing reliability in explainable AI."
    },
    {
        "prompt": "summary: We study the underexplored but fundamental problem of machine understanding of abstract freehand scene sketches. We introduce a sketch encoder that ensures a semanticallyaware feature space, which we evaluate by testing its performance on a semantic sketch segmentation task. To train our model, we rely only on bitmap sketches accompanied by brief captions, avoiding the need for pixel-level annotations. To generalize to a large set of sketches and categories, we build upon a vision transformer encoder pretrained with the CLIP model. We freeze the text encoder and perform visual-prompt tuning of the visual encoder branch while introducing a set of critical modifications. First, we augment the classical key-query (k-q) self-attention blocks with value-value (v-v) self-attention blocks. Central to our model is a two-level hierarchical training that enables efficient semantic disentanglement: The first level ensures holistic scene sketch encoding, and the second level focuses on individual categories. In the second level of the hierarchy, we introduce cross-attention between the text and vision branches. Our method outperforms zero-shot CLIP segmentation results by 37 points, reaching a pixel accuracy of 85.5% on the FS-COCO sketch dataset. Finally, we conduct a user study that allows us to identify further improvements needed over our method to reconcile machine and human understanding of freehand scene sketches. 1. Introduction Even a quick sketch can convey rich information about what is relevant in a visual scene: what objects there are and how they are arranged. However, little work has been devoted to the task of machine scene sketch understanding, largely due to a lack of data. Understanding sketches with methods designed for images is challenging because sketches have very different statistics from images – they are sparser and lack detailed color and texture information. Moreover, sketches contain abstraction at multiple levels: the holisCLIP zero shot segmentation CLIP zero shot segmentation Our segmentation Our segmentation A giraffe and a zebra are  standing on the grass. A man with a kite and a tree  in the background. Figure 1. Comparison of the segmentation result obtained with CLIP visual encoder features and features from our model. tic scene level and the object level. Here we explore the promise of two main ideas: (1) the use of language to guide the learning of how to parse scene sketches and (2) a twolevel training network design for holistic scene understanding and individual categories recognition. Freehand sketches can be represented as a sequence or cloud of individual strokes, or as a bitmap image. As one of the first works on scene sketch understanding, we target a general setting where we assume only the availability of bitmap representations. We also aim at the method that can generalize to a large number of scenes and object categories. To this end, we build our sketch encoder on a Visual Transformer (ViT) encoder pre-trained with a popular CLIP [44] foundation model (Fig. 1). We propose a two-level hierarchical training of our network, where the two levels (“Holistic” and “Category-level”) share the weights of our visual encoder. The first level focuses on ensuring that our model can capture holistic scene understanding (Fig. 2: I. Holistic), while the second level ensures that the encoder can efficiently encode and distinguish individual categories (Fig. 2: II. Category-level). We avoid reliance on tedious user perpixel annotations by leveraging sketch-caption pairs from the FS-COCO dataset [9], and aligning the visual tokens of sketch patches with textual tokens from the sketch captions, using triplet loss training. We strengthen the alignment by introducing sketch-text cross-attention in the second level of the network’s hierarchy (Fig. 2: g.). Additionally, we introduce a modified self-attention computation to the visual This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4176",
        "chosen": "This paper introduces a sketch encoder for understanding abstract freehand scene sketches, evaluated on semantic sketch segmentation. The model, built on a CLIP-pretrained ViT, uses two-level hierarchical training and cross-attention to achieve high pixel accuracy on the FS-COCO dataset.",
        "rejected": "A sketch encoder is introduced that leverages a vision transformer pre-trained with CLIP for semantic segmentation of freehand scene sketches. The hierarchical training approach allows efficient disentanglement at both holistic and category levels, achieving superior zero-shot segmentation accuracy compared to CLIP on the FS-COCO dataset."
    },
    {
        "prompt": "summary: Vision-language foundation models have shown remarkable performance in various zero-shot settings such as image retrieval, classification, or captioning. But so far, those models seem to fall behind when it comes to zero-shot localization of referential expressions and objects in images. As a result, they need to be fine-tuned for this task. In this paper, we show that pretrained vision-language (VL) models allow for zero-shot open-vocabulary object localization without any fine-tuning. To leverage those capabilities, we propose a Grounding Everything Module (GEM) that generalizes the idea of value-value attention introduced by CLIPSurgery [17] to a self-self attention path. We show that the concept of self-self attention corresponds to clustering, thus enforcing groups of tokens arising from the same object to be similar while preserving the alignment with the language space. To further guide the group formation, we propose a set of regularizations that allows the model to finally generalize across datasets and backbones. We evaluate the proposed GEM framework on various benchmark tasks and datasets for semantic segmentation. GEM not only outperforms other training-free open-vocabulary localization methods, but also achieves state-of-the-art results on the recently proposed OpenImagesV7 large-scale segmentation benchmark. 1 1. Introduction Vision-Language models, trained on large-scale web-based datasets such as WIT-400M [29], LAION400M [30], or metaclip-400M [35] with image-text supervision only, have so far shown a remarkable set of capabilities. These models such as CLIP [29], OpenCLIP [30], BLIP [15], or recently MetaCLIP [35] exhibit the ability to generalize to a broad range of downstream tasks like zero-shot image classification [6, 12, 29], visual question answering [13], action recognition [38, 40], image captioning [15, 16], and view synthesis [11]. However, models trained with image-level objectives such as contrastive loss, image-text matching, or 1Code is available at https://github.com/WalBouss/GEM. CLIP GEM (ours) CLIPSurgery MaskCLIP Bowl Man Hamburger Jet ski Figure 1. Qualitative results of training-free methods: given a text prompt, the similarity of each image token with the prompt is calculated (red:high, blue:low). The proposed GEM method provides improved grouping and alignment compared to other approaches. image captioning struggle to maintain their zero-shot capabilities for tasks related to visual localization [17, 42]. Even worse, when prompting such models, e.g., for specific objects, they often exhibit an inverse vision-language relation: the prompt embedding has a larger distance from image patches containing the object compared to patches of the background, as shown in Figure 1. In order to leverage vision-language models (VLMs) to localize objects in an open-vocabulary setting, different sets of approaches have been proposed. The first line of work trains models to detect or segment regions in an imThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3828",
        "chosen": "GEM enables zero-shot open-vocabulary object localization using pretrained VL models. The proposed Grounding Everything Module uses self-self attention and regularizations to improve token grouping and alignment, achieving state-of-the-art results on segmentation benchmarks without fine-tuning.",
        "rejected": "GEM enhances zero-shot object localization capabilities of pretrained vision-language models through self-self attention paths, promoting clustering of tokens representing the same object. GEM outperforms existing methods on various benchmarks and achieves state-of-the-art results on OpenImagesV7 large-scale segmentation."
    },
    {
        "prompt": "summary: Text-to-image diffusion models have recently received increasing interest for their astonishing ability to produce high-ﬁdelity images from solely text inputs. Subsequent research efforts aim to exploit and apply their capabilities to real image editing. However, existing image-toimage methods are often inefﬁcient, imprecise, and of limited versatility. They either require time-consuming ﬁnetuning, deviate unnecessarily strongly from the input image, and/or lack support for multiple, simultaneous edits. To address these issues, we introduce LEDITS++, an efﬁcient yet versatile and precise textual image manipulation technique. LEDITS++’s novel inversion approach requires no tuning nor optimization and produces high-ﬁdelity results with a few diffusion steps. Second, our methodology supports multiple simultaneous edits and is architectureagnostic. Third, we use a novel implicit masking technique that limits changes to relevant image regions. We propose the novel TEdBench++ benchmark as part of our exhaustive evaluation. Our results demonstrate the capabilities of LEDITS++ and its improvements over previous methods. 1. Introduction Text-to-image diffusion models (DM) have garnered recognition for their ability to generate high-quality images from textual descriptions. A growing body of research has recently been dedicated to utilizing these models for manipulating real images. However, several barriers prevent many real-world applications of diffusion-based image editing. Current methods often entail computationally expensive model tuning or other optimization, presenting practical challenges [6, 18, 28, 30, 44]. Additionally, existing techniques have the proclivity to induce profound changes to the original image [17, 26], often resulting in completely different images. Lastly, all these approaches are inherently constrained when editing multiple (arbitrary) concepts simultaneously. We tackle these problems by introducing LEDITS++1, a diffusion-based image editing technique addressing these limitations. LEDITS++2 offers a streamlined approach for textual image editing, eliminating the need for extensive parameter tuning. To this end, we derive image inversion for a more efﬁcient diffusion sampling algorithm to a) drastically reduce computational resources and b) guarantee perfect image reconstruction. Thus, we overcome computational obstacles and avoid changes in the edited image in the ﬁrst place. Furthermore, we use a novel implicit masking approach to semantically ground each edit instruction to its relevant image region. This further optimizes changes to the image by retaining the overall image composition and object identity. *Equal contribution †Partially as research intern at Adobe 1LEDITS++ stands for Limitless Edits with sde-dpm-solver++. 2https://huggingface.co/spaces/leditsplusplus/project This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 8861",
        "chosen": "LEDITS++ is an efficient textual image manipulation technique using a novel inversion approach and implicit masking. It requires no tuning, supports multiple edits, and produces high-fidelity results with few diffusion steps, outperforming previous methods.",
        "rejected": "LEDITS++ presents an efficient textual image manipulation technique requiring no fine-tuning, supporting multiple simultaneous edits, and using implicit masking to limit changes to relevant regions. It provides high-fidelity results with fewer diffusion steps, making it suitable for versatile and precise image editing applications."
    },
    {
        "prompt": "summary: Neural approaches have shown a significant progress on camera-based reconstruction. But they require either a fairly dense sampling of the viewing sphere, or pre-training on an existing dataset, thereby limiting their generalizability. In contrast, photometric stereo (PS) approaches have shown great potential for achieving high-quality reconstruction under sparse viewpoints. Yet, they are impractical because they typically require tedious laboratory conditions, are restricted to dark rooms, and often multi-staged, making them subject to accumulated errors. To address these shortcomings, we propose an end-to-end uncalibrated multi-view PS framework for reconstructing high-resolution shapes acquired from sparse viewpoints in a real-world environment. We relax the dark room assumption, and allow a combination of static ambient lighting and dynamic near LED lighting, thereby enabling easy data capture outside the lab. Experimental validation confirms that it outperforms existing baseline approaches in the regime of sparse viewpoints by a large margin. This allows to bring highaccuracy 3D reconstruction from the dark room to the real world, while maintaining a reasonable data capture complexity. 1. Introduction The challenge of 3D reconstruction stands as a cornerstone in both computer vision and computer graphics. Despite notable progress in recovering an object’s shape from dense image viewpoints, predicting consistent geometry from sparse viewpoints remains a difficult task. Contemporary approaches employing neural data structures depend heavily on extensive training data to achieve generalization in the context of sparse views. Additionally, the presence of a wide baseline or textureless objects form significant obstacles. In contrast, photometric methodologies like photometric stereo (PS) excel in reconstructing geometry, even in textureless regions. This capability is attributed to the abun*The contribution was done while at TUM. Multiillumination data at viewpoint 1 Multiillumination data at viewpoint 2 PS-NeRF [60] Ours Figure 1. We introduce the first framework for multi-view uncalibrated point-light photometric stereo. Given a set of PS images captured from different viewpoints (left), our method recovers high-fidelity 3D reconstruction (right). The acquisition of uncalibrated point-light PS imagery captured under ambient lighting in a sparse multi-view setup does not only allow for easy data capture, but also leads to 3D reconstructions of unprecedented detail. Here, with as few as two views we are able to reconstruct the squirrel’s 3D geometry at higher precision than the state-of-the-art. dance of shading information derived from images acquired under diverse illumination. Yet, such approaches typically require a controlled laboratory setup to fulfill the necessary dark room and directional light assumptions. As a consequence, PS approaches become impractical beyond the confines of a laboratory. To address these shortcomings we combine state-of-theart volume rendering formulations with a sparse multi-view photometric stereo model. In particular, we advocate a physically realistic lighting model that combines ambient light and uncalibrated point-light illumination. Our approach facilitates simplified data acquisition, and we introduce a novel pipeline capable of reconstructing an object’s geometry from a sparse set of viewpoints, even if the object is completely textureless. Furthermore, since we assume a point-light model instead of distant directional lighting, we can infer absolute depth from a single viewpoint, allowing us to address the challenge of wide camera baselines. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11862",
        "chosen": "This paper proposes an end-to-end uncalibrated multi-view photometric stereo framework for high-resolution 3D reconstruction from sparse viewpoints in real-world settings. It combines static ambient and dynamic LED lighting, outperforming baselines and enabling lab-quality reconstruction in practical conditions.",
        "rejected": "An end-to-end uncalibrated multi-view photometric stereo framework is introduced for 3D reconstruction from sparse viewpoints under ambient and near LED lighting conditions. The approach overcomes limitations of laboratory setups, enabling high-accuracy reconstructions in real-world environments without controlled lighting."
    },
    {
        "prompt": "summary: This paper introduces a versatile paradigm for integrating multi-view reﬂectance (optional) and normal maps acquired through photometric stereo. Our approach employs a pixel-wise joint re-parameterization of reﬂectance and normal, considering them as a vector of radiances rendered under simulated, varying illumination. This reparameterization enables the seamless integration of reﬂectance and normal maps as input data in neural volume rendering-based 3D reconstruction while preserving a single optimization objective. In contrast, recent multi-view photometric stereo (MVPS) methods depend on multiple, potentially conﬂicting objectives. Despite its apparent simplicity, our proposed approach outperforms state-of-the-art approaches in MVPS benchmarks across F-score, Chamfer distance, and mean angular error metrics. Notably, it signiﬁcantly improves the detailed 3D reconstruction of areas with high curvature or low visibility. 1. Introduction Automatic 3D reconstruction is pivotal in various ﬁelds, such as archaeological and cultural heritage (virtual reconstruction), medical imaging (surgical planning), virtual and augmented reality, games and ﬁlm production. Multi-view stereo (MVS) [5], which retrieves the geometry of a scene seen from multiple viewpoints, is the most famous 3D reconstruction solution. Coupled with neural volumetric rendering (NVR) techniques [22], it effectively handles complex structures and self-occlusions. However, dealing with non-Lambertian scenes remains a challenge due to the breakdown of the underlying brightness consistency assumption. The problem is also ill-posed in certain conﬁgurations e.g., poorly textured scene [25] or degener*Equal contributions. brument.bcb@gmail.com / rb@di.ku.dk Figure 1. One image from DiLiGenT-MV’s Buddha dataset [12], and 3D reconstruction results from several recent MVPS methods: [11, 26, 27] and ours. The latter provides the ﬁne details closest to the ground truth (GT), while being remarkably simpler. ate viewpoints conﬁgurations with limited baselines. Moreover, despite recent efforts in this direction [13], recovering the thinnest geometric details remains difﬁcult under ﬁxed illumination. In such a setting, estimating the reﬂectance of the scene also remains a challenge. On the other hand, photometric stereo (PS) [24], which relies on a collection of images acquired under varying lighting, excels in the recovery of high-frequency details under the form of normal maps. It is also the only photographic technique that can estimate reﬂectance. And, with the recent advent of deep learning techniques [8], PS gained enough maturity to handle non-Lambertian surfaces and complex illumination. Yet, its reconstruction of geometry’s low frequencies remains suboptimal. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 5230",
        "chosen": "RNb-NeuS integrates multi-view reflectance and normal maps into neural volume rendering via a joint re-parameterization. This approach outperforms state-of-the-art methods in MVPS benchmarks, improving 3D reconstruction of detailed areas.",
        "rejected": "The paper proposes a joint re-parameterization of reflectance and normal maps as radiances under simulated illumination for integration into neural volume rendering-based 3D reconstruction. This method simplifies optimization and improves reconstruction quality, especially in areas with high curvature or low visibility, outperforming state-of-the-art approaches in MVPS benchmarks."
    },
    {
        "prompt": "summary: In this paper we tackle the problem of learning Structurefrom-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provide an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed model outperforms competing learning-based methods, and challenges COLMAP while having lower runtime. Our code is available at: https://github.com/lucasbrynte/gasfm/. 1. Introduction Structure-from-Motion (SfM) is a classic and still relevant problem in computer vision. The goal of SfM is to estimate camera poses and 3D coordinates of keypoints detected across multiple images, and can be formulated as an optimization over m camera matrices {Pi}, i = 1, . . . , m and n 3D points {Xj} , j = 1, . . . , n of the form   \\begin  {aligned }  &  \\unde rset  {\\{\\ma th bf { P} _i\\} ,\\{ \\mathbf {X}_j\\}}{\\text {minimize}} & & \\sum _{ij} r\\left (\\mathbf {m}_{ij}, \\mathbf {z}_{ij} \\right ) \\\\ & \\text {subject to} & & \\mathbf {z}_{ij} = \\mathbf {P}_i \\mathbf {\\bar {X}}_j, \\text { } \\forall i,j \\end {aligned} \\label {eq:sfm_optimization}  (1) where mij holds the 2D coordinates of the jth keypoint in the ith image. The loss in (1) is generally chosen as the reprojection error   r\\le ft ( \\ mathb f  {m}_{ ij}, \\mathbf {z}_{ij} \\right ) = || \\mathbf {m}_{ij} - \\Pi \\left (\\mathbf {z}_{ij}\\right ) ||^2  (2) where Π(x) = \u0010 x1 x3 , x2 x3 \u0011 , and the nonlinear least squares problem (1), referred to as Bundle Adjustment (BA) [15], can be solved iteratively using second-order methods like Levenberg-Marquardt [15, 41]. Given the sparsity of the problem, sparse computation methods [27] can be used in order to increase the efficiency of the optimization, allowing BA to be used even for scenes with a large number of views or points. However, it is widely known that BA is highly non-convex and tends to converge to the nearest local minimum when not initialized close to the globally optimal solution. As a consequence, BA is typically the last step of a reconstruction pipeline, preceeding global SfM methods such as [9, 10, 20, 25], or incremental SfM methods such as [2, 37, 38] that solve a sequence of subproblems like pairwise pose estimation, pose averaging, triangulation or camera resection [13, 15, 22, 32]. A different approach consists of projective factorization methods [8, 17, 18, 28, 39, 48] which factorize the 2m × n measurement matrix into two rank four matrices corresponding to the camera matrices and 3D points (in homogeneous coordinates). In particular, works like [17, 18, 48] allow initialization-free SfM given their wide basin of convergence, meaning that their methods can be initialized with random camera poses and still converge with a high rate of success to the desired global minimum. Even though these methods have been improving in terms of accuracy and robustness to missing data, factorization-based methods require the input data to be almost completely free of outliers which unfortunately cannot be guaranteed in most real world sequences or datasets, and hence severely compromises the usability of these methods. A common challenge with all these approaches to solve SfM is their scalability as the number of views and keypoints increase. Incremental SfM tries to tackle this issue by starting with a subset of the views, estimate its reconstruction and incrementally adding more views. Some factorization-based methods can also take advantage of the same sparse computation methods used in BA, which significantly improves their ability to scale with sequence size. While this allows to reconstruct scenes with thousands of views and millions of points, it can still take hours to recover the reconstruction of a single scene. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4808",
        "chosen": "This work presents GASFM, a graph attention network for learning Structure-from-Motion. The model takes 2D keypoints as input and outputs camera poses and 3D coordinates, offering faster reconstruction than COLMAP with competitive accuracy.",
        "rejected": "Using graph attention networks, this work replaces traditional sub-problems in Structure-from-Motion (SfM) with a learned model for fast inference of camera poses and 3D keypoints. Experimental results demonstrate superior performance compared to competing learning-based methods and challenge COLMAP while reducing runtime."
    },
    {
        "prompt": "summary: Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geographically diverse descriptive knowledge of categories can enhance robustness. For this purpose, we explore the feasibility of probing a large language model for geography-based object knowledge, and we examine the effects of integrating knowledge into zero-shot and learnable soft prompting with CLIP. Within this exploration, we propose geography knowledge regularization to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set. Accuracy gains over prompting baselines on DollarStreet while training only on Europe data are up to +2.8/1.2/1.6 on target data from Africa/Asia/Americas, and +4.6 overall on the hardest classes. Competitive performance is shown vs. few-shot target training, and analysis is provided to direct future study of geographical robustness. 1. Introduction The performance of object recognition models degrades when tested in new geographies (e.g., cities, countries, continents) [7, 21, 33, 39, 43]. Numerous factors contribute to the challenging problem of geographical domain shift, such as cross-geography changes in object design/parts, materials, and context. These changes in turn may be due to cultural, climate, or economic differences around the world. Recent work has shown standard adaptation techniques fail when used for geographical domain shifts [21, 33], but there has yet to be significant progress in the creation of techniques that improve geographical robustness. Such progress is necessary to ensure equitable use of AI in the future. Figure 1. Descriptive knowledge can address concept shifts across geographies. Observe the wide range of object designs and contexts in the DollarStreet [11] category tools around the world. Our work’s premise is that textual representations for classes in vision-language models can be enhanced to better suit diverse object representations across geographies. Map made with [16]. Overall, models need representations that adequately capture a category’s various forms around the world. A natural solution is to collect training data of objects from different regions. However, this approach is expensive, takes significant effort, and is difficult for regions with limited Internet access. Fortunately, geographical shifts have a unique property compared to other common domain shifts (e.g. ones due to artistic style or weather changes)—they can be addressed with descriptive knowledge about concept changes. In other words, it is possible to describe the features of an object in a region and use this information to adapt a model’s default representation. For instance, as shown in Fig. 1, for rural areas in Papua New Guinea, tools can be described as being used for “cooking, hunting, and fishing”, and for rural areas in Malawi, tools may often be “made of metal and wood, for farming”. Models should account for diverse presentations and contexts of a category and not be limited to biased presentations (e.g. if the model learns tools as just being “metallic with logos”). We examine the effects of probing geo-diverse knowledge in two ways. First, we analyze whether a visionThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 13515",
        "chosen": "The paper explores incorporating geo-diverse knowledge into prompting for geographical robustness in object recognition. Using a large language model for geography-based knowledge and integrating it with CLIP via soft prompting and regularization improves accuracy on target data from diverse regions.",
        "rejected": "The research develops an advanced framework combining photometric stereo and neural rendering for reconstructing textureless objects from sparse viewpoints. By integrating ambient and point-light illumination, it achieves detailed 3D reconstructions outside controlled environments, overcoming limitations associated with conventional photometric stereo techniques."
    },
    {
        "prompt": "summary: Despite noise and caption quality having been acknowledged as important factors impacting vision-language contrastive pre-training, in this paper, we show that the full potential of improving the training process by addressing such issues is yet to be realized. Specifically, we firstly study and analyze two issues affecting training: incorrect assignment of negative pairs, and low caption quality and diversity. Then, we devise effective solutions for addressing both problems, which essentially require training with multiple true positive pairs. Finally, we propose training with sigmoid loss to address such a requirement. We show very large gains over the current state-of-the-art for both image recognition (∼+6% on average over 11 datasets) and image retrieval (∼+19% on Flickr30k and ∼+15% on MSCOCO). 1. Introduction Large-scale contrastive image-text pre-training has emerged as the prevalent method for vision-language representation learning [14, 21, 27, 29, 38, 51, 52]. The majority of datasets employed for pre-training are web-collected [4, 10, 34, 40–43, 45]. They offer a varied data distribution and are sufficiently large to effectively train high-performing vision-language models. However, since the raw captions for each image are typically extracted from associated tags or descriptions, they often exhibit low quality, being noisy and suboptimal for training purposes [21, 27]. Although some attempts to fix such issues have been already described, to some extent, in literature (e.g. ALIP [50], BLIP [27]), in this work, we show that the full potential of improving the quality of the training process is far from being fully realized. Specifically, by studying and addressing specific issues related to noise and low data quality, in this work, we show that our improved vision-language training pipeline can achieve massive gains over the current stateof-the-art methods for both image recognition (∼+6% on Figure 1. Our approach, FFF, achieves state-of-the-art accuracy across multiple datasets, largely outperforming prior methods. average over 11 datasets) and image retrieval (∼+19% on Flickr30k [53] and ∼+15% on MSCOCO [30]). The first issue we study is related to noise impacting contrastive learning: near-duplicate samples which are incorrectly treated as negative pairs. Even within a batch, it is not uncommon to find images and/or captions that are semantically similar or even identical. Since standard contrastive learning assumes one positive pair, this significantly hinders the training process and the quality of the trained models. The second issue we study is related to low caption quality and diversity. Captions can be short and lacking detail, noisy, or even entirely irrelevant to the image. Moreover, since the mapping process between image and text is oneto-many, more than one caption is needed to provide an approximate description of the image. To fix issue one, we propose an algorithm that mines new positive pairs based on image-text, image-image, and texttext similarities, aiming to decrease the number of false negatives in the training data arising due to semantically similar images and/or captions. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 14172",
        "chosen": "FFF addresses noise and caption quality in vision-language contrastive pre-training by studying incorrect negative pair assignments and low caption quality/diversity. It proposes training with multiple true positive pairs using a sigmoid loss, achieving large gains over the state-of-the-art in image recognition and retrieval.",
        "rejected": "This study addresses two issues affecting vision-language pre-training: incorrect assignment of negative pairs and low caption quality. By introducing an algorithm that mines new positive pairs and proposes training with sigmoid loss, significant improvements in image recognition and retrieval are achieved. The approach enhances model accuracy by addressing false negatives and improving caption quality."
    },
    {
        "prompt": "summary: We consider a critical issue of false negatives in VisionLanguage Pre-training (VLP), a challenge that arises from the inherent many-to-many correspondence of image-text pairs in large-scale web-crawled datasets. The presence of false negatives can impede achieving optimal performance and even lead to a significant performance drop. To address this challenge, we propose MAFA (MAnaging FAlse negatives), which consists of two pivotal components building upon the recently developed GRouped mIni-baTch sampling (GRIT) strategy: 1) an efficient connection mining process that identifies and converts false negatives into positives, and 2) label smoothing for the image-text contrastive (ITC) loss. Our comprehensive experiments verify the effectiveness of MAFA across multiple downstream tasks, emphasizing the crucial role of addressing false negatives in VLP, potentially even surpassing the importance of addressing false positives. In addition, the compatibility of MAFA with the recent BLIP-family model is also demonstrated. Code is available at https://github.com/jaeseokbyun/MAFA. 1. Introduction With large-scale web-crawled datasets [3, 50–52], majorities of vision-language pre-training (VLP) models are trained in a self-supervised learning manner using the combination of several pre-tasks and losses [2, 33, 34, 63, 65]: e.g., masked language modeling (MLM), image-text contrastive (ITC), and image-text matching (ITM) losses. Despite their promising results, one of the pressing challenges they face is the presence of noisy captions in image-text pairs that often provide incomplete or even incorrect descriptions [9, 13, 41, 44, 47, 59, 64]. Several recent works have focused on addressing such issue of noisy correspondence in image-text pairs [11, 18, 19, 21, 34, 47]. Notably, BLIP [34] introduced a caption refinement process by leveraging an *Equal contribution †Corresponding author Figure 1. Examples of positives, negatives, and false negatives among image-text pairs. image captioning model and a filter to generate synthetic clean captions and remove noisy captions. Such process can be seen as correcting the false positives that were injected by the noisy captions. Contrastively, we note that there is another type of challenge for VLP that stems from the nature of many-to-many correspondence of image-text pairs. Namely, it is common for an image (resp. text) to have additional positive connections (blue lines in Figure 1) with another texts (resp. images), which are paired with their corresponding images (resp. texts). This is due to the fact that the existing imagetext datasets are constructed by only collecting paired imagetext instances, hence the information regarding non-paired but semantically close image-text combination can be missed. Consequently, for each image (resp. text), the text (resp. image) that is given as the pair with the image (resp. text) is treated as the only positive sample during pre-training, while the other texts (resp. images) are all treated as negatives. This setup inevitably leads to the prevalence of false negatives during computing ITC and ITM losses and conThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27314",
        "chosen": "MAFA tackles false negatives in Vision-Language Pre-training (VLP) through an efficient connection mining process and label smoothing for the image-text contrastive loss, building on the GRIT strategy. It demonstrates significant improvements across downstream tasks, highlighting the importance of addressing false negatives in VLP.",
        "rejected": "The paper introduces MAFA, designed to tackle false negatives in vision-language pre-training. It utilizes a connection mining process and label smoothing for ITC loss. Experiments show MAFA's effectiveness across multiple tasks, demonstrating its compatibility with recent models like BLIP, achieving superior performance by managing false negatives."
    },
    {
        "prompt": "summary: Nonlinearities are decisive in neural representation learning. Traditional Activation (Act) functions impose fixed inductive biases on neural networks with oriented biological intuitions. Recent methods leverage selfgated curves to compensate for the rigid traditional Act paradigms in fitting flexibility. However, substantial improvements are still impeded by the norm-induced mismatched feature re-calibrations (see Section 1), i.e., the actual importance of a feature can be inconsistent with its explicit intensity such that violates the basic intention of a direct self-gated feature re-weighting. To address this problem, we propose to learn discriminative neural feature Act with a novel prototype, namely, AdaShift, which enhances typical self-gated Act by incorporating an adaptive shift factor into the re-weighting function of Act. AdaShift casts dynamic translations on the inputs of a re-weighting function by exploiting comprehensive feature-filter context cues of different ranges in a simple yet effective manner. We obtain the new intuitions of AdaShift by rethinking the feature-filter relationships from a common Softmax-based classification and by generalizing the new observations to a common learning layer that encodes features with updatable filters. Our practical AdaShifts, built upon the new Act prototype, demonstrate significant improvements to the popular/SOTA Act functions on different vision benchmarks. By simply replacing ReLU with AdaShifts, ResNets can match advanced Transformer counterparts (e.g., ResNet-50 vs. Swin-T) with lower cost and fewer parameters. 1. Introduction Nonlinear Act functions are indispensable for the learning of discriminative neural features [2, 7, 11, 17, 33, 39, 39, 42]. Neuronal behaviors [24, 40] originate traditional Act models, e.g., Softplus [15] and ReLU [34], which are fixed and monotonic in calculations. To realize finer rectifications, recent works investigated self-gated-style Act funcTop-1 Accuracy (%) 81.2(+2.3) 81.2(+2.3) 80.3(+3.1) 80.3(+3.1) 80.6(+1.7) 80.6(+1.7) 68 70 72 74 76 78 79 80 81 82 75 76 77 78 79 77 78 79 80 81 78.9 78.9 77.2 79.9(+2.7) 79.9(+2.7) 77.5 78.0 78.1(+3.2) 78.1(+3.2) 77.2(+2.3) 77.2(+2.3) 74.9 76.5 76.1 73.9(+5.2) 73.9(+5.2) 72.2(+3.5) 72.2(+3.5) 68.7 70.4 70.0 69.4 75.7 N/A N/A N/A ResNet-26 ResNet-14 ResNet-50 ResNet-101 ResNet-26 ResNet-14 ResNet-50 ResNet-101 Pserf SMU Meta-Acon ReLU AdaShift-B AdaShift-MA Pserf SMU Meta-Acon ReLU AdaShift-B AdaShift-MA Figure 1. Comparison of our AdaShift-B and AdaShift-MA to the ReLU [34] baseline and popular/SOTA Act models [4, 5, 30] on ImageNet [13] with ResNet backbones, where the areas of the circular patterns represent the relative amount of parameters compared to the corresponding ReLU baselines. Our AdaShift-B and AdaShift-MA improve different activation functions consistently and remarkably on different backbones varying by size with negligible parameters added to the ReLU baselines. tions based on the general prototype  \\la b e l { b ase_form} \\phi \\left (x\\right )=\\varsigma \\left (x\\right )x\\,,  (1) where x P R is a given feature unit (i.e., scalar), ϕ : R Ñ R denotes the applied Act function of x, and ς : R Ñ R defines the re-weighting function of ϕ . As a special case, ReLU can be included in this prototype by specifying ς pxq as a binary masking of 0 and 1 for x ď 0 and x ą 0, respectively. Despite the broad applicability, ReLU leaves two practical constraints on neural Act from (1) its rigid masking on positive features, i.e., unified weight assignments that possibly neutralize the discriminativeness, and (2) hard-zero-truncation on negative features that possibly leads to the “dead tensors” problem. Recent methods addressed these by introducing smooth re-weighting functions with two assumed properties: 1. ς pxq is bounded (typically, ς pxq P p0, 1q); 2. ς pxq is monotonically non-decreasing about x . These properties theoretically ensure the stability and conThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 5947",
        "chosen": "AdaShift enhances traditional self-gated activation functions by incorporating an adaptive shift factor into the re-weighting function. This approach, which exploits comprehensive feature-filter context cues, demonstrates significant improvements over popular activation functions on various vision benchmarks when integrated with ResNet backbones.",
        "rejected": "To address feature recalibration mismatches, AdaShift is proposed, incorporating an adaptive shift factor into activation functions. This method enhances neural networks' discriminative power without significantly increasing parameters or costs, leading to improved performance on various benchmarks."
    },
    {
        "prompt": "summary: Representing signals using coordinate networks dominates the area of inverse problems recently, and is widely applied in various scientific computing tasks. Still, there exists an issue of spectral bias in coordinate networks, limiting the capacity to learn high-frequency components. This problem is caused by the pathological distribution of the neural tangent kernel’s (NTK’s) eigenvalues of coordinate networks. We find that, this pathological distribution could be improved using the classical batch normalization (BN), which is a common deep learning technique but rarely used in coordinate networks. BN greatly reduces the maximum and variance of NTK’s eigenvalues while slightly modifies the mean value, considering the max eigenvalue is much larger than the most, this variance change results in a shift of eigenvalues’ distribution from a lower one to a higher one, therefore the spectral bias could be alleviated (see Fig. 1). This observation is substantiated by the significant improvements of applying BN-based coordinate networks to various tasks, including the image compression, computed tomography reconstruction, shape representation, magnetic resonance imaging and novel view synthesis. 1. Introduction Coordinate networks, which take the coordinates as inputs and output the signal attributes using multi-layer perceptron (MLP) models, have become a promising framework for solving various inverse problems. Different from the classical convolution-based networks which could only support up to 3D patterns as input [20, 57], the input coordinates are organized as 1D vectors in coordinate networks, enabling the advantage of a general framework for solving inverse problems with any dimensions. Therefore, coordinate networks have been widely applied in different areas of scientific computing [30], such as the hologram/tomography *This work was supported by the National Natural Science Foundation of China under Grants T2221003, 62071216 and the Leading Technology of Jiangsu Basic Research Plan (BK20192003). Eigenvalues Count Eigenvalues Eigenvalues (a) Histograms of NTK Eigenvalues (b) Reconstructed images 23.25 dB 27.18 dB 28.79 dB 30.05 dB 30.84 dB 31.31 dB without BN with BN MLP PEMLP-1 PEMLP-5 Figure 1. Batch normalization significantly alleviates the spectral bias of coordinate networks. (a) Batch normalization shifts the NTK’s eigenvalues distribution from a lower one to a higher one, thus (b) the spectral bias is alleviated and better performance is achieved compared with the one without batch normalization (e.g., the texture on the lion’s left paw). From left to right, each column refers to the coordinate networks with ReLU activations, and positional encoding [78] with 1 and 5 Fourier bases, respectively. imaging in microscopy [39, 83], 3D reconstruction and freeviewpoint roaming in computer vision/graphics [37, 48], physical simulation in material design and hydrodynamics [8, 59] and medical imaging [67, 68]. Yet, due to the spectral bias [58] of ReLU-based MLP, coordinate networks prefer to learn the low-frequency components of the signal, while the high-frequency components are learned at an extremely slow convergence. Several works have been proposed to alleviate the spectral bias, such as the positional encoding [78] or frequency-related activation functions [60, 72]. However, these explorations introduce the ‘frequency-specified spectral bias’ [81], i.e., This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25160",
        "chosen": "This paper shows that batch normalization can alleviate the spectral bias in coordinate networks by shifting the distribution of the neural tangent kernel's eigenvalues. This results in significant improvements in tasks like image compression and tomography reconstruction, where coordinate networks are widely applied.",
        "rejected": "Batch normalization (BN) is explored to alleviate spectral bias in coordinate networks, enhancing learning of high-frequency components. BN modifies the distribution of NTK eigenvalues, improving performance in tasks such as image compression and computed tomography reconstruction."
    },
    {
        "prompt": "summary: In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology principles, and emulates autonomy by initiating dialogue topics; 2) MoMat-MoGen: a text-driven motion synthesis paradigm for controlling the character’s digital body. It integrates motion matching, a proven industry technique to ensure motion quality, with cutting-edge advancements in motion generation for diversity. Extensive experiments demonstrate that each module achieves state-of-the-art performance in its respective domain. Collectively, they enable virtual characters to initiate and sustain dialogues autonomously, while evolving their socio-psychological states. Concurrently, these characters can perform contextually relevant This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 582",
        "chosen": "The Digital Life Project presents a framework for autonomous 3D characters with social intelligence, featuring SocioMind (a digital brain modeling personalities) and MoMat-MoGen (a text-driven motion synthesis paradigm). This enables characters to engage in autonomous social interactions and express with articulated body motions.",
        "rejected": "The Digital Life Project framework uses language to build autonomous 3D characters capable of social interaction and articulated body motions. SocioMind models personalities and initiates dialogues, while MoMat-MoGen synthesizes motion. Together, they enable virtual characters to autonomously engage in dialogue and evolve socio-psychological states, demonstrating state-of-the-art performance in their domains."
    },
    {
        "prompt": "summary: Traditional 3D content creation tools empower users to bring their imagination to life by giving them direct control over a scene’s geometry, appearance, motion, and camera path. Creating computer-generated videos, however, is a tedious manual process, which can be automated by emerging text-to-video diffusion models. Despite great promise, video diffusion models are difficult to control, hindering a user to apply their own creativity rather than amplifying it. To address this challenge, we present a novel approach that combines the controllability of dynamic 3D meshes with the expressivity and editability of emerging diffusion models. For this purpose, our approach takes an animated, low-fidelity rendered mesh as input and injects the ground truth correspondence information obtained from the dynamic mesh into various stages of a pre-trained text-to-image generation model to output high-quality and temporally consistent frames. We demonstrate our approach on various examples where motion can be obtained by animating rigged assets or changing the camera path. Project page: primecai.github.io/ generative rendering. △: Part of this work was done during an internship at Adobe Research. * : Equal contribution. 1. Introduction Artists, designers, architects, and other creators rely on traditional 3D content creation tools to render computergenerated videos. Unfortunately, existing 3D workflows are laborious, time consuming, and require expertise. Emerging generative artificial intelligence tools, such as text-to-image (T2I) and text-to-video (T2V) models, solve these issues by automating many of the manual steps of traditional workflows. Video generation, however, is difficult to control in that it is not easily possible to specify scene layout and motion in a temporally consistent manner. Recent approaches have attempted to control diffusion models. For example, ControlNet [39] uses a pre-trained T2I diffusion model and finetunes an adapter network that is conditioned on depth, pose, or edge images to control the layout. This strategy is successful for generating individual frames, but results in flicker for video generation. Other approaches aim at learning the complex types of motions encountered in natural videos directly [3, 9, 11–13, 21, 33, 35,38,40]. While successful in generating smooth motions, these approaches are not easily controllable. Finally, videoto-video diffusion models [6,10,20,37] enable video editing and stylization, but they require a high-fidelity video as input, which is not always available. In this paper, we aim to combine the power of 3D workflows with T2I models for generating 4D-guided stylized This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 7611",
        "chosen": "This novel approach combines the controllability of dynamic 3D meshes with the expressivity of text-to-image diffusion models. By injecting ground truth correspondence information from dynamic meshes into pre-trained text-to-image models, it generates high-quality, temporally consistent frames, enabling contextually relevant character motions.",
        "rejected": "A novel approach combining dynamic 3D meshes with text-to-image generation models is presented. This method injects ground truth correspondence information into stages of a pre-trained model, producing high-quality, temporally consistent frames from animated low-fidelity rendered meshes, enhancing control and creativity in video generation."
    },
    {
        "prompt": "summary: In the current era of generative AI breakthroughs, generating panoramic scenes from a single input image remains a key challenge. Most existing methods use diffusion-based iterative or simultaneous multi-view inpainting. However, the lack of global scene layout priors leads to subpar outputs with duplicated objects (e.g., multiple beds in a bedroom) or requires time-consuming human text inputs for each view. We propose L-MAGIC, a novel method leveraging large language models for guidance while diffusing multiple coherent views of 360◦panoramic scenes. L-MAGIC harnesses pre-trained diffusion and language models without ﬁne-tuning, ensuring zero-shot performance. The output quality is further enhanced by super-resolution and multiview fusion techniques. Extensive experiments demonstrate that the resulting panoramic scenes feature better scene layouts and perspective view rendering quality compared to related works, with >70% preference in human evaluations. Combined with conditional diffusion models, L-MAGIC can accept various input modalities, including but not limited to text, depth maps, sketches, and colored scripts. Applying depth estimation further enables 3D point cloud generation and dynamic scene exploration with ﬂuid camera motion. Code is available at https://github.com/ ZhipengCai/L-MAGIC-code-release. 1. Introduction Diffusion models have achieved state-of-the-art performance in image generation. However, generating a 360◦ panoramic scene from a single perspective image remains a challenge, which is an important problem in many computer vision applications, such as architecture design, movie scene creation, and virtual reality (VR). Training a model to directly generate panoramic images is challenging due to the lack of diverse large-scale datasets. Hence, most existing works separate panoramic scenes into multiple perspective views, and inpaint them us*Corresponding author (zhipeng.cai@intel.com) L-MAGIC Depth  estimation Figure 1. Teaser. L-MAGIC is a novel method to generate a 360◦ panoramic scene from a single input image. L-MAGIC utilizes large language models to control perspective diffusion models to generate multiple views with coherent 360◦layout. L-MAGIC is also compatible with images synthesized by conditional generative models, making it capable of creating panoramic scenes from various input modalities. A set of perspective images rather than a single panoramic image also allows our method to leverage offthe-shelf monocular depth estimation models to enable immersive experiences, e.g., scene ﬂy-through or 3D point cloud generation. ing pre-trained diffusion models. To ensure generalization, the diffusion model is either frozen without any architecture change [11] or combined with extra modules trained on small datasets for integrating multi-view information [25]. A common approach to encode the scene information during multi-view inpainting is to provide a textconditioned diffusion model with the description of the input image, which is generated by a user or an image captioning model [13]. Though effective for extending local scene content, such approaches suffer from incoherence in the overall scene layout. Speciﬁcally, using the same text description for diffusing different views along the 360◦ panorama leads to artifacts and unnecessarily repeated objects. Current inpainting methods have no mechanism to leverage global scene information in individual views. In this work, we show that state-of-the-art (vision) language models, without ﬁne-tuning, can be used to control multi-view diffusion and effectively address the above problem. We propose L-MAGIC (Fig. 1), a novel framework This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 7049",
        "chosen": "L-MAGIC leverages large language models to guide the diffusion of multiple coherent views for generating 360° panoramic scenes. It ensures zero-shot performance by utilizing pre-trained models without fine-tuning and enhances output quality with super-resolution and multi-view fusion techniques.",
        "rejected": "L-MAGIC leverages large language models to guide diffusion models for generating coherent views of 360-degree panoramic scenes. It achieves enhanced scene layouts and perspective rendering quality compared to related methods. L-MAGIC supports various input modalities and enables 3D point cloud generation, offering fluid camera motion and immersive experiences."
    },
    {
        "prompt": "summary: Camera-parameter-free multi-view pose estimation is an emerging technique for 3D human pose estimation (HPE). They can infer the camera settings implicitly or explicitly to mitigate the depth uncertainty impact, showcasing significant potential in real applications. However, due to the limited camera setting diversity in the available datasets, the inferred camera parameters are always simply hardcoded into the model during training and not adaptable to the input in inference, making the learned models cannot generalize well under unseen camera settings. A natural solution is to artificially synthesize some samples, i.e., 2D3D pose pairs, under massive new camera settings. Unfortunately, to prevent over-fitting the existing camera setting, the number of synthesized samples for each new camera setting should be comparable with that for the existing one, which multiplies the scale of training and even makes it computationally prohibitive. In this paper, we propose a novel HPE approach under the invariant risk minimization (IRM) paradigm. Precisely, we first synthesize 2D poses from myriad camera settings. We then train our model under the IRM paradigm, which targets at learning a common optimal model across all camera settings and thus enforces the model to automatically learn the camera parameters based on the input data. This allows the model to accurately infer 3D poses on unseen data by training on only a handful of samples from each synthesized setting and thus avoid the unbearable training cost increment. Another appealing feature of our method is that benefited from the capability of IRM in identifying the invariant features, its performance on the seen camera settings is enhanced as well. Comprehensive experiments verify the superiority of our approach. 1. Introduction 3D multi-view Human Pose Estimation (HPE) leverages the camera relationship between multiple viewpoint to mitigate *Corresponding authors the impact of depth uncertainty. Existing methods primarily rely on camera parameters to construct epipolar geometric constraints between camera viewpoints. Unlike cameraparameter-required methods, camera-parameter-free methods can explicitly or implicitly recover camera parameters during training, thus making them applicable in broader scenarios where camera parameters are unavailable, such as HPE in uncontrolled environments or dynamic HPE with moving cameras. HPCP [21] leverages human pose prior such as bone length to optimize potential camera parameters. Flex [4] models viewpoint-consistent 3D poses by hierarchical skeletal representation. MTF-Transformer [20] leverages temporal information to obtain more accurate camera parameters. Probabilistic Triangulation [11] adopts Monte Carlo sampling to select the camera parameters. These methods have achieved commendable results, closely matching the performance of camera-parameter-free methods under seen camera setting. However, when generalized to unseen camera settings, camera-parameter-free methods exhibit a great performance drop, in contrast the drop in the camera-parameter-required methods is negligible [20]. We argue that this discrepancy primarily stems from the reason that the inferred camera parameters in the camer-parameter-free methods are always simply hardcoded into the models during training, i.e., they are not adaptable to the input in inference. To be precise, the number of camera viewpoints within existing datasets is rather limited (typically four or eight), whereas previous mainstream models require at least four camera viewpoints as input to address the challenges such as self-occlusion of the human body or inaccuracies in 2D pose estimation, making the number of available camera settings no larger than two. With such extremely limited diversity of camera settings, because of the training imbalance, cameraparameter-free methods tend to memorize the specific camera settings rather than to generalize to arbitrary settings, thus significantly hindering their generalization capabilities across varied camera settings. Our experiments have substantiated this conjecture. Please refer to Tab. 5 for details. A natural solution to address the above challenge is to arThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 2124",
        "chosen": "PoseIRM enhances 3D human pose estimation on unseen camera settings using invariant risk minimization. It synthesizes 2D poses from various camera settings and trains a model to learn a common optimal solution, allowing accurate 3D pose inference on unseen data with minimal samples per setting.",
        "rejected": "A novel HPE approach under the invariant risk minimization (IRM) paradigm is introduced. It synthesizes 2D poses from myriad camera settings and trains the model to learn common optimal parameters across all settings. This allows accurate inference of 3D poses on unseen data, enhancing generalization capabilities and performance on both seen and unseen camera settings."
    },
    {
        "prompt": "summary: Synthesizing realistic videos of talking faces under custom lighting conditions and viewing angles benefits various downstream applications like video conferencing. However, most existing relighting methods are either time-consuming or unable to adjust the viewpoints. In this paper, we present the first real-time 3D-aware method for relighting in-thewild videos of talking faces based on Neural Radiance Fields (NeRF). Given an input portrait video, our method can synthesize talking faces under both novel views and novel lighting conditions with a photo-realistic and disentangled 3D representation. Specifically, we infer an albedo tri-plane, as well as a shading tri-plane based on a desired lighting condition for each video frame with fast dualencoders. We also leverage a temporal consistency network to ensure smooth transitions and reduce flickering artifacts. Our method runs at 32.98 fps on consumer-level hardware and achieves state-of-the-art results in terms of reconstruction quality, lighting error, lighting instability, temporal consistency and inference speed. We demonstrate the effectiveness and interactivity of our method on various portrait videos with diverse lighting and viewing conditions. 1. Introduction Portrait videos are widely used in various scenarios, such as video conferencing, video editing, entertainment, virtual reality, etc. However, many portrait videos are captured under unsatisfactory conditions, such as environments that are either too dark or too bright, or with virtual backgrounds that do not match the lighting of the foreground. These factors degrade the visual quality and realism of videos and affect the user experience. Of particular significance is the context of augmented re*Corresponding author is Lin Gao Yaw = 0.3 Front Albedo Geometry Lighting Lighting Figure 1. Given a portrait video shown in the leftmost column, our method reconstructs a 3D relightable face for each video frame. Users can then adjust their viewpoints and lighting conditions interactively. The second column displays relighted video frames with a head pose yaw of 0.3, while the third column presents faces relighted under an alternative lighting condition with a frontal head pose. The rightmost column provides the predicted albedo and geometry of the reconstructed face. Please see the supplementary video for the full results. ality (AR) and virtual reality (VR) applications, where users often seek to create 3D faces that can be dynamically relighted to fit the environment. This dynamic relighting capability becomes possible only when the underlying method is inherently 3D-aware and operates in real time. However, 3D-aware portrait video relighting is a challenging task, since it involves modeling the complex interactions between the light, geometry, and appearance of human faces, as well as ensuring the temporal coherence and naturalness of synthesized videos. It is even more challengThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 6221",
        "chosen": "This real-time 3D-aware method for portrait video relighting uses Neural Radiance Fields (NeRF) to synthesize talking faces under novel views and lighting conditions. It infers albedo and shading tri-planes with fast dual-encoders and ensures temporal consistency, achieving state-of-the-art results in reconstruction quality and inference speed.",
        "rejected": "A real-time 3D-aware method for relighting talking face videos based on Neural Radiance Fields (NeRF) is proposed. It synthesizes realistic talking faces under novel views and lighting conditions, ensuring smooth transitions and reducing flickering artifacts. Running at 32.98 fps, it achieves state-of-the-art results in reconstruction quality, temporal consistency, and speed."
    },
    {
        "prompt": "summary: While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary (free-form) visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a “red bounding box” or “pointed arrow”. Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on regionunderstanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available. 1. Introduction Large language models (LLMs) like ChatGPT [21], GPT4 [22], and Bard [9] have recently gained significant attention for their strong reasoning and generalization capabilities, and their ability to chat in a human-like manner. In particular, models such as GPT-4V(ision) [20], which incorporate visual information, have demonstrated humanlevel perception and reasoning capabilities [36]. This has spurred the development of similar open-source models that aim to replicate or even surpass the proprietary models’ performance. Despite their capabilities, current models, including seminal ones like LLaVA [14, 15] and MiniGPT-4 [42], focus predominantly on whole-image understanding; in other words, they lack the capability to process region-specific information in complex scenes. This limitation becomes parVisual Prompt Text Prompt : What is the  person marked  with the red arrow  holding? Large Multimodal Model : The person marked with the red arrow is holding a  green flag. This flag is used for … Figure 1. Main Idea of ViP-LLaVA. We directly overlay diverse visual prompts (e.g., arrows, boxes, circles, scribbles) onto the original image, and then feed the corresponding visual features along with text embeddings into the large multimodal model for conversational assistance. Here we show an example using a red arrow. ticularly apparent when attempting to describe specific objects within an image using only language prompts, which can be difficult when there is ambiguity (e.g., when there are multiple people in the image, and the question relates to a specific person), as shown in Figure 1. To address this issue, recent work explores spatial references in multimodal models. Existing efforts have primarily focused on using textual representations of coordinates [3, 4, 7, 39], learned positional embeddings [23, 38, 41], or ROI features [26, 38]. However, they often lack userfriendliness, as they are limited to fixed-format visual references like bounding boxes and the spatial coordinates of a mask contour. Most of these approaches, including those by Zhang et al. [38] and Chen et al. [4], only employ bounding box inputs for visual referrals. While effective in structured scenarios, this method proves less versatile in natural, user-driven interactions where the visual prompts may not conform to clean geometric shapes. In this paper, we propose a simple yet highly effective solution to this problem: a large multimodal model that can This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12914",
        "chosen": "ViP-LLaVA introduces a multimodal model capable of decoding arbitrary visual prompts, allowing users to interact with the model using natural cues like arrows or bounding boxes. This approach achieves state-of-the-art performance on region-understanding tasks and includes ViP-Bench, a benchmark for assessing visual prompt understanding.",
        "rejected": "This paper introduces a novel multimodal model capable of decoding free-form visual prompts, enabling intuitive image interaction using natural cues like \"red bounding boxes\" or \"pointed arrows.\" By overlaying visual markers directly onto RGB images, the method simplifies user interaction without complex region encodings. It achieves state-of-the-art performance on region-understanding tasks such as Visual7W, PointQA, and Visual Commonsense Reasoning benchmarks. Additionally, the authors present ViP-Bench, a benchmark for evaluating models' capabilities in understanding visual prompts across multiple dimensions. This facilitates future research and advancements in user-friendly visual prompting techniques."
    },
    {
        "prompt": "summary: Neural Radiance Fields (NeRFs) have shown great potential in novel view synthesis. However, they struggle to render sharp images when the data used for training is affected by motion blur. On the other hand, event cameras excel in dynamic scenes as they measure brightness changes with microsecond resolution and are thus only marginally affected by blur. Recent methods attempt to enhance NeRF reconstructions under camera motion by fusing frames and events. However, they face challenges in recovering accurate color content or constrain the NeRF to a set of predefined camera poses, harming reconstruction quality in challenging conditions. This paper proposes a novel formulation addressing these issues by leveraging both modeland learning-based modules. We explicitly model the blur formation process, exploiting the event double integral as an additional model-based prior. Additionally, we model the event-pixel response using an end-to-end learnable response function, allowing our method to adapt to nonidealities in the real event-camera sensor. We show, on synthetic and real data, that the proposed approach outperforms existing deblur NeRFs that use only frames as well as those that combine frames and events by +6.13dB and +2.48dB, respectively. Multimedial Material: For videos, datasets and more visit https://github.com/uzh-rpg/evdeblurnerf. 1. Introduction Neural Radiance Fields (NeRFs) [27] have completely revolutionized the field of 3D reconstruction and novel view synthesis, achieving unprecedented levels of details [2, 3, 43]. As a result, they have quickly found applications in many subfields of computer vision and robotics, such as pose estimation and navigation [36, 53, 59], image processing [12, 24, 28, 47], scene understanding [17, 22, 51], surface reconstruction [1, 48, 54], and many others. Leveraging multi-view consistency from calibrated images, NeRF exploits supervision from multiple view-points, enabling generalization to novel camera poses and the ability to render view-dependent color effects [43]. However, Motion-aware NeRF event CRF Ev-DeblurNeRF Blurry Images Events Reconstructions Figure 1. Ev-DeblurNeRF combines blurry images and events to recover sharp NeRFs. A motion-aware NeRF recovers camera motion and a learnable event camera response function models real camera’s non-idealities, enabling high-quality reconstructions. akin to other methods relying on photometric consistency, NeRF can only deliver high-quality reconstructions when the images used for training are perfectly captured and free from any artifact. Unfortunately, perfect conditions are seldom met in the real world. For example, in robotics, camera motion is prevalent when capturing images, often resulting in motion blur. Under such conditions, NeRFs are unable to reconstruct sharp radiance fields, thereby impeding their practical use in realworld scenes. Although recent works [6, 18, 24, 49] have shown promising results in reconstructing radiance fields from motion-blurred images by learning to infer the camera motion during the exposure time, the task of recovering motion-deblurred NeRFs still remains significantly illposed. Existing image-based approaches typically fail when training images exhibit similar and consistent motion [24], and they are inherently limited by the presence of motion ambiguities and loss of texture details that cannot be recovered from blurry images alone. In this regard, recent works have shown that event-based cameras can substantially aid the task of deblurring images captured with standard cameras [33, 37, 45, 56]. These sensors measure brightness changes at microseconds resolution and are practically unaffected by motion blur [11], This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 9286",
        "chosen": "Ev-DeblurNeRF addresses motion blur in NeRF reconstructions by fusing frames and events. It models the blur formation process using event double integral as a prior and employs a learnable event-pixel response function. This method outperforms existing deblur NeRFs, achieving significant improvements on synthetic and real data.",
        "rejected": "This paper addresses the issue of motion blur in Neural Radiance Fields (NeRFs) by leveraging both model- and learning-based modules, explicitly modeling the blur formation process using event cameras. The proposed method outperforms existing techniques by significant margins."
    },
    {
        "prompt": "summary: Radiologists highly desire fully automated versatile AI for medical imaging interpretation. However, the lack of extensively annotated large-scale multi-disease datasets has hindered the achievement of this goal. In this paper, we explore the feasibility of leveraging language as a naturally high-quality supervision for chest CT imaging. In light of the limited availability of image-report pairs, we bootstrap the understanding of 3D chest CT images by distilling chest-related diagnostic knowledge from an extensively pre-trained 2D X-ray expert model. Speciﬁcally, we propose a language-guided retrieval method to match each 3D CT image with its semantically closest 2D X-ray image, and perform pair-wise and semantic relation knowledge distillation. Subsequently, we use contrastive learning to align images and reports within the same patient while distinguishing them from the other patients. However, the challenge arises when patients have similar semantic diagnoses, such as healthy patients, potentially confusing if treated as negatives. We introduce a robust contrastive learning that identiﬁes and corrects these false negatives. We train our model with over 12K pairs of chest CT images and radiology reports. Extensive experiments across multiple scenarios, including zero-shot learning, report generation, and ﬁne-tuning processes, demonstrate the model’s feasibility in interpreting chest CT images. 1. Introduction Understanding medical images is crucial for precise clinical diagnosis, with doctors routinely engaging in multi-disease *Correspondence to Jianpeng Zhang and Jian Zheng. The work was done during Weiwei Cao’s internship at DAMO Academy. Model Supervision Functionality Single-disease Nodule Emphysema Inflammation Calcification … Ours Nodule Emphysema Inflammation Calcification … Nodule Emphysema Inflammation Calcification … + Multi-disease Figure 1. Models tailored for speciﬁc diseases demand doctors to annotate each image. Creating a multi-disease model involves more time and effort for comprehensive data annotation. In contrast, our model learns to diagnose various diseases from both images and reports, eliminating the need for additional annotations. detection and diagnosis in imaging scans. Deep learning models require extensive datasets with fully annotated diseases for comprehensive training [19, 24, 28]. In practice, the prevailing approach to developing a diagnostic model typically concentrates on a speciﬁc disease, as illustrated in Figure 1. This workﬂow results in models with limited usage, failing to be applied to other diseases. To augment the capability for a new disease, a signiﬁcant additional effort is needed for data collection, annotation, and model training, which is not scalable, as there might be hundreds of diseases and tasks. Therefore, there is a strong anticipation for a new model development paradigm beyond the current labor-intensive annotation and task-speciﬁc training. Imaging diagnostic reports, the core output of radiologists’ intellectual work, inherently offer disease-level labels. The challenge lies in effectively leveraging this text data to enhance medical image understanding. Recent advances in vision-language pretraining (VLP) have yielded valuable insights [2, 11, 20, 22, 27]. Contrastive learning, a powerful technique in VLP, effectively bridges the image and text modalities. This method uniﬁes 1 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11238",
        "chosen": "This paper explores using language as supervision for chest CT imaging by leveraging knowledge from a 2D X-ray expert model. A language-guided retrieval method matches CT images with semantically closest X-ray images, performing knowledge distillation and contrastive learning. The model is trained on over 12K pairs of chest CT images and radiology reports, demonstrating feasibility in interpreting chest CT images across multiple scenarios.",
        "rejected": "This work explores using language as supervision for interpreting chest CT images, distilling knowledge from 2D X-ray images via contrastive learning to understand 3D chest CT scans without extensive annotations. It demonstrates feasibility across various scenarios."
    },
    {
        "prompt": "summary: Diffusion model-based image restoration (IR) aims to use diffusion models to recover high-quality (HQ) images from degraded images, achieving promising performance. Due to the inherent property of diffusion models, most existing methods need long serial sampling chains to restore HQ images step-by-step, resulting in expensive sampling time and high computation costs. Moreover, such long sampling chains hinder understanding the relationship between inputs and restoration results since it is hard to compute the gradients in the whole chains. In this work, we aim to rethink the diffusion model-based IR models through a different perspective, i.e., a deep equilibrium (DEQ) fixed point system, called DeqIR. Specifically, we derive an analytical solution by modeling the entire sampling chain in these IR models as a joint multivariate fixed point system. Based on the analytical solution, we can conduct parallel sampling and restore HQ images without training. Furthermore, we compute fast gradients via DEQ inversion and found that initialization optimization can boost image quality and control the generation direction. Extensive experiments on benchmarks demonstrate the effectiveness of our method on typical IR tasks and real-world settings. 1. Introduction Image restoration (IR) aims at recovering a high-quality (HQ) image from a degraded input. Recently, diffusion models [37, 61] are attracting great attention because they can generate higher quality images than GANs [23] and likelihood-based models [44]. Based on diffusion models [37, 61], many IR methods [20, 41, 68] achieve compelling performance on different tasks. Directly using diffusion models in IR, however, suffers from some limitations. First, diffusion model-based image restoration (DMIR) models rely on a long sampling chain to synthesize HQ images step-by-step, as shown in Figure 2 (a). As a result, it will lead to expensive sampling time during the infer*Corresponding author. DPS DDRM DDNM Ours (a) 4x SR (ImageNet) 19 22 25 28 PSNR 0.2 0.3 0.4 0.5 LPIPS DPS DDRM DDNM Ours (b) Deblurring (ImageNet) 20 25 30 35 40 PSNR 0.0 0.2 0.4 LPIPS DDRM PaletteRePaint Ours (c) Inpainting (CelebA) 37 38 39 40 PSNR 0.0 0.1 0.2 0.3 LPIPS DGP DDRM DDNM Ours (d) Colorization (CelebA) 40 50 60 70 80 FID 0.1 0.2 0.3 LPIPS Figure 1. Comparisons of different zero-shot DMIR methods in various IR applications on different datasets. ence. For example, DPS [20] based on DDPM [37] needs 1k sampling steps. To accelerate the sampling, some DMIR methods [41, 68, 87] use DDIM [61] to make a trade-off between computational cost and the restoration quality. Based on this, these methods can reduce sampling steps to 100 or even fewer. Unfortunately, it may degrade the sample quality when reducing the sampling steps [52]. It raises an interesting question: is it possible to develop an alternative sampling method without sacrificing the sample quality? Second, the long sampling chain makes understanding the relationship between the restoration and inputs difficult. In practice, sampling different Gaussian noises as inputs may have diverse results for some IR tasks (e.g., inpainting and colorization). Such diversity is not necessary for some IR tasks, e.g., super-resolution (SR) or deblurring. Nevertheless, different initializations may affect the quality of SR and deblurring. It raises the second question: is it possible to optimize the initialization such that the generation can be improved or controlled? However, it is difficult for existing methods to compute the gradient along the long sampling chain as they require storing the entire computational graph. In this paper, we rethink the sampling process in IR from a deep equilibrium (DEQ) based on [57]. Specifically, we first derive a proposition to model the sampling chain as a fixed point system, achieving parallel sampling. Then, we use a DEQ solver to find the fixed point of the sampling chain. Last, we use modern automatic differentiation packages to compute the gradients with backpropagating and understand the relationship between input noise and restoration. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 2824",
        "chosen": "DeqIR rethinks diffusion model-based image restoration through a deep equilibrium fixed point system. It models the sampling chain as a joint multivariate fixed point system, enabling parallel sampling and restoring high-quality images without training. The method computes fast gradients via DEQ inversion and optimizes initialization, showing effectiveness on various IR tasks.",
        "rejected": "DeqIR is introduced as a deep equilibrium fixed point system for diffusion model-based image restoration. It enables parallel sampling and restores high-quality images efficiently, showing better performance than traditional methods on multiple image restoration tasks."
    },
    {
        "prompt": "summary: We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body’s shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the generation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deformation field. This facilitates the generation of more complete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face “Janus” problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing methods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation. 1. Introduction The creation of 3D graphical human models has received great attention in recent years due to its wide-ranging applications in fields such as film-making, video games, AR/VR, and human-robotic interaction. Traditional methods for building such complex 3D models require thousands of man-hours of trained artists and engineers [10, 12], making the process both time-consuming and highly expert*Equal contribution †Corresponding authors ‡Webpage: https: //yukangcao.github.io/DreamAvatar/ Acknowlegement: This work is partially supported by Hong Kong Research Grant Council - Early Career Scheme (Grant No. 27208022) and HKU Seed Fund for Basic Research. dependent. With the development of deep learning methods, we have witnessed the emergence of promising methods [5, 54, 61] which can reconstruct 3D human models from monocular images. These techniques, however, still face challenges in fully recovering details from the input images and rely heavily on the training dataset. To tackle these challenges and simplify the modeling process, adopting generative models for 3D human avatar modeling has recently received increasing attention from the research community. This approach has the potential to alleviate the need for large 3D datasets and facilitate easier and more accessible 3D human avatar modeling. To leverage the potential of 2D generative image models for 3D content generation, recent methods [8, 29, 31, 35, 46] have utilized pretrained text-guided image diffusion models to optimize 3D implicit representations (e.g., NeRFs [37] and DMTet [40, 56]). DreamFusion [46] introduces a novel Score Distillation Sampling (SDS) strategy to selfsupervise the optimization process and achieves promising results. However, human bodies, which are the primary focus of this paper, exhibit a complex articulated structure, with head, arms, hands, trunk, legs, feet, etc., each capable of posing in various ways. As a result, while DreamFusion [46] and subsequent methods (e.g., Magic3D [29], ProlificDreamer [58], Fantasia3D [8]) produce impressive results, they lack the proper constraints to enforce consistent 3D human structure and often struggle to generate detailed textures for 3D human avatars. Latent-NeRF [35] introduces a sketch-shape loss based on the 3D shape guidance, but it still faces challenges in generating reasonable results for human bodies. In this paper, we present DreamAvatar, a novel framework for generating high-quality 3D human avatars from text prompts and shape priors. Inspired by previous works [29, 46], DreamAvatar employs a trainable NeRF as the base representation for predicting density and color values for each 3D point. Coupled with pretrained textto-image diffusion models [50, 68], DreamAvatar can be trained to generate 3D avatars using 2D self-supervision. The key innovation of DreamAvatar lies in three main aspects. Firstly, we leverage the SMPL model [32] to provide This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 958",
        "chosen": "DreamAvatar is a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. It uses a trainable NeRF and pretrained text-to-image diffusion models, leveraging the SMPL model for shape and pose guidance. The dual-observation-space design and joint optimization of losses improve texture completeness and facial details, outperforming existing methods.",
        "rejected": "DreamAvatar generates high-quality 3D human avatars guided by text and shape priors, utilizing trainable NeRF and pretrained text-to-image diffusion models. It achieves complete textures and geometry faithful to target poses, significantly improving facial details."
    },
    {
        "prompt": "summary: We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based priors enable more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent sets instead of using global latent codes. This novel 4D representation allows us to learn local shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporallycoherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid computational overhead, we designed *Equal Contribution. †Corresponding author. ‡Work done during master’s thesis. an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations. 1. Introduction Our world, dynamic in its 4D nature, demands an increasingly sophisticated understanding and simulation of our living environment. This offers significant potential for practical applications, including Virtual Reality (VR), Augmented Reality (AR), and robotic simulations. There have been notable advances in 3D object modeling, particularly in representations through parametric models [27, 30, 36, 45, 67]. Unfortunately, these template-based models are not effectively suited to capture the 4D dynamics of general non-rigid objects, due to the assumption of a fixed template mesh. Model-free approaches [25, 32, 52] represent a significant advance by using coordinate-MLP representations for deformable object reconstruction with arbitrary topologies and non-unified structures. However, these stateof-the-art methods still encounter challenges when facing ambiguous observations of noisy, sparse, or partial point clouds since it is an ill-posed problem where multiple posThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20496",
        "chosen": "Motion2VecSets is a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. It learns shape and motion distribution through an iterative denoising process of compressed latent representations, parameterized with latent sets for local shape and deformation patterns. This approach improves generalizability to unseen motions and identities, demonstrating superiority in 4D reconstruction from imperfect observations.",
        "rejected": "Motion2VecSets, a 4D diffusion model, learns shape and motion distribution through iterative denoising processes. By parameterizing dynamics with latent sets, it leads to accurate reconstructions and improved generalizability to unseen motions and identities."
    },
    {
        "prompt": "summary: Generalized Category Discovery (GCD) aims to identify a mix of known and novel categories within unlabeled data sets, providing a more realistic setting for image recognition. Essentially, GCD needs to remember existing patterns thoroughly to recognize novel categories. Recent state-of-the-art method SimGCD transfers the knowledge from known-class data to the learning of novel classes through debiased learning. However, some patterns are catastrophically forgot during adaptation and thus lead to poor performance in novel categories classiﬁcation. To address this issue, we propose a novel learning approach, LegoGCD, which is seamlessly integrated into previous methods to enhance the discrimination of novel classes while maintaining performance on previously encountered known classes. Speciﬁcally, we design two types of techniques termed as Local Entropy Regularization (LER) and Dualviews Kullback–Leibler divergence constraint (DKL). The LER optimizes the distribution of potential known class samples in unlabeled data, thus ensuring the preservation of knowledge related to known categories while learning novel classes. Meanwhile, DKL introduces Kullback–Leibler divergence to encourage the model to produce a similar prediction distribution of two view samples from the same image. In this way, it successfully avoids mismatched prediction and generates more reliable potential known class samples simultaneously. Extensive experiments validate that the proposed LegoGCD effectively addresses the known category forgetting issue across all datasets, e.g., delivering a 7.74% and 2.51% accuracy boost on known and novel classes in CUB, respectively. Our code is available at: https://github.com/Cliffia123/LegoGCD. *Equal Contribution. †Joint Corresponding Authors. 64.44% 57.82% Catastrophic forgetting (a) Baseline SimGCD [39] 72.18%  (↑7.74) 60.33% (↑2.51) (b) Ours LegoGCD Figure 1. Visualization of the accuracy results in unlabeled dataset on CUB dataset [37] during training. (a) shows a decrease in the accuracy of known (Old) classes (green) in the baseline as the accuracy of novel (New) classes (orange) increases. (b) demonstrates that LegoGCD solves the catastrophic forgetting problem and surpasses the baseline by a signiﬁcant margin of 7.74. 1. Introduction Deep learning have achieved superior performance on computer vision tasks [4, 11, 24, 25, 30, 34], particularly on image classiﬁcation [10, 12, 27, 28, 51]. However, conventional methods work in a close-world setting, where all training data comes with pre-deﬁned classes. Consequently, deploying these models in real-world scenarios with potential novel classes becomes a considerable challenge. Furthermore, these achievements rely heavily on large-scale annotated dataset, which is not easily accessible in realistic scenarios. To address these challenges, a new paradigm of Generalized Category Discovery (GCD) [1, 7, 9, 23, 36, 39, 45, 46] has been proposed and attracts increasing attention in recent years. The goal of GCD is to train a classiﬁcation model capable of recognizing both known and novel categories within unlabeled data. To be clear, GCD distinguishes itself from the Novel Class Discovery (NCD) [8], which relies on an This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 16880",
        "chosen": "LegoGCD addresses catastrophic forgetting in Generalized Category Discovery by integrating Local Entropy Regularization and Dual-views Kullback-Leibler divergence constraint. These techniques optimize the distribution of known class samples and encourage similar prediction distributions from different views, effectively solving the forgetting issue and improving accuracy on both known and novel classes.",
        "rejected": "LegoGCD introduces Local Entropy Regularization (LER) and Dual-views Kullback–Leibler divergence constraint (DKL) to address catastrophic forgetting in generalized category discovery. These techniques enhance novel class discrimination while preserving known class performance, boosting accuracy."
    },
    {
        "prompt": "summary: We present SuperNormal, a fast, high-ﬁdelity approach to multi-view 3D reconstruction using surface normal maps. With a few minutes, SuperNormal produces detailed surfaces on par with 3D scanners. We harness volume rendering to optimize a neural signed distance function (SDF) powered by multi-resolution hash encoding. To accelerate training, we propose directional ﬁnite difference and patchbased ray marching to approximate the SDF gradients numerically. While not compromising reconstruction quality, this strategy is nearly twice as efﬁcient as analytical gradients and about three times faster than axis-aligned ﬁnite difference. Experiments on the benchmark dataset demonstrate the superiority of SuperNormal in efﬁciency and accuracy compared to existing multi-view photometric stereo methods. On our captured objects, SuperNormal produces more ﬁne-grained geometry than recent neural 3D reconstruction methods. Our code is available at https:// github.com/CyberAgentAILab/SuperNormal. 1. Introduction Recovering high-quality 3D geometry of real-world objects from their multi-view images is a long-standing challenge in computer vision. Recently, neural implicit surfacebased methods have shown remarkable reconstruction results. Compared to traditional multi-view stereo (MVS) methods [25], neural methods are more robust to viewdependent observations and textureless surfaces [29]. Furthermore, the reconstruction procedure has become highly efﬁcient [30] by introducing multi-resolution hash coding [20]. However, even though this spatial encoding allows ﬁne-grained geometry to be represented, shape reconstruction from multi-view images tends to smooth out highfrequency surface details, as shown in Fig. 1 bottom right. Multi-view photometric stereo (MVPS), on the other hand, aims to recover pixel-level high-frequency surface detail by introducing additional lighting conditions during Multi-view posed normal maps Ours (51 secs) PS-NeRF (8 hrs) Scanner [Rexcan CS+] Ours (5 mins) NeuS2 (5 mins) ~100 images 18 normal maps Scanner  [EinScan SE] Figure 1. (Top) From multi-view surface normal maps, SuperNormal recovers ﬁne-grained surface details comparable to 3D scanners while being orders of magnitude faster than the existing MVPS method PS-NeRF [31]. (Bottom) Using normal maps produces more faithful high-frequency details than the MVSbased method NeuS2 [30], although both use multi-resolution hash encoding [20]. the image acquisition [9]. In a typical workﬂow, a surface normal map is ﬁrst recovered for each view, recording per-pixel surface orientation information. The normal maps estimated at different viewpoints are then fused into a 3D model, also known as multi-view normal integration [5]. However, existing normal fusion methods struggle to reﬂect the details of the normal maps in the shape of the recovered 3D model. Moreover, the reconstruction process takes hours even when only a few low-resolution normal maps are used [4, 31]. Due to the lack of a fast and accurate multiview normal fusion method, current multi-view photometric stereo results remain unsatisfactory. This paper presents SuperNormal to unite the best of both worlds. Normal maps with pixel-level surface details are utilized to exploit the expressive power and efﬁciency This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20581",
        "chosen": "SuperNormal is a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. It optimizes a neural signed distance function powered by multi-resolution hash encoding, using directional finite difference and patch-based ray marching. This method is more efficient and accurate than existing MVPS methods, producing detailed surfaces comparable to 3D scanners.",
        "rejected": "SuperNormal presents a fast, high-fidelity approach to multi-view 3D reconstruction using surface normal maps. It optimizes a neural signed distance function with multi-resolution hash encoding, producing detailed surfaces comparable to 3D scanners efficiently."
    },
    {
        "prompt": "summary: High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, enabling the network to process spherical representations with minimal computational overhead. We demonstrate the superior performance of our model on both synthetic and real automotive datasets, as well as a selection of other image datasets, for semantic segmentation, depth regression and classification tasks. Our code is publicly available1. 1. Introduction High-resolution fisheye cameras are among the most common and important sensors in modern intelligent vehicles [42]. Due to their non-rectilinear mapping functions and large field of view, fisheye images are highly distorted. Moreover, the most commonly used large-scale computer ∗Equal contribution aDepartment of Mathematical Sciences, Chalmers University of Technology, University of Gothenburg, SE-412 96 Gothenburg, Sweden bCorresponding author, email: osccarls@chalmers.se cNeural Information Processing, Science of Intelligence, Technical University Berlin, DE-10623 Berlin, Germany dDepartment of Mathematics and Mathematical Statistics, Ume˚a University, SE-901 87 Ume˚a, Sweden eZenseact, SE-417 56 Gothenburg, Sweden 1https://github.com/JanEGerken/HEAL-SWIN <latexit sha1_base64=\"cQJewpwtZc pJ6NEe50L9/ReM0bc=\">ADT3ichVJNTxRBEH07q8LiF8jFhMvEjcbTZsYoeCRB DBcTFzADEzQ+/S2fnKdC8GNvsruMKP4ugv4WIMr8vGRFHpTW9Vv6r3qrqm0zrX xkbRt1bQvnP3sxsZ+7+g4ePHs8vPNky1bjJVD+r8qrZSROjcl2qvtU2Vzt1o5I izdV2Olpz8e0j1RhdlZ/sca32i2RY6oHOEkvo857VhTJhvPxlvhv1IlnhTSf2Thd +bVYLrafYwEqZBijgEIJSz9HAsPfLmJEqIntY0KsoaclrjDFHLljZilmJERH/B/ ytOvRkmenaYSdsUrO3ZAZ4jn3e1FMme2qKvqG9jv3iWDf1aYiLr8Jg2pWJHFD 8Qtzhkxm3Mwmde93I7093KYoC3chvN/mpB3D2zXzrvGmIjSQSYl0yh9RI5XzECZ S0fXbgpnytEMqND2gTsUpUSq+YUK+hdN3/bgef8YVvspkC7mRY04kVojGQCrk5E 2Ju+9Y+O/2P7Yh72/sQ3Y/5RuL/3xRN52tV714ufm4+vu6gv/2maxhGd4yRe1g lVsYJMzyFj3FGc4Dy6Cy+BH26cGLe8s4rfV7lwB56CrDA=</latexit>⇥16 <latexit sha1_base64=\"H68/dxKI2U seIb13+4XGgtKcZA=\">ADTnichVLThtBECwveRhIwiMXpFxWKCcrDWCJEck SMQFCSQMlgCh3WVsRt6XdsZExPJPcE0+is/khOI1DRLpITXWOPuqe6q6entqEi0 sUFwWfPGXrx89bo+PjH5u27qemZ2V2TD8pYteM8yctOFBqV6Ey1rbaJ6hSlCtM oUXtRf83F905VaXSe7dizQh2mYS/TXR2HlDnwOpUGX/5aLoRNANZ/n2nVTkNVGs rn6nN4QDHyBFjgBQKGSz9BCEMf/toIUB7BDYiU9LXGFESbIHTBLMSMk2ud/j6f 9Cs14dpG2DFvSbhLMn0scH8TxYjZ7lZF39Becf8QrPfoDUNRdhWe0UZUHBfFTe IWJ8x4jplWmXe1PM90r7Lo4ou8RrO+QhD3zvivzjojJbG+RHx8lcweNSI5n7IDGW 2bFbgu3yn48uJj2lCsEpWsUgypV9K67rt6XI23cYXv0tlUXuSYQ4mlotGVGxLyRs Td0yr7/YU25D3EPuE1Y84Y63/J+q+s7vUbH1qrmwvN1YXq2mr4wPm8ZET9Rmr2 MAWe+Am6Rw/8cu78H57197NbapXqzjv8c8aq/8BTCWrzg=</latexit>⇥4 <latexit sha1_base64=\"WGRhOxYokf OXVAs1tZNL6VNKYck=\">ADd3ichVLbtNAEJ3EXFpzS+EFiQciIq+kNpFXB4r cREvSEUibaU4qmxn4q5ir631plEb5TP4Gl7hI/gUJB4O3WQoEA32szsmTlnZ8ab VLmqbRB8a7W9K1evXV9b92/cvHX7Tmfj7n5dzkzKg7TMS3OYxDXnSvPAKpvzYWU 4LpKcD5LpKxc/OGFTq1J/tKcVj4o402qi0tgCOupsRwlnSi9S1pbN0p8rPS7nUTR 8rSyI79WZ+xHrMerhKNOL+gHsroXnbBxetSsvXKjdZ8iGlNJKc2oICZNFn5OMdX 4DSmkgCpgI1oAM/CUxJmW5IM7QxYjIwY6xX+G07BNc5OsxZ2iltybANmlx5jvx XFBNnuVoZfw/7APhMs+cNC1F2FZ7CJlBcF8X3wC0dI+MyZtFkrmq5nOm6sjShl9 KNQn2VIK7P9JfOa0QMsKlEuvRGMjNoJHI+wQ07AVuCmvFLrS8Rg2FsuiohvFGH oG1k3f1eNqPI8zWyhXTkmAuJFaIxkRty8JbA3Xcsmu/2P3YN3t/Yx6jevbHwz xd10dnf6YfP+8+7PR2N5vXtkYP6BFt4UW9oF16R3uYQUqf6DN9oa/t795Db9PbO k9txrOPfpteFP7li5vw=</latexit>window size <latexit sha1_base64=\"Ft6bx/o8LJ GZzlKi7UM7FU4VCGA=\">ADT3ichVJdaxNBFD3ZaJtU+xH7IvQlGIQiGDaFfjxG bKWFKhXNh8RSdreTdMl+sbuJtCG/oq/6o3z0l/SlFM9cp4JG2wmTe+fce869c3fc JPCz3LZ/FKzig4dz86XywqPHi0vLK5Un7SwepZ5qeXEQp13XyVTgR6qV+3mgukm qnNANVMcdvtbxzlilmR9H/PzRB2HziDy+7n5IQ+7e+9Onz5oXPw7mSlZtdtWdV Zp2GcGsw6iuFp/iMU8TwMEIhQg5/QAOMv56aMBGQuwYE2IpPV/iClMskDtilmK GQ3TI/wFPYNGPGvNTNgeqwTcKZlVPOd+I4ous3VRT+jvea+EGzw3woTUdYdnt O6VCyL4lviOc6YcR8zNJm3vdzP1LfK0ceO3MZnf4kg+p7eb51dRlJiQ4lUsSeZA2 q4ch5zAhFtix3oKd8qVOXGp7SOWCUqkVF0qJfS6unrfnSPv+IKX2SyodxIMycSC0 WjLxUC8qbE9XcMzXe7i52R9y/2Gbuf8o01/n5Rs057o97Yqm+36g1X5jXVsIan mGdL2obTezjiDPwWPcSX/HN+m5dWTdFk2oVjLOKP1ax/BN7FqR</latexit>HEAL-SWIN <latexit sha1_base64=\"G8ES7BJ6M+ eZ5PcIQIPR60J+4Wg=\">ADSnichVLJSgNBEH0Z9327CF6CQRAPYSK4HAUX9KAo GiOoyMzYSYbMxsxE0ZBf8Kof5Q/4G95ED74uR8G9Q6eqX9V7XVXTduS5SWqaDzmj o7Oru6e3r39gcGh4ZHRs/DAJm7Gjyk7ohfGRbSXKcwNVTt3U0dRrCzf9lTFbqz qeOVCxYkbBgfpVaROfasWuFXsVIN7Ve2ds5GC2bRlJX/7pQyp4Bs7YZjuUmc4Bw hHDThQyFASt+DhYS/Y5RgIiJ2ihaxmJ4rcYU2+sltMksxwyLa4H+Np+MDXjWmom wHd7icdk5jHDvSGKNrP1rYp+QvCfS1Y7dcbWqKsK7yitanYJ4rbxFPUmfEf08 8y32v5n6m7SlHFsnTjsr5IEN2n86GzxkhMrCGRPNYls0YNW84XnEBAW2YFesrvCn np+JzWEqtEJcgULerFtHr6uh5d41tc4VIm60tHmtmSmC8aVbnBI69NXH9HP/tuf7 ET8n5i1l9m2+s9PVFfXcO54ulxeLC3nxhZS57b2YwjRm+aKWsIJN7HIGDjVvc Is74954NJ6M57dUI5dxJvBpdXS+AiRpqhc=</latexit>SWIN Figure 1. Our HEAL-SWIN model uses the nested structure of the HEALPix grid to lift the windowed self-attention of the SWIN model onto the sphere. vision and autonomous driving datasets do not contain fisheye images. For these reasons, fisheye images have received much less attention than rectilinear images in the literature. Despite the distortions introduced by the mapping function, the traditional approach for dealing with this kind of data is to use standard (flat) convolutional neural networks which are adjusted to the distortions and either preprocess the data [16, 17, 26, 36, 46, 50] or deform the convolution kernels [47]. However, these approaches struggle to capture the inherent spherical geometry of the images since they operate on a flat approximation of the sphere. Errors and artifacts arising from handling the strong and spatially inhomogeneous distortions are particularly problematic in safety-critical applications such as autonomous driving. Utilizing spherical representations is an approach taken by some models [8, 10, 19] which lift convolutions to the sphere. These models rely on a rectangular grid in spherical This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 6067",
        "chosen": "HEAL-SWIN is a transformer model combining the Hierarchical Equal Area iso-Latitude Pixelation grid with the Hierarchical Shifted-Window transformer. This enables efficient processing of high-resolution, distortion-free spherical data, demonstrating superior performance on automotive datasets for semantic segmentation, depth regression, and classification tasks.",
        "rejected": "HEAL-SWIN transformer combines the HEALPix grid used in astrophysics with SWIN transformer to handle high-resolution fisheye images effectively. It processes spherical representations with minimal computational overhead, showing superior performance in various computer vision tasks."
    },
    {
        "prompt": "summary: Advances in camera-based physiological monitoring have enabled the robust, non-contact measurement of respiration and the cardiac pulse, which are known to be indicative of the sleep stage. This has led to research into camera-based sleep monitoring as a promising alternative to “gold-standard” polysomnography, which is cumbersome, expensive to administer, and hence unsuitable for longer-term clinical studies. In this paper, we introduce SleepVST, a transformer model which enables state-of-theart performance in camera-based sleep stage classification (sleep staging). After pre-training on contact sensor data, SleepVST outperforms existing methods for cardiorespiratory sleep staging on the SHHS and MESA datasets, achieving total Cohen’s kappa scores of 0.75 and 0.77 respectively. We then show that SleepVST can be successfully transferred to cardio-respiratory waveforms extracted from video, enabling fully contact-free sleep staging. Using a video dataset of 50 nights, we achieve a total accuracy of 78.8% and a Cohen’s κ of 0.71 in four-class video-based sleep staging, setting a new state-of-the-art in the domain. 1. Introduction Accurate sleep monitoring is critical for the diagnosis of sleep disorders and the discovery of novel treatments and biomarkers. Poor sleep is broadly linked with a number of health conditions, including cardiovascular diseases such as diabetes and hypertension [15]. Additionally, there are links between specific sleep stages and neurodegenerative conditions, such as a decrease in non-rapid-eye-movement stage 3 (N3) sleep with Alzheimer’s disease [23]. Overnight video polysomnography (vPSG), the “goldstandard” [52] technique for sleep monitoring, requires the *jcarter@robots.ox.ac.uk use of a large number of contact sensors including electrodes placed on the scalp (EEG), near the eyes (EOG), and under the chin (EMG) of the patient. Human experts (polysomnographers) must then review the recorded data, classifying the patient’s sleep into five stages at 30-second intervals (epochs) and annotating other important events such as leg movements and apnoeas. This manual process often takes multiple hours to complete. Experts primarily rely on characteristic patterns in brain activity measured from the EEG to classify sleep stages [20]. However, sleep stage information is also encoded in measures of autonomic activity, including cardiac [48] and respiratory signals [19], and body movements [68]. This has led to the development of methods for automatically classifying sleep stages from sensors that measure these parameters, such as smartwatches [63]. Prior work has shown that these physiological parameters can also be measured using video cameras [18, 65, 66], leading to the development of methods for classifying sleep stages entirely from video input [6, 35, 60]. Camerabased methods have particular promise as part of a multipurpose sleep monitoring system. For example, within elderly care settings, in addition to classifying sleep stages, they could also be used to detect specific sleep movement disorders [18], which are linked with conditions such as Parkinson’s, and which are typically distinguished via manual video review [51]. They could also be used to detect falls [12], one of the leading causes of injury and mortality amongst elderly individuals. State-of-the-art video-based sleep staging [6] has used the heart rate (HR) and breathing rate (BR) derived from video to perform sleep staging. However, these rates are typically measured as averages over time, losing higher frequency inter-pulse and inter-breath interval information. EEG- [40] and wearable-based [22] sleep staging methods have both been improved by using raw waveforms as inputs, rather than derived quantities e.g. signal features [37, 45]. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12479",
        "chosen": "SleepVST is a transformer model for camera-based sleep stage classification. Pre-trained on contact sensor data, it outperforms existing methods for cardiorespiratory sleep staging and can be transferred to video-based staging. Using a video dataset of 50 nights, it achieves state-of-the-art performance in contact-free sleep staging.",
        "rejected": "This paper proposes Ev-DeblurNeRF, which combines blurry images and events to recover sharp NeRFs. It leverages event double integral as an additional prior and models the event-pixel response, enabling high-quality reconstructions under challenging conditions."
    },
    {
        "prompt": "summary: The landscape of deep learning research is moving towards innovative strategies to harness the true potential of data. Traditionally, emphasis has been on scaling model architectures, resulting in large and complex neural networks, which can be difficult to train with limited computational resources. However, independently of the model size, data quality (i.e. amount and variability) is still a major factor that affects model generalization. In this work, we propose a novel technique to exploit available data through the use of automatic data augmentation for the tasks of image classification and semantic segmentation. We introduce the first Differentiable Augmentation Search method (DAS) to generate variations of images that can be processed as videos. Compared to previous approaches, DAS is extremely fast and flexible, allowing the search on very large search spaces in less than a GPU day. Our intuition is that the increased receptive field in the temporal dimension provided by DAS could lead to benefits also to the spatial receptive field. More specifically, we leverage DAS to guide the reshaping of the spatial receptive field by selecting task-dependant transformations. As a result, compared to standard augmentation alternatives, we improve in terms of accuracy on ImageNet, Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when pluggingin our DAS over different light-weight video backbones. 1. Introduction Creating models with significantly increased capacity, in an attempt to achieve incremental performance improvements, has been the prevailing approach in designing Convolutional Neural Network (CNN) classifiers. As a result, CNNs with increased depth [16, 40, 45, 60], and Vision Transformers (ViTs) [13] were proposed over the years. Specifically, ViT has demonstrated promising results on a wide variety of computer vision tasks including image classifi(a) Conceptual representation of the proposed approach. We reshape the Receptive Field (RF) by applying affine transformations optimized through our Differentiable Augmentation Search (DAS). On the top right you can see how fusing with random transformation would not lead to benefits as, when concatenating in time, the employed shift mechanism would fuse features related to random parts. On the bottom, the augmentations guided by DAS obtain specific shapes of the RF so that more context is kept. rotate translate zoom found by DAS (b) RF visualization (ResNet-50, with GSF fusion) when different single or composed transformations are applied. The last column shows our DAS selected operation for CIFAR-10 and CIFAR-100, which combines translation, rotation and zoom. More details in Tab. 4. Figure 1. 1a overviews our approach and 1b shows a real example of obtained receptive fields. The employed transformations are fundamental to shape the receptive field, as shown in 1b. The augmented images with DAS (Sec. 3.1) are concatenated in time, and processed through a video network that partially shifts and fuses the features (Sec. 3.2). cation and semantic segmentation [13, 36, 48, 56]. However, while these techniques have demonstrated remarkable success, it is noteworthy that these high-capacity models necessitate increased computational resources for effective training and inference, making them economically impractical for training and deployment within practical application scenarios. Moreover, the over-parametrization of ViT and Deep CNNs makes the networks prone to overfitting, This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 5829",
        "chosen": "DAS is a differentiable augmentation search method for image classification and semantic segmentation. It generates image variations processed as videos, guiding the reshaping of the spatial receptive field with task-dependent transformations. This improves accuracy on multiple datasets when integrated with light-weight video backbones.",
        "rejected": "Radiologists desire fully automated versatile AI for medical imaging interpretation. This paper leverages language as supervision by distilling diagnostic knowledge from pre-trained 2D X-ray models to interpret 3D chest CT images. It uses robust contrastive learning to align images and reports within patients, demonstrating feasibility in real-world applications."
    },
    {
        "prompt": "summary: Due to the high potential for abuse of GenAI systems, the task of detecting synthetic images has recently become of great interest to the research community. Unfortunately, existing image-space detectors quickly become obsolete as new high-fidelity text-to-image models are developed at blinding speed. In this work, we propose a new synthetic image detector that uses features obtained by inverting an open-source pre-trained Stable Diffusion model. We show that these inversion features enable our detector to generalize well to unseen generators of high visual fidelity (e.g., DALL·E 3) even when the detector is trained only on lower fidelity fake images generated via Stable Diffusion. This detector achieves new state-of-the-art across multiple training and evaluation setups. Moreover, we introduce a new challenging evaluation protocol that uses reverse image search to mitigate stylistic and thematic biases in the detector evaluation. We show that the resulting evaluation scores align well with detectors’ inthe-wild performance, and release these datasets as public benchmarks for future research. *Work done during an internship at Google Research. 1. Introduction Recent advances in text-to-image modeling have made it easier than ever to generate harmful or misrepresentative content at scale. Moreover, new versions of most photorealistic commercial models are being continuously updated and released behind closed APIs, making it harder to keep fake image detectors up to date. In this work, we make significant strides towards building a GenAI detector that can reliably identify images from unseen photorealistic text-to-image models. Specifically, we propose a model that can be trained using fake images only from Stable Diffusion (SD) [45] and reliably detect images generated by recent open (Kandinsky [51], W¨uerstchen [39], PixArt-α [16], etc.) and closedsource text-to-image models (Imagen [46], Midjourney [2], DALL·E 3 [12], etc.) of significantly higher visual fidelity. Existing methods [17, 37, 54] focus primarily on detecting traces left by convolutional generators in a way that is robust to re-compression, resizing and other in-the-wild transformations. While these methods worked well for GANs and early diffusion models, we show that they, unfortunately, fail to generalize well to current photorealistic generative models, even when re-trained using better data. Recent diffusion detectors that rely on CLIP embeddings [37] or inversions [55] This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 10759",
        "chosen": "FakeInversion is a synthetic image detector that uses features from an inverted Stable Diffusion model. This method generalizes well to unseen high-fidelity generators, even when trained on lower-fidelity images. It achieves state-of-the-art results across multiple training and evaluation setups and introduces a new evaluation protocol using reverse image search.",
        "rejected": "Due to the rapid development of high-fidelity text-to-image models, there is an increasing need for synthetic image detection. This research introduces a detector using inversion features from a Stable Diffusion model, proving effective against unseen generators. The evaluation protocol includes reverse image search to mitigate biases, aligning with real-world performance."
    },
    {
        "prompt": "summary: Detecting edges in images suffers from the problems of (P1) heavy imbalance between positive and negative classes as well as (P2) label uncertainty owing to disagreement between different annotators. Existing solutions address P1 using class-balanced cross-entropy loss and dice loss and P2 by only predicting edges agreed upon by most annotators. In this paper, we propose RankED, a uniﬁed rankingbased approach that addresses both the imbalance problem (P1) and the uncertainty problem (P2). RankED tackles these two problems with two components: One component which ranks positive pixels over negative pixels, and the second which promotes high conﬁdence edge pixels to have more label certainty. We show that RankED outperforms previous studies and sets a new state-of-the-art on NYUDv2, BSDS500 and Multi-cue datasets. Code is available at https://ranked-cvpr24.github.io. 1. Introduction Detecting contours of objects in a given image is a fundamental problem in Computer Vision. It has been approached as a machine learning problem since the introduction of the inﬂuential BSDS dataset [1]. As with any learning-based approach, characteristics of the training data affects performance. One striking issue regarding groundtruth contour data is that contours are rare events. For example, in the BSDS dataset, only 7% of all pixels within an image are marked as edge pixels1. This creates a signiﬁcant imbalance between the positive (edge) and negative (nonedge) classes, which hinders the training of machine learning models. Another important issue observed in edge data is the uncertainty regarding the ground-truth annotations. †Equal contribution. 1Although some studies call such high-level, semantic edges as “contour” and low-level edges as “edge”, we follow the recent literature [12, 15, 39, 44, 55] and use the term “edge” for contours in the rest of the paper. 1 1 Annotator 1 1 1 Annotator 2 1 1 1 Annotator 3 .33 .66 1 .33 Pixel-wise  Averaging Threshold 1 1 Training  Target (𝐲) 0.4 0.4 0.6 0.5 0.7 0.3 0.4 0.2 0.2 Prediction (𝐩) Score-based Loss (a) Existing Approaches Rank Positives Above Negatives 0.4 0.4 0.6 0.5 0.5 0.3 0.4 0.2 0.2 𝒑𝟑 \" (b) RankED with its Ranking and Sorting Components Sort Positives wrt Certianty Certainty of Labels (c) 𝒑𝟐 \" 𝒑𝟎 \" 𝒑𝟏 \" Prediction (𝐩) 𝑝& \" > 𝑝' ( for each positive 𝑖& negative 𝑗 𝑝& \" > 𝑝' \" ⇒𝑐& \" > 𝑐' \" if each positive 𝑖, 𝑗 Pos./Neg. Annotation Positive Pred. 𝑐) \" 𝑐* \" 𝑐+ \" 𝑐, \" Negative Pred. Figure 1. (a) Current approaches threshold label certainties and class-balanced cross-entropy loss for training edge detectors. (b) With RankED, we propose a uniﬁed approach which ranks positives over negatives to handle the imbalance problem and sorts positives with respect to their certainties. There exist a non-trivial amount of variation between the annotations produced by different human annotators, which essentially creates noise in the supervisory signal. These two issues of edge ground-truth data, namely the imbalance and uncertainty, have long been known, however, efforts to address them have remained limited. For the imbalance problem, although there is a vast literature on long-tailed and imbalance learning (see, e.g., [46, 53]), researchers have only explored using Dice Loss [10–12] and weighted cross-entropy loss [20, 29, 39, 44, 49], which mitigate the problem to a certain extent. However, as we show in this paper, they are far from ﬁnding the optimal soluThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3239",
        "chosen": "RankED is a unified ranking-based approach for edge detection that addresses both class imbalance and label uncertainty. It ranks positive pixels over negatives and promotes high-confidence edge pixels. RankED outperforms previous methods and sets a new state-of-the-art on NYUD-v2, BSDS500, and Multi-cue datasets.",
        "rejected": "This paper presents RankED, a ranking-based method addressing class imbalance and label uncertainty in edge detection. It ranks positive pixels over negatives and promotes high-confidence edge pixels, outperforming previous studies on datasets like NYUD-v2 and BSDS500."
    },
    {
        "prompt": "summary: Detecting objects in 3D under various (normal and adverse) weather conditions is essential for safe autonomous driving systems. Recent approaches have focused on employing weather-insensitive 4D radar sensors and leveraging them with other modalities, such as LiDAR. However, they fuse multi-modal information without considering the sensor characteristics and weather conditions, and lose some height information which could be useful for localizing 3D objects. In this paper, we propose a novel framework for robust LiDAR and 4D radar-based 3D object detection. Specifically, we propose a 3D-LRF module that considers the distinct patterns they exhibit in 3D space (e.g., precise 3D mapping of LiDAR and wide-range, weatherinsensitive measurement of 4D radar) and extract fusion features based on their 3D spatial relationship. Then, our weather-conditional radar-flow gating network modulates the information flow of fusion features depending on weather conditions, and obtains enhanced feature that effectively incorporates the strength of two domains under various weather conditions. The extensive experiments demonstrate that our model achieves SoTA performance for 3D object detection under various weather conditions. 1. Introduction Detecting 3D objects, which aims to classify the objects and localize them in 3D coordinates, plays a crucial role in various applications such as autonomous driving, robotic, and drone systems [1, 8, 44]. Many attempts have been made to utilize various sensors, such as camera, LiDAR, and radar, for 3D object detection [9, 11, 16, 24, 31, 51, 53]. These methods are typically trained and tested in ideal autonomous driving scenarios, demonstrating satisfactory performance under normal conditions. Since real-world driving situations have diverse weather conditions, robust models operating in various conditions are needed. *Code: https://github.com/yujeong-star/RL_3DOD. (b) Point-based Fusion LiDAR Radar Image or Fusion Poor Poor 4D Radar Image LiDAR (d) BEV-based Fusion Fusion Poor LiDAR Radar Image or Poor (c) Pseudo-Image-based Fusion LiDAR Radar Fusion (e) Proposed Fusion LiDAR 4D Radar Fusion Image Weatherinformed (a) Camera, LiDAR, 4D Radar in Adverse Weather Figure 1. In adverse weather conditions, as depicted in (a), radar exhibits the highest robustness, followed by LiDAR, while the image is significantly degraded. Prior multi-modal 3D object detection research follows the fusion method in (b)-(d). They suffer from inaccurate information from images or sub-optimal performance due to the compression of critical 3D information from LiDAR and radar into BEV or pseudo-images for fusion. In contrast, our approach (e) effectively fuses LiDAR and 4D radar in 3D space, taking the strengths of each sensor through weather information, showing robust performance under adverse conditions. Recently, several research has focused on addressing these challenges by employing radar sensors capable of handling various weather conditions [28], and has released datasets containing diverse weather environments [4, 28, 35]. Moreover, research on a new novel sensor, 4D radar, which includes height information, has been initiated [22, 28, 38, 43]. Since the radar relies on radio waves, it has the advantages of long-range detection and robustness under adverse weather conditions. However, it does not provide precise distance or detailed 3D maps and struggles with standalone deployment [12, 19, 43]. Therefore, ongoing This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 15162",
        "chosen": "This paper proposes a novel framework for robust LiDAR and 4D radar-based 3D object detection. It introduces a 3D-LRF module to extract fusion features based on the 3D spatial relationship of LiDAR and radar, and a weather-conditional radar-flow gating network to modulate information flow. The model achieves state-of-the-art performance under various weather conditions.",
        "rejected": "A novel framework for 3D object detection under diverse weather conditions is proposed, utilizing LiDAR and 4D radar. The approach, featuring a 3D-LRF module and weather-conditional radar-flow gating network, demonstrates state-of-the-art performance by effectively fusing sensor data based on their spatial relationships and weather conditions."
    },
    {
        "prompt": "summary: Group robustness strategies aim to mitigate learned biases in deep learning models that arise from spurious correlations present in their training datasets. However, most existing methods rely on the access to the label distribution of the groups, which is time-consuming and expensive to obtain. As a result, unsupervised group robustness strategies are sought. Based on the insight that a trained model’s classiﬁcation strategies can be inferred accurately based on explainability heatmaps, we introduce ExMap, an unsupervised two stage mechanism designed to enhance group robustness in traditional classiﬁers. ExMap utilizes a clustering module to infer pseudo-labels based on a model’s explainability heatmaps, which are then used during training in lieu of actual labels. Our empirical studies validate the efﬁcacy of ExMap - We demonstrate that it bridges the performance gap with its supervised counterparts and outperforms existing partially supervised and unsupervised methods. Additionally, ExMap can be seamlessly integrated with existing group robustness learning strategies. Finally, we demonstrate its potential in tackling the emerging issue of multiple shortcut mitigation1. 1. Introduction Deep neural network classiﬁers trained for classiﬁcation tasks, have invited increased scrutiny from the research community due to their overreliance on spurious correlations present in the training data [4, 5, 9, 31, 38]. This is related to the broader aspect of Shortcut Learning [10], or the Clever Hans effect [15], where a model picks the path of least resistance to predict data, thus relying on shortcut features that are not causally linked to the label. The consequence of this phenomenon is that, although such models may demonstrate impressive mean accuracy on the test data, they may still fail on challenging subsets of the data, i.e. the groups [7, 8, 27]. As a result, group robustness is a natural 1Code available at https://github.com/rwchakra/exmap objective to be met to mitigate reliance on spurious correlations. Thus, instead of evaluating models based on mean test accuracy, evaluating them on worst group accuracy has been the recent paradigm [12, 21, 25, 40], resulting in the emergence of group robustness techniques. By dividing a dataset into pre-determined groups of spurious correlations, classiﬁers are then trained to maximize the worst group accuracy - As a result, the spurious attribute that the model is most susceptible to is considered the shortcut of interest. In Figure 1, we illustrate the group robustness paradigm. Given a dataset, a robustness strategy takes as input the group labels and retrains a base classiﬁer (such as Expected Risk Minimization, i.e. ERM) to improve the worst group accuracy (G3 in this case). GroupDRO [28] was one of the early inﬂuential works that introduced the group robustness paradigm. Further, it demonstrated a strategy that could indeed improve worst group accuracy. One limitation of this approach was the reliance on group labels in the training data, which was replaced with the reliance on group labels in the validation data in successive works [13, 19]. However, while these efforts have made strides in enhancing the accuracy of trained classiﬁers for underperforming groups, many hinge on the assumption that the underlying groups are known apriori and that the group labels are available, which is often impractical in real-world contexts. An unsupervised approach, as illustrated in Figure 1, would ideally estimate pseudo-labels that could be inputs to any robustness strategy, leading to improved worst group robustness. An example of such a fully unsupervised worst group robustness approach is (GEORGE) [32]. GEORGE clusters the penultimate layer features in a UMAP reduced space, demonstrating impressive results on multiple datasets. In this work, we instead show that clustering explainability heatmaps instead, is more beneﬁcial in improving worst group robustness. Intuitively, this stems from the fact that a pixel-attribution based explainability method in input space focuses only on the relevant image features (pixel space) in the task, discarding other entangled features irrelevant for the ﬁnal prediction. In our work, we circumvent the need for a group labeled This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12017",
        "chosen": "ExMap is an unsupervised two-stage mechanism enhancing group robustness in classifiers using explainability heatmaps for pseudo-label inference. It bridges the performance gap with supervised methods and outperforms existing unsupervised approaches, demonstrating effectiveness in mitigating spurious correlations.",
        "rejected": "ExMap, an unsupervised mechanism, enhances group robustness in classifiers without relying on labeled groups. By clustering explainability heatmaps, it infers pseudo-labels used during training, bridging the performance gap with supervised methods and outperforming existing partially supervised and unsupervised techniques."
    },
    {
        "prompt": "summary: Bird’s-eye View (BeV) representations have emerged as the de-facto shared space in driving applications, offering a unified space for sensor data fusion and supporting various downstream tasks. However, conventional models use grids with fixed resolution and range and face computational inefficiencies due to the uniform allocation of resources across all cells. To address this, we propose PointBeV, a novel sparse BeV segmentation model operating on sparse BeV cells instead of dense grids. This approach offers precise control over memory usage, enabling the use of long temporal contexts and accommodating memoryconstrained platforms. PointBeV employs an efficient twopass strategy for training, enabling focused computation on regions of interest. At inference time, it can be used with various memory/performance trade-offs and flexibly adjusts to new specific use cases. PointBeV achieves stateof-the-art results on the nuScenes dataset for vehicle, pedestrian, and lane segmentation, showcasing superior performance in static and temporal settings despite being trained solely with sparse signals. We release our code with two new efficient modules used in the architecture: Sparse Feature Pulling, designed for the effective extraction of features from images to BeV, and Submanifold Attention, which enables efficient temporal modeling. The code is available at https://github.com/valeoai/PointBeV. 1. Introduction Bird’s-eye View (BeV) representations are now ubiquitously in driving applications. Indeed, a top-view egocentric grid is not only a convenient shared space for fusing inputs from multiple sensors [28, 39, 42, 49], but also a space-aware representation relevant for many downstream tasks such as detection [31, 34], segmentation [15, 39], forecasting [11], tracking [48], or planning [8, 18, 40]. BeV segmentation encompasses a broad family of tasks such as 2D instance segmentation [15], 3D instance segmentation [20, 21, 54], occupancy forecasting [22, 35] and online *Work done at Valeo.ai. 0.5 1 2 4 31 32 CVT Memory Usage (GiB) (←) 35 36 37 38 PointBeV 40k 23k 13k 8k 5.5k 4.1k 3.6k Simple-BEV BEVFormer LaRa IoU (→) Figure 1. BeV vehicle IoU vs. memory footprint on nuScenes [3] validation set. Models are evaluated without visibility filtering (i.e all annotated vehicles are considered) at resolution 224 × 480. The memory consumption is calculated using a 40GB A100 GPU. The size of a dot represents the number of BeV points being evaluated, the smaller the better. PointBeV has the capacity to explore various trade-offs between efficiency and performance by varying the number of points being considered. The remaining points are considered as zeros in the final prediction. Using PointBeV we can achieve state-of-the-art performance with only a small portion of the points and without losing performance. mapping [27]. In this paper, we focus on BeV segmentation from multiple cameras, in scenarios with or without past frames, respectively referred to as temporal and static. BeV representations are usually implemented using grids of fixed resolution and range [9, 13, 15, 26, 31]. This limits their efficiency in terms of compute, even more clearly when considering temporal tasks, where aggregating past frames for long horizons can be a very costly endeavor. Departing from these dense BeV grid approaches, we present in this paper PointBeV, a camera-based BeV segmentation model that operates on sparse BeV features. Our approach offers control over the model’s memory usage by restricting the number of points considered, and enables adaptive focus on specific regions of interest. Notably, we develop two modules for efficient sparse operations: the Sparse Feature Pulling module, which retrieves features from multiple cameras using sparse coordinates, and the Submanifold Attention module, adapted from subThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 15195",
        "chosen": "PointBeV is a sparse Bird’s-eye View (BeV) segmentation model operating on sparse BeV cells. It offers memory usage control, enabling long temporal contexts and efficient computation. PointBeV achieves state-of-the-art results on nuScenes for vehicle, pedestrian, and lane segmentation with sparse signals.",
        "rejected": "PointBeV is introduced as a sparse Bird’s-eye View (BeV) segmentation model that operates efficiently on sparse BeV cells instead of dense grids. This approach offers precise memory control and achieves state-of-the-art results on the nuScenes dataset, particularly in static and temporal settings, through its efficient two-pass training strategy."
    },
    {
        "prompt": "summary: Coordinate based implicit neural representations have gained rapid popularity in recent years as they have been successfully used in image, geometry and scene modeling tasks. In this work, we present a novel use case for such implicit representations in the context of learning anatomically constrained face models. Actor speciﬁc anatomically constrained face models are the state of the art in both facial performance capture and performance retargeting. Despite their practical success, these anatomical models are slow to evaluate and often require extensive data capture to be built. We propose the anatomical implicit face model; an ensemble of implicit neural networks that jointly learn to model the facial anatomy and the skin surface with high-ﬁdelity, and can readily be used as a drop in replacement to conventional blendshape models. Given an arbitrary set of skin surface meshes of an actor and only a neutral shape with estimated skull and jaw bones, our method can recover a dense anatomical substructure which constrains every point on the facial surface. We demonstrate the usefulness of our approach in several tasks ranging from shape ﬁtting, shape editing, and performance retargeting. 1. Introduction Deformable face models are an important tool in the arsenal of visual effects artists dealing with facial animation. As they are ubiquitously used both in high-end production workﬂows and lightweight consumer applications, building expressive face models for various applications continues to remain an active area of research [17]. Face models today can range from simple linear global shape models [4, 27, 29] to highly complex local models that incorporate the underlying facial anatomy through physical simulation [15, 44, 48] or through anatomical constraints [47]. In this work, we concern ourselves primarily with the high-quality facial animation workﬂow where actor speciﬁc linear blendshape models [27] continue to remain the most commonly used tool for creating facial animations [10, 33, 47]. We propose a new class of actor speciﬁc shape models named the Anatomical Implicit face Model (AIM) which provides several unique advantages over the existing actor speciﬁc face models, and can be used as a drop-in replacement for traditional blendshape models. An actor speciﬁc blendshape model is a collection of 3D shapes of the given actor performing a number of facial expressions, usually created by face scanning [2] or by an artist. While the user-friendliness of such actor speciﬁc blendshape models contributes to their wide adoption, it is a well known limitation that such models often require hundreds of shapes to accurately model complex facial deformation [27]. To address these shortcomings, local blendshape models [10, 42, 47] were proposed. By splitting the face into regions, and allowing the individual regions to deform independently, local shape models are able to capture complex deformations with a limited number of shapes. While local models address the lack of expressivity in global shape models, state-of-the-art methods in facial performance capture [47] and retargeting [10] often incorporate anatomical constraints on the facial surface to plausibly restrict the range of the skin deformations. The anatomical constraints employed by these models [10, 47] provide a few hidden advantages that end up contributing towards their practical success. For example, in the context of facial performance capture, Wu et al. [47] demonstrated that including anatomical constraints derived from the relationship between the facial skin and underlying bones (skull and mandible) helps to separate the rigid and nonrigid components of facial deformation, leading to better face performance capture. In the context of facial performance retargeting, Chandran et al. [10] made use of such an anatomically constrained local face model to restrict a retargeted shape to lie within the space of anatomically plausible shapes of the target actor. Despite their practical success, anatomical constraints are often formulated in practice as regularization terms that have to be satisﬁed as part of complex optimization problems involving several objectives. As a result, ﬁtting these anatomical face models to a target scan or an image for instance, is a computationally intensive procedure taking several minutes per frame on a CPU, or requires hand crafted This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 2220",
        "chosen": "AIM is an ensemble of implicit neural networks modeling facial anatomy and skin surface with high-fidelity. It recovers dense anatomical substructures from skin surface meshes and neutral shapes, providing advantages in shape fitting, editing, and performance retargeting.",
        "rejected": "The anatomical implicit face model uses implicit neural networks to jointly learn facial anatomy and skin surface with high fidelity. Given skin surface meshes and neutral shapes with estimated bones, it recovers dense anatomical substructures, enabling applications ranging from shape fitting to performance retargeting."
    },
    {
        "prompt": "summary: Superpixels play a crucial role in image processing by partitioning an image into clusters of pixels with similar visual attributes. This facilitates subsequent image processing tasks, offering computational advantages over the manipulation of individual pixels. While numerous oversegmentation techniques have emerged in recent years, many rely on predefined initialization and termination criteria. In this paper, a novel top-down superpixel segmentation algorithm called Hierarchical Histogram Threshold Segmentation (HHTS) is introduced. It eliminates the need for initialization and implements auto-termination, outperforming state-of-the-art methods w.r.t. boundary recall. This is achieved by iteratively partitioning individual pixel segments into foreground and background and applying intensity thresholding across multiple color channels. The underlying iterative process constructs a superpixel hierarchy that adapts to local detail distributions until color information exhaustion. Experimental results demonstrate the superiority of the proposed approach in terms of boundary adherence, while maintaining competitive runtime performance on the BSDS500 and NYUV2 datasets. Furthermore, an application of HHTS in refining machine learningbased semantic segmentation masks produced by the Segment Anything Foundation Model (SAM) is presented. 1. Introduction Superpixel segmentation is an important preprocessing step in computer vision. It groups pixels with similar properties to reduce the number of primitives and enhance object representation. Various image processing tasks benefit from high-quality superpixels, such as semantic segmentation [12, 33, 42, 52], object tracking [47], object categorization [11], simultaneous localization and mapping (SLAM) [6, 20], image segmentation [10, 34, 50], video segmentation [39, 40], and stereo matching [37, 44]. There exists a wide range of approaches for image oversegmention (a) (b) (c) (d) Figure 1. Visual comparison of 500 superpixels resulting from (a, c) ETPS [previous], (b, d) HHTS [proposed] segmentation. (a) (b) (c) Figure 2. Visual comparison of semantic segment masks (a) original image, (b) semantic segment (SAM ViT-H) [previous] and (c) refined semantic segment (SAM + HHTS) [proposed] [1, 5, 7–9, 18, 21, 28, 30, 46, 49, 51], often involving tradeoffs between boundary adherence, regular segment sizes, and computational efficiency. However, there are also various applications e.g. in computer graphics or medical imaging [37, 50] requiring the maximization of boundary recall. In this work, a novel hierarchical oversegmentation approach based on auto-terminating local histogram thresholding is introduced, resulting in superpixels with significantly higher boundary adherence. The corresponding results can be utilized e.g. for fine-tuning semantic segmentation masks, as will be shown for the Segment Anything Model (SAM) [15]. Most established superpixel algorithms depend on a priori knowledge of image content and structure: Examples are seed-based methods requiring inforThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3195",
        "chosen": "HHTS is a novel top-down superpixel segmentation algorithm eliminating initialization and implementing auto-termination. It outperforms state-of-the-art methods in boundary recall by iteratively partitioning pixel segments and applying intensity thresholding, demonstrating superior boundary adherence on BSDS500 and NYUV2 datasets.",
        "rejected": "HHTS, a top-down superpixel segmentation algorithm, automatically terminates and outperforms state-of-the-art methods in boundary recall. It iteratively partitions pixel segments into foreground and background, applying intensity thresholding across multiple channels, demonstrating superior performance on BSDS500 and NYUV2 datasets."
    },
    {
        "prompt": "summary: The study of complex human interactions and group activities has become a focal point in human-centric computer vision. However, progress in related tasks is often hindered by the challenges of obtaining large-scale labeled datasets from real-world scenarios. To address the limitation, we introduce M3Act, a synthetic data generator for multi-view multi-group multi-person human atomic actions and group activities. Powered by Unity Engine, M3Act features multiple semantic groups, highly diverse and photorealistic images, and a comprehensive set of annotations, which facilitates the learning of human-centered tasks across singleperson, multi-person, and multi-group conditions. We demonstrate the advantages of M3Act across three core experiments. The results suggest our synthetic dataset can significantly improve the performance of several downstream methods and replace real-world datasets to reduce cost. Notably, M3Act improves the state-of-the-art MOTRv2 on DanceTrack dataset, leading to a hop on the leaderboard from 10th to 2nd place. Moreover, M3Act opens new research for controllable 3D group activity generation. We define multiple metrics and propose a competitive baseline for the novel task. Our code and data are available at our project page: http://cjerry1243.github.io/M3Act. 1. Introduction Understanding collective human activities and social groups carries significant implications across diverse domains, as it contributes to bolstering public safety within †Work done during internship at Roblox surveillance systems, ensuring safe navigation for autonomous robots and vehicles amidst human crowds, and enriching social awareness in human-robot interactions [8, 9, 11, 12, 21, 37, 49, 51]. However, the advancement in related tasks is often impeded by the challenges of obtaining large-scale human group activity datasets in real-world scenarios with fine-grained multifaceted annotations. Generating synthetic data is an emerging alternative to collecting real-world data due to its capability of producing large-scale datasets with perfect annotations. Nonetheless, most synthetic datasets [4, 20, 40, 48, 53] are primarily designed to facilitate human pose and shape estimation. They can only provide data with independentlyanimated persons, which is unsuitable for tasks in singlegroup and multi-group conditions [51]. To address the limitation, we propose M3Act, a synthetic data generator, with multi-view multi-group multi-person human actions and group activities. As presented in Tab. 1, M3Act stands out by offering comprehensive annotations including both 2D and 3D annotations as well as fine-grained person-level and group-level labels, thereby making it an ideal synthetic dataset generator to support tasks such as human activity recognition and multi-person tracking across all listed realworld datasets. Illustrated in Fig. 1, our synthetic data generator features multiple semantic groups, highly diverse and photorealistic images, and a rich set of annotations. It encompasses 25 photometric 3D scenes, 104 HDRIs (High Dynamic Range Images), 5 lighting volumes, 2200 human models, 384 animations (categorized into 14 atomic action classes), and 6 group activities. For our experiments, We generated two This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 21922",
        "chosen": "M3Act is a synthetic data generator for multi-view, multi-group, multi-person human actions and activities. Powered by Unity Engine, it offers diverse, photorealistic images and comprehensive annotations, improving downstream methods and enabling controllable 3D group activity generation.",
        "rejected": "M3Act, a synthetic data generator for multi-view multi-group human activities, is presented. Utilizing Unity Engine, M3Act provides photorealistic images and comprehensive annotations, enhancing learning for human-centric tasks. It improves state-of-the-art MOTRv2 on DanceTrack, opening new research avenues for controllable 3D group activity generation."
    },
    {
        "prompt": "summary: 3D visual grounding involves matching natural language descriptions with their corresponding objects in 3D spaces. Existing methods often face challenges with accuracy in object recognition and struggle in interpreting complex linguistic queries, particularly with descriptions that involve multiple anchors or are view-dependent. In response, we present the MiKASA (Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model integrates a self-attention-based scene-aware object encoder and an original multi-key-anchor technique, enhancing object recognition accuracy and the understanding of spatial relationships. Furthermore, MiKASA improves the explainability of decision-making, facilitating error diagnosis. Our model achieves the highest overall accuracy in the Referit3D challenge for both the Sr3D and Nr3D datasets, particularly excelling by a large margin in categories that require viewpoint-dependent descriptions. The source code and additional resources for this project are available on GitHub: https://github.com/dfki-av/MiKASA-3DVG 1. Introduction 3D visual grounding serves as a crucial component in the intersection of natural language processing and computer vision. This task aims to identify and localize objects within a 3D space, using linguistic cues for spatial and semantic grounding. While existing research has made significant strides, challenges remain. Key issues include the lack of explainability in current models, limitations in object recognition within point cloud data, and the complexity of handling intricate spatial relationships. Most existing 3D visual grounding models [2, 15, 16, 33, 40] consist of three parts: (1) object encoder, (2) text encoder, and (3) fusion model. The object encoder processes the provided point cloud and generates features in the embedding space. However, because the points in a (a) Target category: “chair” (b) “The chair in the front of the blue-lit monitor.” Figure 1. Our methodology utilizes a dual-prediction framework for 3D visual grounding. First, we assign a target category score based on object categorization, as detailed in Fig. 1a. Next, a spatial score is integrated according to the object’s alignment with the textual description, as shown in Fig. 1b. point cloud are unordered and inconsistent in sparsity [26], it is not straightforward to apply the methodology typically used for 2D images. An additional challenge is that 3D point cloud datasets are not as extensive as those for 2D images [10, 25], which makes it difficult for the models to correctly recognize object categories. While enlarging the dataset could conceivably improve performance, we refrain from doing so to ensure a fair comparison with existing state-of-the-art methods. Existing works [6, 18, 27] mainly use different techniques such as noise addition, dropping out colors, and transformations to expand the sample space. Though these techniques may increase the stability of the produced object embeddings, the improvement is limited. Inspired by previous works [20, 21, 39] which aims to solve object recognition problem, we leverage the fact that data availability on objects within a specific space can provide valuable insights into the characteristics and relationships of their surrounding entities. For instance, when we come across a cuboid-shaped object in a kitchen, we may naturally assume that it is a dishwasher. Conversely, if we spot the same shape in a bathroom, it is more plausible that it is a washing machine. Contextual information is crucial in determining the identity of objects and gives us a nuanced understanding of our surroundings. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 14131",
        "chosen": "MiKASA is a transformer model for 3D visual grounding integrating a scene-aware object encoder and multi-key-anchor technique. It enhances object recognition accuracy and spatial relationship understanding, achieving the highest accuracy in the Referit3D challenge.",
        "rejected": "The paper discusses the challenges in detecting synthetic images due to advancements in generative AI systems. A new detector leveraging inversion features from a pre-trained model shows robust generalization to unseen generators. Additionally, a novel evaluation protocol using reverse image search helps mitigate biases in detector evaluations."
    },
    {
        "prompt": "summary: The spiking cameras offer the benefits of high dynamic range (HDR), high temporal resolution, and low data redundancy. However, reconstructing HDR videos in high-speed conditions using single-bit spikings presents challenges due to the limited bit depth. Increasing the bit depth of the spikings is advantageous for boosting HDR performance, but the readout efficiency will be decreased, which is unfavorable for achieving a high frame rate (HFR) video. To address these challenges, we propose a readout mechanism to obtain rolling-mixed-bit (RMB) spikings, which involves interleaving multi-bit spikings within the single-bit spikings in a rolling manner, thereby combining the characteristics of high bit depth and efficient readout. Furthermore, we introduce RMB-Net for reconstructing HDR and HFR videos. RMBNet comprises a cross-bit attention block for fusing mixed-bit spikings and a cross-time attention block for achieving temporal fusion. Extensive experiments conducted on synthetic and real-synthetic data demonstrate the superiority of our method. For instance, pure 3-bit spikings result in 3 times of data volume, whereas our method achieves comparable performance with less than 2% increase in data volume. 1. Introduction Real-world scenes possess a significantly wider dynamic range that exceeds the capability of conventional sensors. Typical high dynamic range (HDR) video reconstruction methods [3, 23, 24, 53] with conventional sensors encode exposure times to capture images with alternating exposures. And by fusing the low dynamic range (LDR) images taken under different exposures, the pitfalls of underexposure and # Equal contribution. ∗Corresponding author. † Majority of this work was done at Peking University. Project page: https://github.com/yongqiye00/RMB-Net Input HDR and HFR video (c) … 7 1 0 Qc time rolling-mixed-bit spikings … N = 255 (b) N = 1 multi-bit spikings … … 7 0 Qc (a) N = 1 N = 255 single-bit spikings … … 1 0 Qc Figure 1. The HDR performance of a spiking camera is closely tied to the bit depth of the spikings. (a) From left to right: Single-bit quantization (Qc is the accumulated photon electrons), diagram of single-bit spiking planes, and reconstructed image by accumulating N spiking planes. Increasing N to 255 significantly boosts HDR performance. (b) HDR can be boosted by reading out multi-bit spikings. However, multi-bit spikings decrease the readout efficiency, which is not conducive to obtaining HFR videos. (c) The proposed RMB spikings with time-varying quantization. We further reconstruct HDR and HFR videos from RMB spikings. overexposure are alleviated. This kind of approach has a dilemma between the frame rate and exposure time [20], i.e., long exposure restricts the improvement of frame rate [2], which makes it challenging to capture high frame rate (HFR) videos with conventional sensors in high-speed scenes. Recent advancements in the field of HDR and HFR photography have benefited from the integration of neuromorphic sensors such as event cameras [6, 29, 30] and spiking cameras [2, 21]. These sensors offer appealing characterisThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25117",
        "chosen": "RMB-Net reconstructs HDR and HFR videos from rolling-mixed-bit spikings. It uses a cross-bit attention block for fusing mixed-bit spikings and a cross-time attention block for temporal fusion, achieving superior performance with minimal data volume increase.",
        "rejected": "A novel approach for robust 3D object detection under various weather conditions combines LiDAR and 4D radar data. By considering distinct patterns in 3D space and modulating information flow based on weather conditions, this method enhances fusion features, achieving state-of-the-art performance in adverse weather scenarios."
    },
    {
        "prompt": "summary: In subject-driven text-to-image synthesis, the synthesis process tends to be heavily inﬂuenced by the reference images provided by users, often overlooking crucial attributes detailed in the text prompt. In this work, we propose Subject-Agnostic Guidance (SAG), a simple yet effective solution to remedy the problem. We show that through constructing a subject-agnostic condition and applying our proposed dual classiﬁer-free guidance, one could obtain outputs consistent with both the given subject and input text prompts. We validate the efﬁcacy of our approach through both optimization-based and encoder-based methods. Additionally, we demonstrate its applicability in second-order customization methods, where an encoder-based model is ﬁne-tuned with DreamBooth. Our approach is conceptually simple and requires only minimal code modiﬁcations, but leads to substantial quality improvements, as evidenced by our evaluations and user studies. 1. Introduction Subject-driven text-to-image synthesis focuses on generating diverse image samples, conditioned on user-given text descriptions and subject images. This domain has witnessed a surge of interest and signiﬁcant advancements in recent years. Optimization-based methods [16, 37, 41] tackle the problem by overﬁtting pre-trained text-to-image synthesis models [36, 38] and text tokens to the given subject. Recently, encoder-based approaches [10, 24, 49] propose to train auxiliary encoders to generate subject embeddings, bypassing the necessity of per-subject optimization. In the aforementioned approaches, both the embeddings and networks are intentionally tailored to closely ﬁt the target subject. As a consequence, these learnable conditions tend to dominate the synthesis process, often obscuring the attributes speciﬁed in the text prompt. For instance, as shown in Fig. 1, when employing S∗1 alongside the style 1S∗denotes a pseudo-word, where its embedding is substituted by a learnable subject embedding. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 6733",
        "chosen": "Subject-Agnostic Guidance (SAG) is introduced to improve subject-driven text-to-image synthesis. SAG constructs a subject-agnostic condition and applies dual classifier-free guidance, ensuring outputs align with both the subject and text prompts. This method enhances quality in optimization-based, encoder-based, and second-order customization approaches.",
        "rejected": "In subject-driven text-to-image synthesis, existing methods often emphasize subject-specific details over attributes described in text prompts. This paper introduces Subject-Agnostic Guidance (SAG), which ensures outputs align with both the subject and input text descriptions. SAG uses dual classifier-free guidance to improve image synthesis quality significantly."
    },
    {
        "prompt": "summary: Single-photon Light Detection and Ranging (LiDAR) systems are often equipped with an array of detectors for improved spatial resolution and sensing speed. However, given a fixed amount of flux produced by the laser transmitter across the scene, the per-pixel Signal-to-Noise Ratio (SNR) will decrease when more pixels are packed in a unit space. This presents a fundamental trade-off between the spatial resolution of the sensor array and the SNR received at each pixel. Theoretical characterization of this fundamental limit is explored. By deriving the photon arrival statistics and introducing a series of new approximation techniques, the Mean Squared Error (MSE) of the maximum-likelihood estimator of the time delay is derived. The theoretical predictions align well with simulations and real data. 1. Introduction Single-photon LiDAR has a wide range of applications in navigation and object identification [21, 24–26, 30, 32]. By actively illuminating the scene with a laser pulse of a known shape, we measure the time delays of single photons upon their return, which correspond to the distance of the object [4, 19, 36]. The advancement of photo detectors has significantly improved the resolution of today’s LiDAR [8, 15, 17, 33, 39–41]. Moreover, algorithms have shown how to reconstruct both the scene reflectivity and 3D structure [2, 6, 16, 20, 22, 23, 29, 36, 38, 42, 43]. As an imaging device, a photodetector used in LiDAR faces similar problems as any other CCD or CMOS pixels. Packing more pixels into a unit space decreases the SNR because the amount of photon flux seen by each pixel diminishes [12]. This fundamental limit is linked to the stochastic nature of the underlying Poisson arrival process of the photons [11, 37]. Unless noise mitigation schemes are employed [2, 14, 22, 31], there is a trade-off between the number of pixels one can pack in a unit space and the SNR we will observe at each pixel. The situation can be visualized in Fig. 1, where we highlight the phenomenon that if we Figure 1. As we pack more pixels in a unit space, we gain the spatial resolution with a reduction in the SNR. The goal of this paper is to understand the trade-off between the two factors. use many small pixels, the spatial resolution is good but the per pixel noise caused by the random fluctuation of photons will be high. The bias and variance trade-off will then lead to a performance curve that tells us how the accuracy of the depth estimate will behave as we vary the spatial resolution. The goal of this paper is to rigorously derive the above phenomenon. In particular, we want to answer the following question: Can we theoretically derive, ideally in closed-form, the mean squared error of the LiDAR depth estimate as a function of the number of pixels per unit space? The theoretical analysis presented in this paper is unique from several perspectives: • Beyond Single Pixel. The majority of the computer viThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25307",
        "chosen": "This paper explores the fundamental trade-off in single-photon LiDAR between spatial resolution and Signal-to-Noise Ratio (SNR). It derives the Mean Squared Error (MSE) of the maximum-likelihood estimator for time delay, providing theoretical insights into the sensor's performance limits.",
        "rejected": "Single-photon LiDAR systems face a trade-off between spatial resolution and per-pixel Signal-to-Noise Ratio (SNR). By analyzing photon arrival statistics, this paper derives the Mean Squared Error (MSE) of time delay estimations, providing theoretical insights into optimizing SNR versus spatial resolution in single-photon detectors."
    },
    {
        "prompt": "summary: We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field. Additional materials can be found on the project website. 1 1. Introduction We investigate the problem of generalizable novel view synthesis from sparse image observations. This line of work has been revolutionized by differentiable rendering [29, 40, 41, 50] but has also inherited its key weakness: training, reconstruction, and rendering are notoriously memory- and time-intensive because differentiable rendering requires evaluating dozens or hundreds of points along each camera ray [58]. This has motivated light-field transformers [10, 37, 43, 47], where a ray is rendered by embedding it into a query token and a color is obtained via cross-attention over image tokens. While significantly faster than volume rendering, such methods are still far from real-time. Additionally, they do not reconstruct 3D scene representations that can be edited or exported for downstream tasks in vision and graphics. 1dcharatan.github.io/pixelsplat Figure 1. Overview. Given a pair of input images, pixelSplat reconstructs a 3D radiance field parameterized via 3D Gaussian primitives. This yields an explicit 3D representation that is renderable in real time, remains editable, and is cheap to train. Meanwhile, recent work on single-scene novel view synthesis has shown that it is possible to use 3D Gaussian primitives to enable real-time rendering with little memory cost via rasterization-based volume rendering [19]. We present pixelSplat, which brings the benefits of a primitive-based 3D representation—fast and memoryefficient rendering as well as interpretable 3D structure— to generalizable view synthesis. This is no straightforward task. First, in real-world datasets, camera poses are only reconstructed up to an arbitrary scale factor. We address this by designing a multi-view epipolar transformer that reliably infers this per-scene scale factor. Next, optimizing primitive parameters directly via gradient descent suffers from local minima. In the single-scene case, this can be addressed via non-differentiable pruning and division heuristics [19]. In contrast, in the generalizable case, we need to back-propagate gradients through the representation and thus cannot rely on non-differentiable operations. We thus propose a method by which Gaussian primitives can implicitly be spawned or deleted during training, avoiding local minima, but which nevertheless maintains gradient flow. Specifically, we parameterize the positions (i.e., means) of Gaussians implicitly via dense probability distributions predicted by our encoder. In each forward pass, we sample This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 19457",
        "chosen": "pixelSplat is a feed-forward model reconstructing 3D radiance fields from image pairs using 3D Gaussian primitives. It offers real-time rendering and memory-efficient training, outperforming state-of-the-art methods in novel view synthesis with faster rendering and explicit 3D representations.",
        "rejected": "PixelSplat is introduced as a feed-forward model for reconstructing 3D radiance fields from pairs of images using 3D Gaussian primitives. It offers real-time rendering and efficient memory usage, making it suitable for scalable training and fast 3D reconstruction. Experiments show superior performance compared to state-of-the-art methods."
    },
    {
        "prompt": "summary: This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text prompts. To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects’ category, and thus, it is applicable to general objects. For motion generation, a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically This research was conducted when Jihyeon Kim was a graduate student (Master candidate) at UNIST†. Co-last authors∗. plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the objecthand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. Codes and data are available in: https://github.com/JunukCha/Text2HOI. 1. Introduction Imagine handing over an apple on a table to your friends: you might first grab it and convey this to them. During a social interaction, the hand pose and motion are often defined as a function of object’s pose, shape, and category. While existing works [3, 8, 9, 15, 21, 27, 30, 31] have been successful in modeling diverse and realistic 3D human body motions from a text prompt (where there exists no text-guided hand motion generaThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1577",
        "chosen": "A method for generating 3D hand-object interaction sequences from text prompts is proposed. It decomposes the task into contact generation and motion generation, using a VAE-based network and a Transformer-based diffusion model. This approach generates diverse interactions applicable to unseen objects.",
        "rejected": "This paper presents a novel method for generating sequences of hand-object interactions in 3D guided by text. It decomposes the task into contact generation and motion generation subtasks, utilizing a VAE-based network and Transformer-based diffusion model respectively. The approach generates realistic interactions applicable to unseen objects, enhancing the potential for diverse applications."
    },
    {
        "prompt": "summary: We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose, we first investigate embedding the respective hierarchy to be used for tree preserving embedding and feature extraction. Thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. Finally, we demonstrate the performance of our methods on several datasets. 1. Introduction Data clustering plays an essential role in unsupervised learning and exploratory data analysis. It is used in a variety of applications including web mining, network analysis, image segmentation, bioinformatics, user analytics and knowledge management. Its goal is to partition the data into groups in a way that the objects in the same cluster are more similar according to some criterion, compared to the objects in different clusters. Many clustering methods partition the data into K flat clusters for example, K-means [55], spectral clustering [62,68] and correlation clustering [8]. In many applications, however, the clusters are preferred to be presented at different levels, encompassing both high-level and detailed information. Hierarchical clustering is useful to produce such structures, usually encoded by a dendrogram. A dendrogram is a tree data structure where each node corresponds to a cluster, with the leaf nodes (those at the bottom of the tree) containing only one object. Higher-level clusters are formed by aggregating lower-level clusters and the intercluster dissimilarity between them. Hierarchical clustering can be performed either in an agglomerative (i.e., bottom-up) or in a divisive (i.e., top-down) manner [56]. Agglomerative methods are often computationally more efficient, making them more popular in practice [64]. In both approaches, the clusters are aggregated or split based on various criteria, such as single, average, centroid, complete and Ward. Several studies aim to improve these methods. The works in [49,52] focus on the statistical significance of hierarchical clustering. [24,25,65] formulate this problem as an optimization problem and propose approximate solutions. [82] considers multiple dissimilarities for a pair of clusters, and [11, 17] suggest merging multiple clusters at each step instead of one. [6] employs global information to eliminate the influence of noisy similarities, and [19] proposes to apply agglomerative methods to small subsets of the data instead of individual data objects. [33,38] augment agglomerative methods with probabilistic models, and finally, [23,60] propose efficient but approximate methods for hierarchical clustering. On the other hand, most clustering methods, either flat or hierarchical, assume non-negative pairwise (dis)similarities. However, in several practical applications, pairwise similarities can be any real number, positive or negative. For example, it could be preferable for a user or oracle to indicate whether two objects are similar (considered a positive relation) or dissimilar (considered a negative relation), rather than solely providing a positive (nonnegative) pairwise similarity, even if the two objects are dissimilar. The former approach yields more precise information because, in the latter scenario, the dissimilarity between two objects (i.e., zero similarity) could be confused with a lack of available information. Some relevant applications for this setting include image segmentation with higher order correlation information [47,48], webpage segmentation [12], community detection over graphs [67], social media mining [73], analysis of connections over web [43], dealing with attraction/rejection data [26], automated label generation from clicks [3] and entity resolution [7,34]. Hence, a specialized clustering model known as correlation clustering has been developed to work with such data. This model was first introduced on the graphs with only +1 or −1 pairwise similarities [7, 8], and then was generalized to the graphs with arbitrary positive or negative This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 23083",
        "chosen": "Hierarchical correlation clustering is extended to handle positive and negative pairwise dissimilarities. The paper studies unsupervised representation learning using hierarchical correlation clustering, proposing methods for tree preserving embedding and feature extraction.",
        "rejected": "A hierarchical correlation clustering method is proposed that extends traditional correlation clustering to handle positive and negative pairwise dissimilarities. It explores unsupervised representation learning through embedding hierarchies and minimax distance measures. This approach provides an effective solution for clustering complex data structures."
    },
    {
        "prompt": "summary: Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to “think” from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate twenty-one popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics. *Equal contribution, ‡ Project lead, B Corresponding author Project page: https://adacheng.github.io/EgoThink/ GitHub page: https://github.com/AdaCheng/EgoThink/ Dataset page: https://huggingface.co/datasets/EgoThink/EgoThink/ EgoThink Object What is around me? Activity What am I doing? Localization Where am I? Reasoning What about the situation around me? Forecasting What will happen to me? Planning   How will I do?   Figure 1. The main categories of our EgoThink benchmark to comprehensively assess the capability of thinking from a firstperson perspective. 1. Introduction Benefiting from the rapid development of large language models (LLMs) [8, 60, 73], vision-language models (VLMs) [2, 15, 43, 80] have shown remarkable progress in both conventional vision-language downstream tasks [2, 15, 43, 80] and following diverse human instructions [13, 42, 48, 81, 89]. Their application has expanded into broader domains such as robotics [21, 31, 40] and embodied artificial intelligence (EAI) [71, 78]. As a result, the thorough evaluation of VLMs has become increasingly important and challenging. Observing and understanding the world from a first-person perspective is a natural approach for both humans and artificial intelligence agents. We propose that the ability to “think” from a first-person perspective, especially when interpreting egocentric images, is crucial for VLMs. However, as shown in Table 1, the ability to think from a first-person perspective is not adequately addressed by curThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 14291",
        "chosen": "EgoThink is a visual question-answering benchmark assessing VLMs' ability to think from a first-person perspective. It includes six core capabilities with twelve detailed dimensions, evaluated using manually annotated ego-centric video clips and GPT-4 for answer grading.",
        "rejected": "EgoThink benchmarks vision-language models on their ability to understand first-person perspective tasks. With six core capabilities and twelve detailed dimensions, it evaluates twenty-one popular models using GPT-4 for grading. Results indicate significant room for improvement in first-person perspective understanding, highlighting the benchmark's importance for future research."
    },
    {
        "prompt": "summary: This paper proposes a novel task named ”3D part grouping”. Suppose there is a mixed set containing scattered parts from various shapes. This task requires algorithms to find out every possible combination among all the parts. To address this challenge, we propose the so called Gradient Field-based Auto-Regressive Sampling framework (GFARS) tailored specifically for the 3D part grouping task. In our framework, we design a gradient-field-based selection graph neural network (GNN) to learn the gradients of a log conditional probability density in terms of part selection, where the condition is the given mixed part set. This innovative approach, implemented through the gradient-fieldbased selection GNN, effectively captures complex relationships among all the parts in the input. Upon completion of the training process, our framework becomes capable of autonomously grouping 3D parts by iteratively selecting them from the mixed part set, leveraging the knowledge acquired by the trained gradient-field-based selection GNN. Our code is available at: https://github.com/JF-Cheng/G-FARS-3DPartGrouping. 1. Introduction Assuming that you purchase multiple unassembled IKEA chairs and carelessly mix all the parts together, it can quickly become a nightmare to sort through and assemble each chair. The task can be especially daunting if the pieces from different chairs are mixed together, making it challenging to identify the correct components for each chair. Similarly, in the field of archaeology, recovering broken cultural relics can be incredibly difficult, as the fragments are often intermingled with the pieces from other relics. In such cases, archaeologists must carefully separate the mixed fragments and piece them together to reconstruct the original relics. In a similar vein, the field of LEGO automatic assembly requires AI agents to select different combinations of parts from massive LEGO blocks and assemble them into a shape. All of these examples contain two goals: The first goal is to identify the correct combinations from the mixed part set (i.e. part grouping) and the second one is to assemble them into reasonable shapes (i.e. part assembly). To achieve these two objectives, algorithms must first be capable of comprehending the geometric relationships among all the parts. Next, they should be able to separate the parts by their shapes, and finally, assemble the chosen parts into reasonable shapes. For the part assembly, previous works have researched some methods for assembling a given group of parts. DGLNet [15] is the first work to explore the assembly problem without prior instruction. The DGL-Net algorithm can predict the 6-DoF poses for each input part, enabling translation and rotation of the parts to their expected positions. RGL-Net [27] is another part assembly work that utilizes sequential information among all the input parts. By assembling shapes in a specific order (e.g., top-to-bottom), RGLNet achieves more accurate assembly. IET [41] is a recently proposed algorithm that utilizes an instance encoded transformer and self-attention mechanisms [30, 32, 36, 37, 40] to enhance the network’s assembly ability. However, part grouping still remains an unsolved problem. As previously mentioned, the goal of part grouping is to use algorithms to identify all possible combinations in a mixed part set. To address this, we introduce the 3D part grouping task. The definition of this task is presented in Fig. 1. Suppose we have a set of mixed parts from N different shapes. The 3D part grouping task mandates the algorithms to process all these parts and categorize them into groups based on their originating shapes. Our proposed task 3D part grouping is challenging for two main reasons. First, the algorithms must understand the relationships among all the parts. Second, the exact number of potential groups, N, is unknown. This uncertainty complicates both the problem formulation and the creation of effective algorithms. To tackle these challenges, we introduce Gradient-Field-based Auto-Regressive Sampling (GFARS) framework in this paper. This framework integrates a gradient-field-based graph neural network for the encoded parts, aiding in understanding the relationships among all This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27652",
        "chosen": "G-FARS is a framework for 3D part grouping using a gradient-field-based selection graph neural network. It learns part relationships and groups 3D parts autonomously, applicable to mixed part sets with unknown group numbers.",
        "rejected": "Introducing \"3D part grouping,\" this paper proposes a Gradient Field-based Auto-Regressive Sampling framework (G-FARS) designed to identify all possible combinations among scattered parts. Using a gradient-field-based selection graph neural network, G-FARS autonomously groups parts from mixed sets, demonstrating its capability in handling complex part relationships."
    },
    {
        "prompt": "summary: Backdoor attack poses a significant security threat to Deep Learning applications. Existing attacks are often not evasive to established backdoor detection techniques. This susceptibility primarily stems from the fact that these attacks typically leverage a universal trigger pattern or transformation function, such that the trigger can cause misclassification for any input. In response to this, recent papers have introduced attacks using sample-specific invisible triggers crafted through special transformation functions. While these approaches manage to evade detection to some extent, they reveal vulnerability to existing backdoor mitigation techniques. To address and enhance both evasiveness and resilience, we introduce a novel backdoor attack LOTUS. Specifically, it leverages a secret function to separate samples in the victim class into a set of partitions and applies unique triggers to different partitions. Furthermore, LOTUS incorporates an effective trigger focusing mechanism, ensuring only the trigger corresponding to the partition can induce the backdoor behavior. Extensive experimental results show that LOTUS can achieve high attack success rate across 4 datasets and 7 model structures, and effectively evading 13 backdoor detection and mitigation techniques. The code is available at https://github.com/Megum1/LOTUS. 1. Introduction Backdoor attack is a prominent security threat to Deep Learning applications, evidenced by the large body of existing attacks [5, 19, 37, 51, 64] and defense techniques [20, 32, 35, 67, 72]. It injects malicious behaviors to a model such that the model operates normally on clean samples but misclassifies inputs that are stamped with a specific trigger. A typical way of injecting such malicious behaviors is through data poisoning [1, 19, 39]. This approach introduces a small set of trigger-stamped images paired with the target label into the training data. Attackers may also manipulate the training procedure [10, 45, 46], and tamper with the model’s internal mechanisms [37, 41]. The majority of existing attacks rely on a uniform pattern [5, 19, 39, 64] or a transformation function [6, 51] as the trigger. The uniform trigger tends to be effective on any input, which can be detected by existing techniques. For instance, trigger inversion methods [20, 38, 67, 68] aim to reverse engineer a small trigger that can induce the target prediction on a set of inputs. According to the results reported in the literature [20, 61, 67], for a number of attacks, it is feasible to invert a pattern that closely resembles the groundtruth trigger and has a substantially high attack success rate (ASR), hence detecting backdoored models. Recent studies introduce sample-specific invisible attacks [10, 33, 45, 46] that encourage the model to emphasize the correlation between the trigger and the input sample. Although these attacks effectively evade certain detection methods [20, 67], they are not resilient to backdoor mitigation techniques [32, 35, 72]. For instance, a straightforward approach such as fine-tuning the backdoored model using only 5% of the training data can significantly reduce ASR. This is due to the fact that imperceptible trigger patterns are not persistent during the retraining process. Moreover, the sample-specific characteristic of these attacks make them less robust to backdoor mitigation methods. In this paper, we introduce an innovative attack that not only evades backdoor detection approaches but also exhibits resilience against backdoor mitigation techniques. It is a label-specific attack, aiming to misclassify the samples of a victim class to a target class. For the victim-class samples, we divide them into sub-partitions and use a unique trigger for each partition. With such an attack design, existing defense such as trigger inversion is unlikely to find a uniform trigger. This is because the available set of samples used by trigger inversion is likely from different partitions, which makes the detection fail. In addition, we develop a novel trigger focusing technique to ensure that a partition can only be attacked by its designated trigger, not by any This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 24798",
        "chosen": "LOTUS is a novel backdoor attack leveraging secret functions to partition victim-class samples and apply unique triggers. It incorporates a trigger focusing mechanism, achieving high attack success rates across datasets and model structures while evading detection and mitigation techniques.",
        "rejected": "LOTUS is a novel backdoor attack leveraging a secret function to partition victim class samples and apply unique triggers to different partitions. It includes a trigger focusing mechanism ensuring only designated triggers can induce backdoor behavior. LOTUS achieves high attack success rates across various datasets and models while evading numerous detection and mitigation techniques."
    },
    {
        "prompt": "summary: We present Cutie, a video object segmentation (VOS) network with object-level memory reading, which puts the object representation from memory back into the video object segmentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In contrast, Cutie performs top-down object-level memory reading by adapting a small set of object queries. Via those, it interacts with the bottom-up pixel features iteratively with a query-based object transformer (qt, hence Cutie). The object queries act as a high-level summary of the target object, while high-resolution feature maps are retained for accurate segmentation. Together with foreground-background masked attention, Cutie cleanly separates the semantics of the foreground object from the background. On the challenging MOSE dataset, Cutie improves by 8.7 J &F over XMem with a similar running time and improves by 4.2 J &F over DeAOT while being three times faster. Code is available at: hkchengrex.github.io/Cutie. 1. Introduction Video Object Segmentation (VOS), specifically the “semisupervised” setting, requires tracking and segmenting objects from an open vocabulary specified in a first-frame annotation. VOS methods are broadly applicable in robotics [1], video editing [2], reducing costs in data annotation [3], and can also be combined with Segment Anything Models (SAMs) [4] for universal video segmentation (e.g., Tracking Anything [5–7]). Recent VOS approaches employ a memory-based paradigm [8–11]. A memory representation is computed from past segmented frames (either given as input or segmented by the model), and any new query frame “reads” from this memory to retrieve features for segmentation. Importantly, these approaches mainly use pixel-level matching for memory reading, either with one [8] or multiple matching layers [10], and generate the segmentation bottom-up from Figure 1. Comparison of pixel-level memory reading v.s. objectlevel memory reading. In each box, the left is the reference frame, and the right is the query frame to be segmented. Red arrows indicate wrong matches. Low-level pixel matching (e.g., XMem [9]) can be noisy in the presence of distractors. We propose object-level memory reading for more robust video object segmentation. the pixel memory readout. Pixel-level matching maps every query pixel independently to a linear combination of memory pixels (e.g., with an attention layer). Consequently, pixellevel matching lacks high-level consistency and is prone to matching noise, especially in the presence of distractors. This leads to lower performance in challenging scenarios with occlusions and frequent distractors. Concretely, the performance of recent approaches [9, 10] is more than 20 points in J &F lower when evaluating on the recently proposed challenging MOSE [12] dataset rather than the simpler DAVIS-2017 [13] dataset. We think this unsatisfactory result in challenging scenarios is caused by the lack of object-level reasoning. To address this, we propose object-level memory reading, which effectively puts the object from a memory back into the query frame (Figure 1). Inspired by recent query-based object detection/segmentation [14–18] that represent objects as “object queries,” we implement our object-level memory reading with an object transformer. This object transformer uses a small set of end-to-end trained object queries to 1) iteratively probe and calibrate a feature map (initialized by a pixel-level This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3151",
        "chosen": "Cutie is a video object segmentation network with object-level memory reading using object queries and a query-based transformer. It improves segmentation accuracy by separating foreground and background semantics, outperforming existing methods on challenging datasets.",
        "rejected": "We present Cutie, a video object segmentation method that leverages spatiotemporal context for accurate and efficient segmentation. Cutie integrates deep learning techniques with dynamic programming to refine object boundaries over time, achieving state-of-the-art performance on several benchmark datasets."
    },
    {
        "prompt": "summary: The problem of calibrating deep neural networks (DNNs) for multi-label learning is considered. It is wellknown that DNNs trained by cross-entropy for single-label, or one-hot, classification are poorly calibrated. Many calibration techniques have been proposed to address the problem. However, little attention has been paid to the calibration of multi-label DNNs. In this literature, the focus has been on improving labeling accuracy in the face of severe dataset unbalance. This is addressed by the introduction of asymmetric losses, which have became very popular. However, these losses do not induce well calibrated classifiers. In this work, we first provide a theoretical explanation for this poor calibration performance, by showing that these loses losses lack the strictly proper property, a necessary condition for accurate probability estimation. To overcome this problem, we propose a new Strictly Proper Asymmetric (SPA) loss. This is complemented by a Label Pair Regularizer (LPR) that increases the number of calibration constraints introduced per training example. The effectiveness of both contributions is validated by extensive experiments on various multi-label datasets. The resulting training method is shown to significantly decrease the calibration error while maintaining state-of-the-art accuracy. 1. Introduction Deep neural networks (DNNs) including convolutional neural networks (CNNs) [27, 35] and vision transformers (ViTs) [13] have demonstrated great capacity for solving supervised learning tasks in computer vision. However, many applications require trust-worthy machine learning systems, which are not only accurate but also probability calibrated, i.e. able to produce accurate estimates of the posterior probabilities of the various classes. A classifier is calibrated if it predicts a posterior class probability of p when the selection of the class is correct p × 100% of the time. The importance of calibration has been noted for many applications. For example, in medical diagnosis [44, 82], probabilities can be used to determine which examples require human inspection, thus avoiding the cost of manually inspecting all images. However, the process can only be trusted if the DNN provides accurate posterior estimates. The safety-critical nature of the application makes probability calibration a critical requirement to enable this functionality. Cost-sensitive applications [15], e.g. fraud detection [2, 50] or business decision making [56, 75], involve This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27589",
        "chosen": "A new Strictly Proper Asymmetric (SPA) loss and Label Pair Regularizer (LPR) are proposed to calibrate multi-label DNNs. These methods address poor calibration in multi-label learning, significantly reducing calibration error while maintaining accuracy.",
        "rejected": "This paper addresses the calibration of deep neural networks (DNNs) for multi-label learning. While asymmetric losses improve labeling accuracy under dataset imbalance, they fail to produce well-calibrated classifiers. The authors show that these losses lack strictly proper properties, essential for accurate probability estimation. To solve this, they propose a Strictly Proper Asymmetric (SPA) loss and introduce a Label Pair Regularizer (LPR) to increase calibration constraints per training example. Extensive experiments validate the effectiveness of both contributions, significantly reducing calibration error while maintaining state-of-the-art accuracy. This work highlights the importance of calibration in safety-critical applications such as medical diagnosis and cost-sensitive domains like fraud detection."
    },
    {
        "prompt": "summary: Image denoising is a fundamental task in computer vision. While prevailing deep learning-based supervised and self-supervised methods have excelled in eliminating indistribution noise, their susceptibility to out-of-distribution (OOD) noise remains a significant challenge. The recent emergence of contrastive language-image pre-training (CLIP) model has showcased exceptional capabilities in open-world image recognition and segmentation. Yet, the potential for leveraging CLIP to enhance the robustness of low-level tasks remains largely unexplored. This paper uncovers that certain dense features extracted from the frozen ResNet image encoder of CLIP exhibit distortion-invariant and content-related properties, which are highly desirable for generalizable denoising. Leveraging these properties, we devise an asymmetrical encoder-decoder denoising network, which incorporates dense features including the noisy image and its multi-scale features from the frozen ResNet encoder of CLIP into a learnable image decoder to achieve generalizable denoising. The progressive feature augmentation strategy is further proposed to mitigate feature overfitting and improve the robustness of the learnable decoder. Extensive experiments and comparisons conducted across diverse OOD noises, including synthetic noise, real-world sRGB noise, and low-dose CT image noise, demonstrate the superior generalization ability of our method. 1. Introduction Image denoising is a significant task in computer vision and image processing. Current supervised denoising methods leveraging powerful deep neural networks and largescale datasets have achieved exceptional performance in both synthetic and real-world noise removal [33, 60]. However, these supervised denoisers tend to overfit the noise present in the training datasets, resulting in poor generalization to out-of-distribution (OOD) noise [6]. On the other hand, unsupervised and self-supervised denoising methods *Corresponding author. [12, 18, 26, 28, 44, 54] directly focus on the target domain in which the target noisy images reside and hence bypass OOD generalization. Nevertheless, these methods are inherently vulnerable to unseen noise [7] and the collection of target noisy datasets is not always available. Therefore, it is critical to enhance the generalization of deep denoisers. OOD generalization has been popular research in highlevel vision tasks like image recognition and segmentation [51, 70]. In contrast, attention to OOD generalization within image denoising is limited. Existing research in this area primarily consists of two aspects: generalization across degradation levels and generalization across degradation types. Regarding the former, some works trained blind denoisers [18, 60, 62] or bias-free networks [41, 63] to handle noise with varying levels. However, these methods are confined to specific noise and cannot generalize to unseen noise types. For the latter, several works aimed to fortify models against general OOD noise. Particularly, MaskDenoising [6] incorporated dropout units into the model training to enforce the denoiser to learn the reconstruction of image contents. DIL [32] built upon causality and meta-learning and encouraged the model to learn distortion-invariant representations. HAT [58] designed an adversarial attack for deep denoisers and then conducted adversarial training. Recently, through solving the image-text alignment problem based on hyper-scale datasets, the contrastive language-image pre-training (CLIP) model [48] has demonstrated remarkable generalization capacity in downstream open-world image recognition tasks. A series of extensions on CLIP through frozen models [65, 69], model fine-tuning [49], visual prompts [71], distillations [19, 31], and so on [34] have been proposed to transfer the generalization ability of CLIP from classification to dense prediction tasks, including open-vocabulary segmentation [34] and zero-shot depth estimation [65]. However, the feasibility of CLIP for robust restoration in low-level tasks remains unexplored. We therefore ask, is CLIP robust to image noise and can we transfer it for generalizable image denoising? In this paper, we find that the dense feature maps from the frozen ResNet image encoder of CLIP within specific scales exhibit remarkable resilience to noise, a property that This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25974",
        "chosen": "This paper proposes a method to enhance the generalization of image denoising models using features from the frozen ResNet encoder of CLIP. The authors devise an asymmetrical encoder-decoder network and introduce a progressive feature augmentation strategy. Experiments show superior generalization across diverse out-of-distribution noises.",
        "rejected": "Image denoising methods struggle with out-of-distribution (OOD) noise. This paper explores using dense features from CLIP's frozen ResNet encoder for generalizable denoising, proposing an asymmetrical encoder-decoder network. It incorporates a progressive feature augmentation strategy to mitigate overfitting, demonstrating superior performance across various OOD noises."
    },
    {
        "prompt": "summary: The Segment Anything Model (SAM) has garnered significant attention for its versatile segmentation abilities and intuitive prompt-based interface. However, its application in medical imaging presents challenges, requiring either substantial training costs and extensive medical datasets for full model fine-tuning or high-quality prompts for optimal performance. This paper introduces H-SAM: a prompt-free adaptation of SAM tailored for efficient fine-tuning of medical images via a two-stage hierarchical decoding procedure. In the initial stage, H-SAM employs SAM’s original decoder to generate a prior probabilistic mask, guiding a more intricate decoding process in the second stage. Specifically, we propose two key designs: 1) A class-balanced, mask-guided self-attention mechanism addressing the unbalanced label distribution, enhancing image embedding; 2) A learnable mask cross-attention mechanism spatially modulating the interplay among different image regions based on the prior mask. Moreover, the inclusion of a hierarchical pixel decoder in H-SAM enhances its proficiency in capturing fine-grained and localized details. This approach enables SAM to effectively integrate learned medical priors, facilitating enhanced adaptation for medical image segmentation with limited samples. Our H-SAM demonstrates a 4.78% improvement in average Dice compared to existing prompt-free SAM variants for multi-organ segmentation using only 10% of 2D slices. Notably, without using any unlabeled data, H-SAM even outperforms state-of-the-art semisupervised models relying on extensive unlabeled training data across various medical datasets. Our code is available at https://github.com/Cccccczh404/H-SAM. 1. Introduction Accurate delineation of tissues, organs, and regions of interest through medical image segmentation is pivotal in aidUnlabeled Labeled 78.27 86.83 SOTA SemiSupervised Methods H-SAM Mean Dice (%) Methods 75.57 80.35 H-SAM SOTA SAM Variants Figure 1. H-SAM is advantageous in few-shot medical image segmentation. It achieves over 80% in average Dice using only 10% slices for multi-organ segmentation, outperforming existing prompt-free SAM adaptation methods. Without using any unlabeled data at all, it even outperforms state-of-the-art semisupervised models that use extensive unlabeled training data for prostate segmentation. ing medical professionals’ diagnostic precision and treatment planning processes [13, 15]. Furthermore, it plays a fundamental role in propelling disease research and discovery [53]. Nonetheless, a significant challenge in this field lies in the demand for deep learning models to undergo extensive training on large annotated datasets, a resource often challenging to procure within the medical domain. Recently, Segment Anything Model (SAM) [37], trained with over a billion masks from diverse natural images, demonstrates remarkable zero-shot learning capabilities. This breakthrough presents an avenue for significant advancements in medical image segmentation, especially considering the limited availability of extensive datasets in the medical realm. However, SAM’s performance on medical This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3511",
        "chosen": "This paper introduces H-SAM, a prompt-free adaptation of SAM for efficient fine-tuning in medical image segmentation. It employs a two-stage hierarchical decoding procedure with novel attention mechanisms and a hierarchical pixel decoder. H-SAM achieves significant improvements in average Dice scores with limited training data and outperforms semi-supervised models without using unlabeled data.",
        "rejected": "H-SAM enhances Segment Anything Model (SAM) for medical image segmentation through a prompt-free adaptation using a two-stage hierarchical decoding procedure. By addressing unbalanced label distribution and spatial modulation among regions, H-SAM shows a 4.78% improvement in Dice score compared to existing SAM variants, even outperforming state-of-the-art semi-supervised models."
    },
    {
        "prompt": "summary: Driver’s eye gaze holds a wealth of cognitive and intentional cues crucial for intelligent vehicles. Despite its signiﬁcance, research on in-vehicle gaze estimation remains limited due to the scarcity of comprehensive and wellannotated datasets in real driving scenarios. In this paper, we present three novel elements to advance in-vehicle gaze research. Firstly, we introduce IVGaze, a pioneering dataset capturing in-vehicle gaze, collected from 125 subjects and covering a large range of gaze and head poses within vehicles. In this dataset, we propose a new visionbased solution for in-vehicle gaze collection, introducing a reﬁned gaze target calibration method to tackle annotation challenges. Second, our research focuses on in-vehicle gaze estimation leveraging the IVGaze. In-vehicle face images often suffer from low resolution, prompting our introduction of a gaze pyramid transformer that leverages transformer-based multilevel features integration. Expanding upon this, we introduce the dual-stream gaze pyramid transformer (GazeDPTR). Employing perspective transformation, we rotate virtual cameras to normalize images, utilizing camera pose to merge normalized and original images for accurate gaze estimation. GazeDPTR shows stateof-the-art performance on the IVGaze dataset. Thirdly, we explore a novel strategy for gaze zone classiﬁcation by extending the GazeDPTR. A foundational tri-plane and project gaze onto these planes are newly deﬁned. Leveraging both positional features from the projection points and visual attributes from images, we achieve superior performance compared to relying solely on visual features, substantiating the advantage of gaze estimation. The project is available at https://yihua.zone/work/ivgaze. 1. Introduction Understanding driver intention and behavior based on driver gaze is in high demand in intelligent vehicles, facilitating Figure 1. In-vehicle gaze estimation illustration. The driver’s gaze direction is estimated based on the facial images captured by the camera behind the steering wheels. diverse applications such as in-vehicle interaction [1, 2, 25] and driver monitor systems [17, 21, 22]. Recent advances in vehicle gaze estimation concentrate primarily on gaze zone estimation [16, 18, 31, 32]. These approaches deﬁne multiple coarse regions, such as side mirrors and windshields, and conduct classiﬁcation based on face images. Gaze estimation1 serves as an upstream task of gaze zone estimation and can offer more precise information to understand driver attention. However, these methods typically require a large-scale dataset for training. Although there are numerous gaze datasets, collected in indoor [39, 40] or outdoor [20] environments, their applicability to the vehicle environment is limited due to the different environments and camera settings, resulting in suboptimal performance. Creating an in-vehicle gaze dataset proves challenging due to the conﬁned and irregular nature of the vehicular environment. Constructing in-vehicle gaze collection systems remains an unsolved issue, as traditional gaze collection systems are impractical for use within vehicles. The absence of in-vehicle gaze datasets acts as a signiﬁcant barrier to the progress of in-vehicle gaze estimation. In this paper, we present a comprehensive vision-based 1Our work focuses on gaze direction estimation. We abbreviate gaze direction as gaze in the rest. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1556",
        "chosen": "This paper presents IVGaze, a novel dataset for in-vehicle gaze estimation, along with a vision-based collection solution. The authors propose gaze pyramid transformers and a dual-stream gaze pyramid transformer (GazeDPTR) for accurate gaze estimation. They also explore gaze zone classification by projecting gaze onto tri-planes and leveraging positional features and visual attributes.",
        "rejected": "This paper introduces IVGaze, a dataset for in-vehicle gaze estimation, and proposes GazeDPTR, which utilizes transformer-based multilevel features integration and dual-stream processing. GazeDPTR achieves state-of-the-art performance on the IVGaze dataset, enhancing accuracy in gaze zone classification by defining tri-plane projections and leveraging positional features from these projections."
    },
    {
        "prompt": "summary: Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an “adapt then align” paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel “Align before Adapt” (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This paradigm reuses the visual-language alignment of VLP during adaptation and tries to explain an action by the underlying entities. This helps understand actions by bridging the gap with complex activity semantics, particularly when facing unfamiliar or unseen categories. ALT demonstrates competitive performance while maintaining remarkably low computational costs. In fully supervised experiments, it achieves 88.1% top-1 accuracy on Kinetics-400 with only 4947 GFLOPs. Moreover, ALT outperforms the previous state-of-the-art methods in both zero-shot and fewshot experiments, emphasizing its superior generalizability across various learning scenarios. 1. Introduction Video action recognition is a fundamental task in the pursuit of intelligent video understanding. The recent trend of utilizing the visual-language pre-trained (VLP) models [26, 31, 49, 73] have significantly advanced the research of action recognition [27, 37, 43, 45, 64, 71]. By lightly fine-tuning the model, VLP-based methods outperform the previous end-to-end network architectures, including twostream networks [54, 63, 78], 3D convolutional neural networks [6, 14, 15, 22, 48, 58, 59, 67], and vision-transformerbased (ViT) networks [3, 12, 38, 46, 70]. Employing a pretrained VLP model for action recognition can better encode the semantic meaning of items in images, even if they have very different visual appearances. This is very helpful in understanding human action and also explains why VLP models have achieved superior performance. As shown in Fig. 1, the current VLP-based action recognition methods follow an “adapt then align” paradigm. They either introduce temporal interaction upon image representations or insert temporal modules into pre-trained image encoders. However, the “adapt then align” paradigm will merely fit the video representation to the action name, which potentially destroys the other visual-semantic correspondences provided by VLP models. As far as we are concerned, actions are complex concepts that involve multiple fine-grained entities, such as body parts, scenes, and objects. With VLP model, the text embedding of the relevant entities should also be grounded in some image region. “adapt then align” paradigm does not emphasize the underlying entities-to-regions correspondence behind the action concept. Furthermore, human-centric activities often share common entities, implying that visuallanguage correspondences can be reused across different actions, even for those that were not included in the training set. The re-usability of entities-to-regions correspondences allow the model to quickly recognize new action categories. In this paper, we propose the “ALign before adapt” (ALT) paradigm. Unlike the “adapt then align” approaches that align the image-level visual embedding with the text embedding of the action name, ALT aims to establish an entity-level correspondence to support action recognition. The relevant entities should have evidence in specific regions of the image. To achieve entities-to-regions alignment, the VLP model is leveraged in two aspects: (1) Aggregating adjacent and similar image token embeddings from the VLP model. The resulting embedding typically represents a region containing the same entity. (2) Selecting the most relevant entities for each region by matching their image embeddings to the text This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18688",
        "chosen": "This paper proposes \"Align before Adapt\" (ALT), a novel paradigm for video action recognition that establishes entity-to-region alignments before adapting to video representation learning. ALT reuses visual-language alignments during adaptation and demonstrates competitive performance with low computational costs across fully supervised, zero-shot, and few-shot experiments.",
        "rejected": "ALT paradigm advances video action recognition by aligning entity-to-region before adapting to video-level representations. By grounding text embeddings of relevant entities into image regions, ALT bridges the gap between static images and complex activity semantics. It achieves competitive performance while maintaining low computational costs, outperforming previous methods in both zero-shot and few-shot scenarios."
    },
    {
        "prompt": "summary: We present the pioneering Large Visual Motion Model (LVMM), meticulously engineered to analyze the intrinsic dynamics encapsulated within real-world imagery. Our model, fortified with a wealth of prior knowledge extracted from billions of image pairs, demonstrates promising results in predicting a diverse spectrum of scene dynamics. As a result, it can infuse any generic image with authentic dynamic effects, enhancing its visual allure. Project page: https://github.com/densechen/ LVMM. 1. Introduction Recent progress in generative models [39], specifically conditional diffusion models [11,22], and large-scale models [39], have substantially enhanced our capability to represent complex and rich distributions. These models have underscored the transformative potential of harnessing vast data and intensive training [14], exhibiting unparalleled proficiency in comprehending and generating human-like text, and creating visually rich and diverse images from textual descriptions. This has facilitated a variety of previously unachievable applications, such as text-conditioned generation of arbitrary, realistic image content [21]. The advent of these models [5, 19, 24], propelled by the availability of large-scale datasets [25] and advancements in training methodologies [4, 20], has ignited interest in probing other domains, including audio [38] and multimodal data [15]. In this paper, we present a novel Large Visual Motion Model (LVMM), specifically designed to proficiently predict local motion embedded within a given scene, thereby enhancing the dynamic appeal of a static image. The dynamism of the natural world is characterized by subtle changes even in seemingly static landscapes, influenced by various factors such as wind, water currents, and inherent rhythms. When observing a still image, we can envisage plausible motions that might have been occurring when the (a) I0 (b) ˆz1,··· ,K (c) ˆp1,··· ,K (d) X-t slices (e) Capability to animate faces, generating natural and realistic smiles. Figure 1. Beginning with a reference image I0 as depicted in Fig. 1a, the Large Visual Motion Model (LVMM) estimates a latent motion trajectory ˆz1,··· ,K as shown by the t-SNE plot in Fig. 1b, utilizing the motion denoising model ϵθ. This trajectory is subsequently processed by the Motion Decoder D, generating a sequence of optical flows ˆδ1,··· ,K (visualized in the left column of Fig. 1c) and intention maps ˆω1,··· ,K (displayed in the right column of Fig. 1c). Ultimately, the Neural Image Renderer R transforms I0 into a series of novel images ˆI1,··· ,K, guided by optical flows and intention maps. Fig 1d illustrates the resultant videos, employing space-time X-t slices across 300 frames (corresponding to the scanline shown in Fig. 1a). photograph was captured. This predictability is ingrained in our human perception of real scenes, i.e., we can imagine a distribution of natural motions conditioned on that image if there could have been multiple possible motions. Given the ease with which humans can envision these potential motions, an intriguing research question is to computationally model this motion distribution with a large-scale model. The proposed LVMM excels in associating salient visual and motion patterns, thereby accurately predicting local motion trajectory, as shown in Fig. 1. It comprises two components: the motion rendering model and the motion diffusion model. The former extracts a latent motion vector from the scene and reconstructs the target image. The latter generates suitable motion trajectories from the given scene and feeds them to the motion rendering model to produce realistic dynamic effects. Our primary contribution is the pioneering proposal and This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 7131",
        "chosen": "This paper presents LVMM, a large visual motion model designed to predict local motion within scenes. The model comprises a motion rendering model and a motion diffusion model, and it can infuse static images with dynamic effects. The authors demonstrate promising results in predicting diverse scene dynamics.",
        "rejected": "LVMM predicts scene dynamics to animate static images, enhancing visual allure. The model extracts latent motion vectors and generates motion trajectories using a diffusion model. LVMM accurately predicts local motion trajectory, producing realistic dynamic effects. Its contributions include pioneering the prediction of diverse scene dynamics and introducing mechanisms for motion rendering and diffusion."
    },
    {
        "prompt": "summary: Recent advancements in text-to-image technology have significantly advanced the field of image customization. Among various applications, the task of customizing diverse scenes for user-specified composited elements holds great application value but has not been extensively explored. Addressing this gap, we propose AnyScene, a specialized framework designed to create varied scenes from composited foreground using textual prompts. AnyScene addresses the primary challenges inherent in existing methods, particularly scene disharmony due to a lack of foreground semantic understanding and distortion of foreground elements. Specifically, we develop a foreground injection module that guides a pre-trained diffusion model to generate cohesive scenes in visual harmony with the provided foreground. To enhance robust generation, we implement a layout control strategy that prevents distortions of foreground elements. Furthermore, an efficient image blending mechanism seamlessly reintegrates foreground details into the generated scenes, producing outputs with overall visual harmony and precise foreground details. In addition, we propose a new benchmark and a series of quantitative metrics to evaluate this proposed image customization task. Extensive experimental results demonstrate the effectiveness of AnyScene, which confirms its potential in various applications. 1. Introduction Recently, text-to-image (T2I) synthesis models [24, 28, 30, 34] have seen rapid advancements and gained significant attention due to their ability to generate high-fidelity images from textual prompts. Among the various applications that harness T2I technology to enhance design efforts, generating diverse scenes tailored to specific composited foregrounds is valuable. It is effective in image editing tasks demanding background alteration and creation, such as situating e-commerce products in customized scenes, depicting objects in various environmental settings, etc. Despite its clear utility, this task of “customizing diverse scenes for 1† Equal contribution. 2∗Corresponding author: An-An Liu (anan0422@gmail.com). Figure 1. Our proposed AnyScene is capable of synthesizing highquality scenes that align with textual prompts based on the given foregrounds. Compared to previous alternative methods [42, 45], AnyScene provides precise control over the introduced foreground and generates visually harmonious images. composited foreground” has not been extensively explored. Currently, several alternative methods [42, 45] can achieve this task. One such method is employing the canny edge as a control condition [23, 45], combined with overlaying the original foreground onto the generated image, often requiring manual adjustments to achieve visual harmony. Another strategy involves inpainting models [30, 42] that construct backgrounds by painting around the exterior of a masked foreground area. Furthermore, in cases where the foreground comprises only one or two specific objects, subject-driven techniques [17, 19, 32] have shown proficiency in generating customized images. However, these methods have limitations in practical applications. The method of directly controlling the foreground edge [45] does not consider the visual context, leading to scenes that lack harmony (e.g. Fig. 2(a)). Moreover, inpainting-based approaches [30, 42], though capable of creating cohesive scenes, are commonly trained under a random area recovering paradigm, which may distort the foreground elements or cause unintended regeneration of elements from foreground descriptions (e.g. Fig. 2(b)). Finally, subject-driven methods [17, 19, 32] often compromise high-level details of objects (e.g. Fig. 2(c)), leading to significant distortions of details such as logos, text, etc, which is problematic for the applications like commodity poster design. Meanwhile, their limited capacity to comThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 8724",
        "chosen": "This paper introduces AnyScene, a framework for creating customized scenes from composited foregrounds using textual prompts. It includes a foreground injection module, layout control strategy, and image blending mechanism. The framework addresses scene disharmony and foreground distortion issues and shows effectiveness in various applications.",
        "rejected": "AnyScene customizes scenes based on composited foreground elements using textual prompts. It addresses disharmony and distortion issues by developing a foreground injection module, layout control strategy, and efficient image blending mechanism. AnyScene ensures visual harmony and precise foreground details, demonstrating its effectiveness in various applications through extensive experiments."
    },
    {
        "prompt": "summary: This work introduces ArtAdapter, a transformative textto-image (T2I) style transfer framework that transcends traditional limitations of color, brushstrokes, and object shape, capturing high-level style elements such as composition and distinctive artistic expression. The integration of a multilevel style encoder with our proposed explicit adaptation mechanism enables ArtAdapter to achieve unprecedented fidelity in style transfer, ensuring close alignment with textual descriptions. Additionally, the incorporation of an Auxiliary Content Adapter (ACA) effectively separates content from style, alleviating the borrowing of content from style references. Moreover, our novel fast finetuning approach could further enhance zero-shot style representation while mitigating the risk of overfitting. Comprehensive evaluations confirm that ArtAdapter surpasses current state-ofthe-art methods. 1. Introduction Bridging the realms of artificial intelligence and artistic creativity, text-to-image (T2I) style transfer [13, 24, 25] stands out as a captivating domain that masterfully transforms textual descriptions into visually rich and stylistic representations. The core challenge lies in not just generating text-resonant images but infusing them with artistic depth and nuance, spanning from delicate brushstrokes to bold compositional elements—thereby capturing the essence of artistic vision. Conventional arbitrary style transfer (AST) methods [4, 7–9, 15, 21, 28, 54] typically struggle beyond low-level features such as medium and colors, failing to grasp the more sophisticated realms of artistic expression. Diffusion approaches [25, 55], including Textual Inversion [14], DreamBooth [39], and Low-Rank Adaptation (LoRA) [2, 20], have shown potential in style representation. Yet, these methods are hindered by laborious finetuning proThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 8619",
        "chosen": "This paper presents ArtAdapter, a text-to-image style transfer framework that captures high-level style elements. It integrates a multi-level style encoder with an explicit adaptation mechanism and an Auxiliary Content Adapter. The framework achieves unprecedented fidelity in style transfer and surpasses current state-of-the-art methods.",
        "rejected": "ArtAdapter transforms text descriptions into stylistically rich images, capturing high-level style elements beyond color and brushstrokes. Featuring a multi-level style encoder and explicit adaptation mechanism, it separates content from style and employs fast finetuning to enhance zero-shot style representation. Comprehensive evaluations confirm ArtAdapter's superiority over current state-of-the-art methods."
    },
    {
        "prompt": "summary: Understanding how attention varies across individuals has significant scientific and societal impacts. However, existing visual scanpath models treat attention uniformly, neglecting individual differences. To bridge this gap, this paper focuses on individualized scanpath prediction (ISP), a new attention modeling task that aims to accurately predict how different individuals shift their attention in diverse visual tasks. It proposes an ISP method featuring three novel technical components: (1) an observer encoder to characterize and integrate an observer’s unique attention traits, (2) an observer-centric feature integration approach that holistically combines visual features, task guidance, and observer-specific characteristics, and (3) an adaptive fixation prioritization mechanism that refines scanpath predictions by dynamically prioritizing semantic feature maps based on individual observers’ attention traits. These novel components allow scanpath models to effectively address the attention variations across different observers. Our method is generally applicable to different datasets, model architectures, and visual tasks, offering a comprehensive tool for transforming general scanpath models into individualized ones. Comprehensive evaluations using valuebased and ranking-based metrics verify the method’s effectiveness and generalizability. 1. Introduction Saccadic eye movements, such as fixations and saccades, enable individuals to shift their attention quickly and redirect their focus to different points in the visual field. Studying various factors driving people’s eye movements is important for understanding human attention and developing human-like attention systems. Computational models predicting eye movements have broad impacts across various domains, such as assessing image and video quality [8, 27, 47], developing intuitive human-computer interaction systems [33, 40, 55, 64, 67], creating immersive virtual reality experiences [1, 57, 58], enhancing the safety and efficiency of autonomous vehicles [28, 77, 78], and diag(a) (b) (c) Figure 1. Understanding and predicting the distinct eye movements of each observer is the key objective of individualized scanpath prediction. These examples reveal the variations in the scanpaths of different observers, showing their distinct attention preferences in (a) faces, (b) objects, and (c) background. Each dot represents a fixation, with the number and radius indicating its order and duration, respectively. The blue and red dots indicate the beginning and the end of the scanpath, respectively. nosing neurodevelopmental conditions [11, 22, 39]. While existing models of saccadic eye movements predominantly focus on modeling generic gaze patterns manifested as observer-agnostic scanpaths (i.e., a spatiotemporal sequence of fixations), this work seeks to model the individual variations in eye movements. As shown in Figure 1, there exists significant inter-observer variations in visual scanpaths. Such variations can be attributed to a multitude of individual characteristics, such as gender, age, and neurodevelopmental conditions [56, 61]. For instance, females show more explorative gaze patterns than males [53, 62, 63], older adults prefer faces [54] and objects with high color visibility [74], individuals with neurodevelopmental disorders, such as autism spectrum disorder (ASD), may show a preference for repetitive patterns This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25420",
        "chosen": "This paper focuses on individualized scanpath prediction (ISP) to model individual variations in eye movements. The proposed method includes an observer encoder, observer-centric feature integration, and adaptive fixation prioritization. The approach effectively transforms general scanpath models into individualized ones and shows strong generalizability.",
        "rejected": "ISP method focuses on predicting individual scanpaths during visual tasks. It integrates observer-specific characteristics, combines task guidance with visual features, and refines predictions using an adaptive fixation prioritization mechanism. The approach effectively addresses attention variations across individuals, verified through comprehensive evaluations on various datasets and visual tasks."
    },
    {
        "prompt": "summary: How to effectively explore multi-scale representations of rain streaks is important for image deraining. In contrast to existing Transformer-based methods that depend mostly on single-scale rain appearance, we develop an end-to-end multi-scale Transformer that leverages the potentially useful features in various scales to facilitate high-quality image reconstruction. To better explore the common degradation representations from spatially-varying rain streaks, we incorporate intra-scale implicit neural representations based on pixel coordinates with the degraded inputs in a closed-loop design, enabling the learned features to facilitate rain removal and improve the robustness of the model in complex scenarios. To ensure richer collaborative representation from different scales, we embed a simple yet effective inter-scale bidirectional feedback operation into our multi-scale Transformer by performing coarse-to-ﬁne and ﬁne-to-coarse information communication. Extensive experiments demonstrate that our approach, named as NeRDRain, performs favorably against the state-of-the-art ones on both synthetic and real-world benchmark datasets. The source code and trained models are available at https: //github.com/cschenxiang/NeRD-Rain. 1. Introduction Recent years have witnessed signiﬁcant progress in image deraining due to the development of numerous deep convolutional neural networks (CNNs) [15, 22, 51, 60, 63]. However, as the basic operation in CNNs, the convolution is spatially invariant and has limited receptive ﬁelds, which cannot effectively model the spatially-variant property and non-local structures of clear images [49, 62]. Moreover, simply increasing the network depth to obtain larger receptive ﬁelds does not always lead to better performance. To alleviate this problem, several recent approaches utilize Transformers to solve single image deraining [5, 8, 20, 55, 56, 62], since Transformers can model the non-local information for better image restoration. Although these approaches achieve better performance than most of the CNN∗Corresponding author. INR INR (a) Coarse-to-ﬁne (b) Multi-patch (c) Ours Figure 1. Illustration of the proposed approach and the currently existing multi-scale solutions. (a) coarse-to-ﬁne scheme [22, 63]; (b) multi-patch scheme [61]; (c) our method. Compared to previous approaches, the method one integrates implicit neural representations (INR) into our bidirectional multi-scale model to form a closed-loop framework, which allows for better exploration of multi-scale information and modeling of complex rain streaks. based ones, they mostly explore feature representations at a ﬁxed image scale (i.e., a single-input single-output architecture), while ignoring potentially useful information from other scales. As the rain effect decreases signiﬁcantly at coarser image scales, exploring the multi-scale representation would facilitate the rain removal. To this end, several approaches introduce the coarse-toﬁne mechanism [12, 47] or multi-patch strategy [61] into deep neural networks to exploit multi-scale rain features. As shown in Figure 1, the decoder’s feature or derained image is initially estimated at a coarse scale and then used as the input at a ﬁner scale for guidance. Although impressive performance has been achieved, these methods are less effective when handling complex and random rain streaks because these rain streaks cannot be removed by downsampling operations and inaccurate estimation of a coarser scale will result in suboptimal restoration performance at ﬁner scales. Despite the spatially-varying rain streaks exhibit a variety of scale properties (e.g., size, shape, length, and density), the degraded rainy images tend to share some similar visual degradation characteristics (i.e., common rain degradation representation) [52, 53, 56]. However, existing methods do not effectively model the common degradation as they usually rely on traditional representation forms that are sensitive to the input variation rather than capturing underlying implicit functions, which limits their performance on complex scenarios. Thus, it is of great interest to learn the underlying correlations among features to encode rain This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25627",
        "chosen": "This paper develops an end-to-end multi-scale Transformer for image deraining, incorporating intra-scale implicit neural representations and inter-scale bidirectional feedback. The method, named NeRD-Rain, explores multi-scale representations and demonstrates favorable performance against state-of-the-art approaches on various benchmark datasets.",
        "rejected": "This paper proposes an efficient data-driven solution for self-localization within a floorplan, which does not require retraining per map or location and avoids needing a large image database of the area. The method introduces a novel probabilistic model with an observation module (comprising single and multiview components) and a temporal filtering module. It operates using a ray-based representation internally and is designed to work on conventional consumer hardware, overcoming common limitations such as the need for upright images. The system meets real-time requirements and outperforms state-of-the-art methods significantly."
    },
    {
        "prompt": "summary: An effective pre-training framework with universal 3D representations is extremely desired in perceiving largescale dynamic scenes. However, establishing such an ideal framework that is both task-generic and label-efficient poses a challenge in unifying the representation of the same primitive across diverse scenes. The current contrastive 3D pre-training methods typically follow a framelevel consistency, which focuses on the 2D-3D relationships in each detached image. Such inconsiderate consistency greatly hampers the promising path of reaching an universal pre-training framework: (1) The cross-scene semantic self-conflict, i.e., the intense collision between primitive segments of the same semantics from different scenes; (2) Lacking a globally unified bond that pushes the cross-scene semantic consistency into 3D representation learning. To This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 19925",
        "chosen": "This paper addresses the challenge of establishing a universal pre-training framework for large-scale dynamic scene perception. The authors propose a method to unify cross-scene semantic consistency in 3D representation learning, tackling issues like cross-scene semantic self-conflict and the lack of a globally unified bond in current contrastive 3D pre-training methods.",
        "rejected": "This research explores Handwritten Mathematical Expression Generation (HMEG) from symbolic sequences, framing it as a graph-to-image (G2I) generation problem. To address the lack of real masks for supervision, the authors propose an end-to-end G2I pipeline consisting of graph to layout, layout to mask, and mask to image stages. They introduce a Less-is-More (LiM) learning strategy to enhance prediction accuracy and a differentiable layout refinement module to clarify ambiguous areas. Their model, optimized in an end-to-end manner, generates high-quality HME images, surpassing previous generative methods. Additionally, the generated images improve HME recognition models through data augmentation."
    },
    {
        "prompt": "summary: The rebroadcasting of screen-recaptured document images introduces a signiﬁcant risk to the conﬁdential documents processed in government departments and commercial companies. However, detecting recaptured document images subjected to distortions from online social networks (OSNs) is challenging since the common forensics cues, such as moir´e pattern, are weakened during transmission. In this work, we ﬁrst devise a pixel-level distortion model of the screen-recaptured document image to identify the robust features of color artifacts. Then, we extract a chromaticity map from the recaptured image to highlight the presence of color artifacts even under low-quality samples. Based on the prior understanding, we design a chromaticity map adapter (CMA) to efﬁciently extract the chromaticity map, and feed it into the transformer backbone as multi-modal prompt tokens. To evaluate the performance of the proposed method, we collect a recaptured ofﬁce document image dataset with over 10K diverse samples. Experimental results demonstrate that the proposed CMA method outperforms a SOTA approach (with RGB modality only), reducing the average EER from 26.82% to 16.78%. Robustness evaluation shows that our method achieves 0.8688 and 0.7554 AUCs under samples with JPEG compression (QF=70) and resolution as low as 534×503 pixels. 1. Introduction Document images, such as certiﬁcates, contracts, and identity documents, are gaining popularity in e-business and e-government applications, which brings both convenience and threat to our applications. Traditionally, an organization controls the distribution of hard-copy documents to guard *B. Li is the corresponding author. Screenrecaptured Document  Image Genuine Document  Image (a) RGB image (d) Chromaticity (ours) (c) Spectrum (previous) (b) LBP (previous) Figure 1. Illustrations of the low-quality genuine (top row) and recaptured (bottom row) samples and their transformed domain representations. (a) Image patches in RGB space. The samples are collected by Canon C3530 (4800×1200 DPI) printer, OnePlus 5T camera (resolution at 1280×960 pixels), Dell P2418D display (resolution at 2560×1440 pixels, size 23.8 inches), and subject to JPEG compression with a quality factor of 70. (b) LBP maps with a radius 1 containing 8 elements [40]. (c) Amplitude spectrum in the Fourier domain. (d) Chromaticity map extracted by Eq. (5) in our work. The color artifacts highlighted in (d) show clear differences between the genuine and recaptured samples. against the leakage of conﬁdential information. Many ofﬁce documents with conﬁdential information can be viewed on screen for a limited time but cannot be printed as hard copies [29]. However, illegal users could acquire the genuine document image shown on display with their smartphones and distribute the recaptured document image without being noticed [2, 31]. Worse still, these images are usually transmitted through online social networks (OSN), e.g., WhatsApp, and WeChat, which introduces further distortions (compression, resizing, etc.) to the document images. Thus, there is a pressing need to develop a recaptured document detection scheme robust to various distortions. Existing works on recaptured image detection focus on face and natural images. For instance, literature exploits physical traces of distortion, e.g., specularity distribution [52], color saturation [7], edge blurriness [45] and moir´e This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 15577",
        "chosen": "This paper presents a method to detect screen-recaptured document images using a chromaticity map adapter (CMA). The CMA extracts chromaticity maps highlighting color artifacts, which are fed into a transformer backbone. Evaluated on a dataset with over 10K samples, the method reduces average EER from 26.82% to 16.78% and shows robustness under JPEG compression and low resolution.",
        "rejected": "This paper introduces a method for detecting screen-recaptured document images by analyzing color artifacts using a chromaticity map adapter (CMA) and transformer backbone. The proposed approach improves upon existing state-of-the-art methods, reducing the average Equal Error Rate (EER) from 26.82% to 16.78%, even under JPEG compression and low-resolution conditions."
    },
    {
        "prompt": "summary: Passive depth estimation based on stereo or defocus relies on the presence of the texture on an object to resolve its depth. Hence, recovering the depth of a textureless object— for example, a large white wall—is not just hard but perhaps even impossible. Or is it? We show that spatial coherence, a property of natural light sources, can be used to resolve the depth of a scene point even when it is textureless. Our approach relies on the idea that natural light scattered off a scene point is locally coherent with itself, while incoherent with the light scattered from other surface points; we use this insight to design an optical setup that uses selfinterference as a texture feature for estimating depth. Our lab prototype is capable of resolving depths of textureless objects in sunlight as well as indoor lights. 1. Introduction Shape recovery is a problem of fundamental importance in computer vision, where the goal is to recover a 3D description of a scene from one or more images. In passive settings, shape information can be recovered from disparity [13], shading [23, 24], focus [7], defocus [16], motion [15], or even polarization [19, 22]. All of these approaches, however, rely on making certain assumptions about the scene, limiting the generality of each approach. Consider, for example, the scenario shown in Figure 1(b), where a camera measures a textureless surface with a uniform albedo and lit by an unknown white light source. At ﬁrst glance, the 3D reconstruction problem appears to be underconstrained. It has been widely accepted that it is impossible to passively reconstruct the depth of such a textureless plane through stereo imaging or depth from (de)focus; irrespective of the viewing angle and focus settings of the camera, the captured images will always be uniform in brightness, providing no visual information that can be used for 3D reconstruction. So when is it possible to see the shape of a textureless *Corresponding author: wyharveychen@gmail.com (a) 2mm (b) (c) (d) x y z y Figure 1. Passive 3D reconstruction of a textureless plane under white, incoherent illumination. (a) A photo of the capture setup. A tilted plane target (enlarged in the inset) lies in front of the scanning lens of our setup. (b) Front view of the target. Within the sensor area, it is a uniform, textureless plane target. (c) Depth estimation with our approach, resolving the desired tilt. Note that we can measure the explicit depth, which can not be recovered by other passive methods that only measure normals. (d) A side view of the reconstructed target reveals it is a tilted plane. object, under unknown illumination? The classic aperture problem [14] tells us that correspondences cannot be found for textureless objects and hence, disparity across viewpoints is unobservable. Shape from shading [23, 24] and shape from polarization [19, 22] permit the recovery of the normal of a textureless plane, but not its depth. Sundaram et al. [20] conclude that the depth of a textureless plane can be reconstructed, but only if the plane is heavily tilted with respect to the camera viewpoint. To overcome the lack of texture, numerous works apply active imaging approaches such as structured light [2] or time-of-ﬂight cameras [5, 8]. Kotwal et al. [10] have demonstrated that OCT based depth acquisition can be implemented under sunlight without coherent laser illumination, yet their approach is not fully passive because it uses optics to control and direct the sunlight This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25058",
        "chosen": "This paper introduces a novel approach for passive 3D reconstruction of textureless objects using spatial coherence of natural light. The method leverages self-interference as a texture feature, enabling depth estimation of textureless surfaces. A lab prototype demonstrates successful depth resolution of textureless objects in various lighting conditions.",
        "rejected": "The research presents an innovative technique for depth estimation of textureless objects through spatial coherence in natural light sources. By utilizing self-interference as a texture feature, the optical setup can estimate depths accurately under both sunlight and indoor lighting conditions, overcoming traditional limitations of passive depth estimation methods."
    },
    {
        "prompt": "summary: This paper proposes ConsistDreamer – a novel framework that lifts 2D diffusion models with 3D awareness and 3D consistency, thus enabling high-fidelity instructionguided scene editing. To overcome the fundamental limitation of missing 3D consistency in 2D diffusion models, our key insight is to introduce three synergistic strategies that augment the input of the 2D diffusion model to become 3D-aware and to explicitly enforce 3D consistency during the training process. Specifically, we design surrounding views as context-rich input for the 2D diffusion model, and generate 3D-consistent structured noise instead of image-independent noise. Moreover, we introduce selfsupervised consistency-enforcing training within the perscene editing procedure. Extensive evaluation shows that our ConsistDreamer achieves state-of-the-art performance for instruction-guided scene editing across various scenes and editing instructions, particularly in complicated largescale indoor scenes from ScanNet++, with significantly improved sharpness and fine-grained textures. Notably, ConsistDreamer stands as the first work capable of successfully editing complex (e.g., plaid/checkered) patterns. Our project page is at immortalco.github.io/ConsistDreamer. †Work started during an internship at Meta Reality Labs Zurich. 1. Introduction With the emergence of instruction-guided 2D generative models as in [2], it has never been easier to generate or edit images. Extending this success to 3D, i.e., instructionguided 3D scene editing, becomes highly desirable for artists, designers, and the movie and game industries. Nevertheless, editing 3D scenes or objects is inherently challenging. The absence of large-scale, general 3D datasets makes it difficult to create a counterpart generative model similar to [2] that can support arbitrary 3D scenes. Therefore, state-of-the-art solutions [8, 25] circumvent this challenge by resorting to generalizable 2D diffusion models. This approach, known as 2D diffusion distillation, renders the scene into multi-view images, applies an instructionconditioned diffusion model in 2D, and then distills the editing signal back to 3D, such as through a neural radiance field (NeRF) [6, 8, 15]. However, a fundamental limitation of this solution is the lack of 3D consistency: a 2D diffusion model, acting independently across views, is likely to produce inconsistent edits, both in color and shape. For example, a person in one view might be edited to be wearing a red shirt, while appearing in a green shirt in another view. Using these images to train a NeRF can still produce reasonable edits, but the model will naturally converge towards an “averaged” representation of the inconsistent 2D supervision, and lose This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 21071",
        "chosen": "This paper proposes ConsistDreamer, a framework enhancing 2D diffusion models with 3D awareness for high-fidelity scene editing. The approach introduces three strategies: using surrounding views as context, generating 3D-consistent noise, and self-supervised consistency training. ConsistDreamer achieves state-of-the-art performance in instruction-guided scene editing, particularly in complex indoor scenes.",
        "rejected": "ConsistDreamer is introduced as a novel framework that enhances 2D diffusion models with 3D awareness and consistency, enabling high-fidelity instruction-guided scene editing. It achieves state-of-the-art performance across various scenes, particularly improving sharpness and fine-grained textures in large-scale indoor scenes."
    },
    {
        "prompt": "summary: In this paper, we democratise caricature generation, empowering individuals to effortlessly craft personalised caricatures with just a photo and a conceptual sketch. Our objective is to strike a delicate balance between abstraction and identity, while preserving the creativity and subjectivity inherent in a sketch. To achieve this, we present Explicit Rank-1 Model Editing alongside single-image personalisation, selectively applying nuanced edits to cross-attention layers for a seamless merge of identity and style. Additionally, we propose Random Mask Reconstruction to enhance robustness, directing the model to focus on distinctive identity and style features. Crucially, our aim is not to replace artists but to eliminate accessibility barriers, allowing enthusiasts to engage in the artistry. 1. Introduction Ever wondered when you would finally decide to get that personalised caricature created, perhaps during a holiday? Look no further, this paper is for you – we strive to democratise caricature [6, 26, 27] generation for everyone! With a portrait of yourself and a conceptual sketch of how you envision your caricature, we will automatically generate a high-fidelity caricature that unmistakably captures This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 8629",
        "chosen": "This paper presents a method for generating personalized caricatures from a photo and a conceptual sketch. The approach combines Explicit Rank-1 Model Editing and single-image personalization, focusing on cross-attention layers. Random Mask Reconstruction is used to enhance robustness, ensuring a balance between identity and style.",
        "rejected": "A new method for generating personalized caricatures from photos and conceptual sketches is proposed, balancing abstraction and identity while preserving artistic creativity. Techniques like Explicit Rank-1 Model Editing and Random Mask Reconstruction are employed to enhance robustness and ensure seamless integration of identity and style features."
    },
    {
        "prompt": "summary: We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusionbased co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpaintingbased sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms the superiority of DiffSHEG over prior approaches. By enabling the real-time generation of expressive and synchronized motions, DiffSHEG showcases its potential for various applications in the development of digital humans and embodied agents. 1. Introduction Non-verbal cues such as facial expressions, body language, and hand gestures play a vital role in effective communication alongside verbal language [13, 43]. Speech-driven gesture and expression generation has gained significant interest in applications like the metaverse, digital human development, gaming, and human-computer interaction. Generating synchronized and realistic gestures and expressions based on speech is key to bringing virtual agents and digital humans to life in virtual environments. *The work was done during Junming’s internship at the International Digital Economy Academy. †Corresponding authors. … … … … journalist never tell lie … … want to be a journalist … Expression Generator Gesture Generator Diffusion Procedure Figure 1. DiffSHEG is a unified co-speech expression and gesture generation system based on diffusion models. It captures the joint expression-gesture distribution by enabling the uni-directional information flow from expression to gesture inside the model. Existing research on co-speech motion synthesis has focused on generating either expressions or gestures independently. Rule-based approaches [8, 18, 22, 38, 44] were prevalent initially, but recent advancements have leveraged data-driven techniques using deep neural networks. However, co-speech gesture generation poses a challenge due to its inherently many-to-many mapping. State-of-the-art methods have explored generative models such as normalizing flow models [48], VQ-VAE [2], GAN [14] and Diffusion models [1, 3, 47, 54]. These approaches have made progress in improving synchronization and diversifying generated gestures. However, none of them specifically target the co-speech generation of both expressions and gestures simultaneously. Recently, some works have aimed to generate co-speech holistic 3D expressions and gestures [14, 49]. These methods either combine independent co-speech expression and gesture models [49] or formulate the problem as a multitask learning one [14]. However, these approaches separate the generation process of expressions and gestures, neglecting the potential relationship between them. This can lead to disharmony and deviation in the joint expression-gesture distribution. Additionally, deterministic CNN-based models [14] may not be well-suited for approximating the manyto-many mapping inherent in co-speech gesture generation. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 7352",
        "chosen": "This paper proposes DiffSHEG, a diffusion-based approach for speech-driven holistic 3D expression and gesture generation. The method enables uni-directional information flow from expression to gesture and introduces an outpainting-based sampling strategy for generating sequences of arbitrary length. Evaluated on public datasets, DiffSHEG achieves state-of-the-art performance.",
        "rejected": "DiffSHEG is a diffusion-based model designed for speech-driven holistic 3D expression and gesture generation, capable of producing synchronized expressions and gestures from arbitrary length audio inputs. Evaluation on public datasets shows superior qualitative and quantitative performance, offering potential applications in digital human development and embodied agents."
    },
    {
        "prompt": "summary: We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public benchmarks validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics. 1. Introduction Image inpainting is the task of filling the missing pixels of a masked image with appropriate contents that are coherent to its visible regions. As a long-studied topic in computer vision, image inpainting has evolved from a restoration technique solely relying on existing information from the input image (e.g. [3]) to data-driven generative methods (e.g. [23, 27, 36, 41, 44, 48]) that hallucinates detailed contents from not only the observable pixels but also learned, rich image priors. Pluralistic inpainting refers to the ability of a model to generate multiple plausible results that complete a partial image. It offers a view of image inpainting as a generative method that models the smooth distributions of the complete images given the partial image as prior information [48]. However, modeling such distributions is challengThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 7591",
        "chosen": "This paper presents a method for large-mask pluralistic image inpainting using discrete latent codes. The approach includes a restrictive partial encoder, a bidirectional transformer, and a synthesis network. Experiments show the method outperforms baselines in visual quality and diversity metrics.",
        "rejected": "This paper describes a pluralistic image inpainting method based on discrete latent codes, which generates coherent and diverse complete images even under extreme mask settings. The approach outperforms strong baselines in visual quality and diversity metrics, demonstrating its effectiveness in handling challenging inpainting tasks."
    },
    {
        "prompt": "summary: We present DRESS , a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction ﬁnetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multiturn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and reﬁnement. The critique NLF identiﬁes the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The reﬁnement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs– which focuses on LVLMs’ ability to reﬁne responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVLMs. 1. Introduction Large vision-language models (LVLMs) can perceive the visual world and follow the instructions to generate userfriendly responses [6, 43, 90]. This is achieved by effectively combining large-scale visual instruction ﬁnetuning [78] with *Work done during internship at SRI International. Figure 1. We instruct DRESS to improve both alignment with human preferences and interaction ability via natural language feedback, which is categorized into critique and reﬁnement. large language models (LLMs) [5, 53]. However, existing LVLMs solely leverage the LLMsgenerated or hand-crafted datasets to achieve alignment via supervised ﬁne-tuning (SFT) [6, 43, 78]. While it’s effective at transforming LVLMs from caption generators to instruction-following models, LVLMs can still generate responses that are unhelpful, hallucinated, or even harmful (see Figure 4). This indicates that their present level of alignment with human preference is still relatively low [81]. In addition, although existing work motivates to structure visual instruction tuning samples in multi-turn formats, the connection and dependencies among various turns are weak, which restricts the interaction ability of the LVLMs. Here the interaction ability measures whether LVLMs can effectively leverage the previous context in multi-turn interactions and reﬁne their responses [72]. These two limitations restrict the potential of LVLMs to serve as visual assistants in practice. In this work, we introduce DRESS , an LVLM distinctively trained through the application of Natural This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 14239",
        "chosen": "This paper introduces DRESS, a large vision-language model that uses natural language feedback (NLF) for enhanced alignment and interaction. The model categorizes NLF into critique and refinement types, improving response quality and multi-turn interaction ability. Experimental results demonstrate DRESS's superiority over existing LVLMs.",
        "rejected": "DRESS, a large vision language model, leverages Natural Language Feedback (NLF) to improve alignment and multi-turn interaction capabilities. It categorizes NLF into critique and refinement types to better align responses with human preferences and improve interaction abilities. Experimental results show significant improvements in helpfulness, honesty, and harmlessness compared to state-of-the-art LVLMs."
    },
    {
        "prompt": "summary: Self-supervised denoising has attracted widespread attention due to its ability to train without clean images. However, noise in real-world scenarios is often spatially correlated, which causes many self-supervised algorithms that assume pixel-wise independent noise to perform poorly. Recent works have attempted to break noise correlation with downsampling or neighborhood masking. However, denoising on downsampled subgraphs can lead to aliasing effects and loss of details due to a lower sampling rate. Furthermore, the neighborhood masking methods either come with high computational complexity or do not consider local spatial preservation during inference. Through the analysis of existing methods, we point out that the key to obtaining high-quality and texture-rich results in real-world selfsupervised denoising tasks is to train at the original input resolution structure and use asymmetric operations during training and inference. Based on this, we propose Asymmetric Tunable Blind-Spot Network (AT-BSN), where the blindspot size can be freely adjusted, thus better balancing noise correlation suppression and image local spatial destruction during training and inference. In addition, we regard the pre-trained AT-BSN as a meta-teacher network capable of generating various teacher networks by sampling different blind-spots. We propose a blind-spot based multi-teacher distillation strategy to distill a lightweight network, significantly improving performance. Experimental results on multiple datasets prove that our method achieves state-ofthe-art, and is superior to other self-supervised algorithms in terms of computational overhead and visual effects. 1. Introduction Image denoising is an essential low-level computer vision problem. With the advancements in deep learning, an in*Corresponding author. (a) Noisy Input (b) AP-BSN (R3) (c) LG-BPN   (d) SDAP (E) (e) SpatiallyAdaptive (f) Ours  AT-BSN  Figure 1. Comparisons of our AT-BSN with other methods. Our method recovers more high frequency texture details. creasing number of studies are focused on supervised learning using clean-noisy pairs [2, 17, 32, 47–49]. Typically, additive white Gaussian noise (AWGN) is introduced into clean datasets to synthesize clean-noisy denoising datasets. However, real-world noise is known to be spatially correlated [7, 23, 37]. Some generative-based methods attempt to synthesize real-world noise from existing clean data [5, 8, 18, 21, 43]. However, synthesizing real-world noise remains challenging, and suffers from generalization issues. To address the issue, some researchers attempt to capture clean-noisy pairs in real-world scenarios [1, 4]. However, in certain scenarios, such as medical imaging and electron microscopy, constructing such datasets can be impractical or even infeasible. Self-supervised denoising algorithms, represented by Noise2Noise [28], have brought new life to the denoising field. These methods only require noisy observations to train the denoising model. However, in real-world scenarios, noise often exhibits spatial correlation, which contradicts the pixel-wise independent noise assumption [25, 28] This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 2814",
        "chosen": "This paper proposes an asymmetric tunable blind-spot network (AT-BSN) for self-supervised denoising in real-world scenarios. The method balances noise correlation suppression and image local spatial preservation, using asymmetric operations during training and inference. The approach achieves state-of-the-art performance with lower computational overhead.",
        "rejected": "AT-BSN is introduced as a solution for real-world self-supervised denoising, addressing the challenge of spatially correlated noise. This asymmetric tunable blind-spot network balances noise correlation suppression and local spatial preservation, achieving state-of-the-art performance in terms of computational overhead and visual effects."
    },
    {
        "prompt": "summary: In this paper we propose an efﬁcient data-driven solution to self-localization within a ﬂoorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal ﬁltering module. Operating internally with an efﬁcient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to beneﬁt from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods [16, 17, 20, 28] that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art [20, 28] by a signiﬁcant margin. 1. Introduction Camera localization is an essential research topic in computer vision. It is key to many AR/VR applications for head-mounted or handheld mobile devices and is of great practical interest to the robotics community. Most existing works localize the camera using a pre-collected database [40][1][2] or within a pre-built 3D model [24, 34, 37– 39]. However, these representations of the environment are costly in terms of storage and maintenance. In contrast, indoor environments including most commercial real estate such as warehouses, ofﬁces and apartments already possess a ﬂoorplan. The ﬂoorplan is a generic representation for indoor environments that is easily accessible, lightweight, and preserves long-term scene structure independent of a changing visual appearance, such as the furnishing of the scene. It This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18029",
        "chosen": "This paper presents F3Loc, a method for self-localization within a floorplan using a probabilistic model with observation and temporal filtering modules. The method operates on consumer hardware and meets real-time requirements, outperforming existing approaches.",
        "rejected": "An efficient data-driven solution for camera localization within floorplans is proposed, utilizing a probabilistic model with observation and temporal filtering modules. The method operates on conventional consumer hardware and meets real-time requirements, significantly outperforming current state-of-the-art methods."
    },
    {
        "prompt": "summary: In this paper, we explore a novel challenging generation task, i.e. Handwritten Mathematical Expression Generation (HMEG) from symbolic sequences. Since symbolic sequences are naturally graph-structured data, we formulate HMEG as a graph-to-image (G2I) generation problem. Unlike the generation of natural images, HMEG requires critic layout clarity for synthesizing correct and recognizable formulas, but has no real masks available to supervise the learning process. To alleviate this challenge, we propose a novel end-to-end G2I generation pipeline (i.e. graph →layout →mask →image), which requires no real masks or nondifferentiable alignment between layouts and masks. Technically, to boost the capacity of predicting detailed relations among adjacent symbols, we propose a Less-is-More (LiM) learning strategy. In addition, we design a differentiable layout refinement module, which maps bounding boxes to pixel-level soft masks, so as to further alleviate ambiguous layout areas. Our whole model, including layout prediction, mask refinement, and image generation, can be jointly optimized in an end-to-end manner. Experimental results show that, our model can generate highquality HME images, and outperforms previous generative methods. Besides, a series of ablations study demonstrate effectiveness of the proposed techniques. Finally, we validate that our generated images promisingly boosts the performance of HME recognition models, through data augmentation. Our code and results are available at: https: //github.com/AiArt-HDU/HMEG. 1. Introduction Handwritten Mathematical Expressions (HMEs) are common and play significant roles in our daily life, especially in the research and education areas. HMEs generally present complex structures, serious deformations, and diverse writ∗Equal Contributions. † Corresponding Author. (a) (b) BBox-to-Mask Transformation End-to-end Training Layout Prediction Image  Decoder  real image Image Layout Discriminator BBox  Indices Input  Graph Layout Prediction Mask Prediction Image  Decoder  real layout real mask Image real image real layout  Stage-II Stage-I Scene Graph human horse ride Symbol Graph x 2 sup Figure 1. Differences between (a) typical two-stage graph-toimage generation pipeline and (b) our end-to-end pipeline. In previous methods, the real masks are available as the input. In contrast, we propose a novel end-to-end pipeline of graph → layout →mask →image, and requires no real masks or nondifferentiable alignment between layouts and masks. ing styles. Such characteristics, along with data scarcity, make HME Recognition (HMER) a grand challenge in the OCR community. Despite the tremendous efforts that have been made to this task, the HMER performance is still unsatisfactory [14, 27, 62]. Recently, synthetic data augmentation has shown inspiring performance in various recognition tasks [23, 51]. This is mainly achieved by the remarkable progress of conditional image generation [21] in the past few years [58]. The advances allow for creating realistic images through texts [39, 57, 66], scene graphs [24, 60], layouts [28, 69], semantic masks [37, 72], sketches [61, 67], and more [12, 65]. However, there has been no effective generative model for generating high-quality HME images. Some previous works synthesize HMEs by recomposing real online HMEs [46, 59]. Besides, FormulaGAN [41] generates HMEs from rendered images, via an Imageto-Image (I2I) Translation model. However, their generated images are limited in either diversity or realism. BeThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 15675",
        "chosen": "This paper explores generating handwritten mathematical expressions from symbolic sequences, formulating it as a graph-to-image generation problem. The proposed end-to-end pipeline includes layout prediction, mask refinement, and image generation. Experimental results show the model generates high-quality images and boosts recognition performance through data augmentation.",
        "rejected": "The paper explores Handwritten Mathematical Expression Generation (HMEG) as a graph-to-image generation problem, proposing an end-to-end pipeline with layout prediction, mask refinement, and image generation stages. The Less-is-More (LiM) learning strategy and differentiable layout refinement module improve detailed relation prediction among symbols, resulting in high-quality HME images."
    },
    {
        "prompt": "summary: In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable capabilities in representing 3D scenes. To expedite the rendering process, learnable explicit representations have been introduced for combination with implicit NeRF representation, which however results in a large storage space requirement. In this paper, we introduce the Context-based NeRF Compression (CNC) framework, which leverages highly efﬁcient context models to provide a storage-friendly NeRF representation. Speciﬁcally, we excavate both level-wise and dimension-wise context dependencies to enable probability prediction for information entropy reduction. Additionally, we exploit hash collision and occupancy grids as strong prior knowledge for better context modeling. To the best of our knowledge, we are the ﬁrst to construct and exploit context models for NeRF compression. We achieve a size reduction of 100⇥and 70⇥with improved ﬁdelity against the baseline Instant-NGP on Synthesic-NeRF and Tanks and Temples datasets, respectively. Additionally, we attain 86.7% and 82.3% storage size reduction against the SOTA NeRF compression method BiRF. Our code is available here: https://github.com/YihangChen-ee/CNC. 1. Introduction High-quality photo-realistic rendering at novel viewpoints remains a pivotal challenge in both computer vision and computer graphics. Traditional explicit 3D representations, such as voxel grids [17, 25, 34, 37], have earned their place due to their efﬁciency across numerous applications. However, their discrete nature makes them susceptible to the limitations imposed by the Nyquist sampling theorem, often necessitating exponentially increased memory for capturing detailed nuances. In the past few years, Neural Radiance Field (NeRF) [28] has emerged as a game-changer for novel view synthesis. NeRF deﬁnes both density and radiance at a 3D point as 1The size of INGP is calculated under 16 levels with resolution from 16 to 2048. The feature vector dimension is 2 and represented with FP32. σ c Rendering MLP Occupancy Grid Feature Embeddings: Context Models 3D Feature Embeddings Rendering MLP Occupancy Grid 45.56MB  33.76dB 3D !! … (x, d) concat Indexing 45.56MB  36.98dB 103x smaller 103x smaller … ∈−1; +1 … 3D=>2D 3D 2D … Dimension-wise Context Models Level-wise Context Models Hash  Fusion 2D 2D Feature Embeddings 0.444MB  34.35dB 0.442MB  37.31dB Context Models Figure 1. Motivation of our work. Instant-NGP represents 3D scenes using 3D hash feature embeddings along with a rendering MLP, which takes a non-negligible storage size with the embeddings accounting for over 99% of storage size (upper-left). To tackle this, we introduce context models to substantially compress feature embeddings, with the three key technical components (bottom-left). Our approach achieves a size reduction of over 100⇥while simultaneously improving ﬁdelity.1 functions of the 3D coordinates. Its implicit representation, encapsulated within a Multi-Layer Perceptron (MLP), captures continuous signals of a 3D scene seamlessly. Leveraging frequency-based positional embeddings of 3D coordinates [28, 41, 49], NeRF has showcased superior novel view synthesis quality in comparison to traditional explicit 3D representations. While NeRF exhibits good characteristics in memory efﬁciency and image quality, its complex queries of the MLP slow down its rendering speed. To boost NeRF’s rendering speed, recent approaches have converged towards a hybrid representation, merging explicit voxelized feature encoding with implicit neural networks. This combination promises faster rendering without compromising on quality. These methods include varied data structures such as dense grids [7, 38–40], octrees [26, 50], sparse voxel grids [24], and hash tables [29]. Among them, Instant-NGP (INGP) [29] which This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20321",
        "chosen": "This paper presents CNC, a framework for compressing NeRF representations using context models to reduce storage requirements. The approach leverages level-wise and dimension-wise context dependencies and utilizes hash collisions and occupancy grids for better context modeling. Experiments show significant size reductions and improved fidelity compared to baseline methods.",
        "rejected": "In recent years, Neural Radiance Field (NeRF) has shown remarkable 3D scene representation abilities but faced challenges with storage space due to explicit representations. This paper presents Context-based NeRF Compression (CNC), which utilizes context models for efficient compression while improving fidelity. CNC achieves significant size reduction compared to Instant-NGP and BiRF methods."
    },
    {
        "prompt": "summary: Diffusion models have shown an impressive ability to model complex data distributions, with several key advantages over GANs, such as stable training, better coverage of the training distribution’s modes, and the ability to solve inverse problems without extra training. However, most diffusion models learn the distribution of fixed-resolution images. We propose to learn the distribution of continuous images by training diffusion models on image neural fields, which can be rendered at any resolution, and show its advantages over fixed-resolution models. To achieve this, a key challenge is to obtain a latent space that represents photorealistic image neural fields. We propose a simple and effective method, inspired by several recent techniques but with key changes to make the image neural fields photorealistic. Our method can be used to convert existing latent diffusion autoencoders into image neural field autoencoders. We show that image neural field diffusion models can be trained using mixed-resolution image datasets, outperform fixed-resolution diffusion models followed by super-resolution models, and can solve inverse problems with conditions applied at different scales efficiently. 1. Introduction Diffusion models [16, 34, 50] have recently become attractive alternatives to GANs. These likelihood-based models †Equal advising. exhibit fewer artifacts, stable training, can model complex data distributions, do not suffer from mode collapse, and can solve inverse problems using the score function without extra training. Since diffusion typically requires many iterations at a fixed dimension, directly modeling the diffusion process in the pixel space [17, 40, 43] can be inefficient for high-resolution image synthesis. Latent diffusion models (LDMs) [41, 57] were proposed as a more efficient alternative. The key idea is to learn an autoencoder to map images to a latent representation from which the image can be decoded back, and train a diffusion model on the lower-dimensional latent representation. Despite their success, LDMs’ latent space still represents images at fixed resolution (for example, 256 in LDM [41] and 512 in Stable Diffusion). To generate higher-resolution images (e.g., 2K), LDMs usually first generate a low-resolution image and upsample it using a separate super-resolution model. In this work, we propose Image Neural Field Diffusion models (INFD). Our method is based on the latent diffusion framework, where we first learn a latent representation that represents an image neural field (which can be rendered at any resolution), then learn a diffusion model on this latent representation. A key challenge of our approach is to learn a latent space of photorealistic image neural fields where the diffusion model is applied. We propose a simple and effective method that can convert an existing autoencoder of latent diffusion models to a neural field autoencoder. We find that directly implementing an autoencoder with LIIF [8] This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 8007",
        "chosen": "This paper introduces Image Neural Field Diffusion models (INFD), which learn the distribution of continuous images by training diffusion models on image neural fields. The method converts existing latent diffusion autoencoders into image neural field autoencoders and demonstrates advantages over fixed-resolution models, including better performance on mixed-resolution datasets and efficient solving of inverse problems.",
        "rejected": "Diffusion models have demonstrated advantages over GANs in modeling complex data distributions. This work proposes Image Neural Field Diffusion models (INFD), capable of rendering images at any resolution and solving inverse problems efficiently without extra training. INFD outperforms fixed-resolution diffusion models followed by super-resolution models."
    },
    {
        "prompt": "summary: The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, visionlanguage tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models. † This work is done when they are interns at Shanghai AI Laboratory; B corresponding author (daijifeng@tsinghua.edu.cn) 1. Introduction Large language models (LLMs) largely promote the development of artificial general intelligence (AGI) systems with their impressive capabilities in open-world language tasks, and their model scale and performance are still increasing at a fast pace. Vision large language models (VLLMs) [3, 5, 19, 21, 28, 69, 87, 113, 147], which leverage LLMs, have also achieved significant breakthroughs, enabling sophisticated vision-language dialogues and interactions. However, the progress of vision and vision-language foundation models, which are also crucial for VLLMs, has lagged behind the rapid growth of LLMs. To bridge vision models with LLMs, existing VLLMs [5, 61, 100, 138, 147] commonly employ lightweight “glue” layers, such as QFormer [61] or linear projection [69], to align features of vision and language models. Such alignment contains several limitations: (1) Disparity in parameter scales. The large LLMs [38] now boosts up to 1000 billion parameters, while the widely-used vision encoders of VLLMs are still around one billion. This gap may lead to the under-use of LLM’s capacity. (2) Inconsistent representation. Vision models, trained on pure-vision data or This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 24185",
        "chosen": "This paper presents InternVL, a large-scale vision-language foundation model with 6 billion parameters. The model is progressively aligned with large language models (LLMs) using web-scale image-text data and achieves state-of-the-art performance on 32 visual-linguistic benchmarks. It offers powerful visual capabilities and can serve as an alternative to ViT-22B.",
        "rejected": "This paper introduces a large-scale vision-language foundation model named InternVL, aligning with large language models using web-scale image-text data. It performs well on various visual-linguistic benchmarks, offering a promising alternative to ViT-22B. The model bridges the gap between vision models and language models."
    },
    {
        "prompt": "summary: Speech-preserving facial expression manipulation (SPFEM) aims to modify facial emotions while meticulously maintaining the mouth animation associated with spoken content. Current works depend on inaccessible paired training samples for the person, where two aligned frames exhibit the same speech content yet differ in emotional expression, limiting the SPFEM applications in real-world scenarios. In this work, we discover that speakers who convey the same content with different emotions exhibit highly correlated local facial animations, providing valuable supervision for SPFEM. To capitalize on this insight, we propose a novel adaptive spatial coherent correlation learning (ASCCL) algorithm, which models the aforementioned correlation as an explicit metric and integrates the metric to supervise manipulating facial expression and meanwhile better preserving the facial animation of spoken contents. To this end, it first learns a spatial coherent correlation metric, ensuring the visual disparities of adjacent local regions of the image belonging to one emotion are similar to those of the corresponding counterpart of the image belonging to another emotion. Recognizing that visual disparities are not uniform across all regions, we have also crafted a disparity-aware adaptive strategy that prioritizes regions that present greater challenges. During SPFEM model training, we construct the adaptive spatial coherent correlation metric between corresponding local regions of the input and output images as addition loss to supervise the generation *Zhijing Yang is the corresponding author. Tianshui Chen, Jianman Lin, and Zhijing Yang are with Guangdong University of Technology. Chunmei Qing is with South China University of Technology. Liang Lin is with Sun Yat-Sen University. This work was supported in part by National Natural Science Foundation of China (NSFC) under Grant No. 62206060, in Part by Natural Science Foundation of Guangdong Province (2022A1515011555, 2023A1515012568, 2023A1515012561), Guangdong Provincial Key Laboratory of Human Digital Twin (2022B1212010004), and in part by Guangzhou Basic and Applied Basic Research Foundation under Grant No. SL2022A04J01626. Reference Source NED ASCCL Figure 1. Several examples are generated by the current advanced NED with and without the proposed ASCCL algorithm. Incorporating the ASCCL can better manipulate the expressions and meanwhile preserve mouth shapes. process. We conduct extensive experiments on variant datasets, and the results demonstrate the effectiveness of the proposed ASCCL algorithm. Code is publicly available at https://github.com/jianmanlincjx/ASCCL 1. Introduction Speech-preserving facial expression manipulation (SPFEM), which aims to manipulate facial emotions while preserving the mouth animations in static images or dynamic videos, can enhance human expressiveness and thus benefit variant applications including virtual avatars and film & television production. For example, it requires lots of effects and repeated remakes to capture an expected actor’s emotions in a movie & shooting. In contrast, a robust SPFEM system can easily modify the facial emotions to achieve comparable performance in the post-production stage and thus is urgently expected. Current SPFEM literature either predominantly previous face reenactment algorithms [10, 28] or harnesses decouThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 7267",
        "chosen": "This paper proposes a novel adaptive spatial coherent correlation learning (ASCCL) algorithm for speech-preserving facial expression manipulation (SPFEM). The algorithm models the correlation of local facial animations and integrates it as a metric to supervise the manipulation process, ensuring better preservation of mouth animations associated with spoken content.",
        "rejected": "Speech-preserving facial expression manipulation aims to modify facial emotions while preserving mouth animations. This paper proposes an adaptive spatial coherent correlation learning (ASCCL) algorithm that improves expression manipulation while better preserving speech-related facial animations. Extensive experiments validate the effectiveness of ASCCL."
    },
    {
        "prompt": "summary: Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability to perceive and understand multi-modal signals. However, most of the existing MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text pairs, leading to insufficient extraction and reasoning of visual knowledge. To address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model (LION), which empowers the MLLM by injecting visual knowledge in two levels. 1) Progressive incorporation of fine-grained spatialaware visual knowledge. We design a vision aggregator cooperated with region-level vision-language (VL) tasks to incorporate fine-grained spatial-aware visual knowledge into the MLLM. To alleviate the conflict between imagelevel and region-level VL tasks during incorporation, we devise a dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This progressive incorporation scheme contributes to the mutual promotion between these two kinds of VL tasks. 2) Soft prompting of high-level semantic visual evidence. We facilitate the MLLM with highlevel semantic visual evidence by leveraging diverse image tags. To mitigate the potential influence caused by imperfect predicted tags, we propose a soft prompting method by embedding a learnable token into the tailored text instruction. Comprehensive experiments on several multi-modal benchmarks demonstrate the superiority of our model (e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over InstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2). 1. Introduction Recently, Large Language Models (LLMs) have demonstrated remarkable zero-shot abilities on various linguistic tasks. Assisted by LLMs, several multimodal large language models (MLLMs), such as MiniGPT-4 [54], Otter †Corresponding author Instruction: What is in the photo? LLM Vision Encoder LION chicken, potatoes and cauliflower on a white plate with a knife and fork in the middle of the table next to a red wine bottle that sits on the table next to it. Instruction: What is in the photo? LLM Grilled chicken breast [                      ] Spatial-aware Visual Knowledge: Semantic Visual Evidence:  cauliflower, chicken, table, plate, food, meat,  potato, white Chicken, potatoes, and cauliflower on a white plate with a wooden table in the foreground. The plate is sitting on a wooden table. Response: Response: Exsiting MLLMs Bridge Module Bridge Module Vision Encoder A piece of steamed cauliflower [                      ]  Roasted baby yellow potato [                      ]   Mixture of Adapters Figure 1. Comparison between existing MLLMs and LION . The existing MLLM generates a vague and inaccurate response, while LION provides a more precise and contextually accurate description by progressively incorporating spatial-aware knowledge and softly prompting semantic visual evidence. [19], and InstructBLIP [7], achieve significant improvements in reasoning abilities to deal with various visionlanguage (VL) tasks. In most of the existing MLLMs, the visual information is mainly extracted from a vision encoder pretrained with image-level supervision (e.g., CLIP [32]), and then are adapted to a LLM by using a tiny bridge module. This makes these MLLMs inherently possess limited image understanding capabilities [21]. As shown in Fig. 1, the insufficient visual information misleads MLLMs to provide erroneous and hallucinated responses. An intuitive solution to this problem is to replace or tune the vision encoder [41]. However, it requires pretraining on massive data or suffers from the catastrophic forgetting issue [35, 36, 50], which diminishes the practical efficacy of this strategy. These predicaments highlight that the insufficient extraction of visual knowledge has become a central obstacle impeding the development of MLLMs. To overcome this dilemma, as depicted in Fig. 1, we devise a dual-Level vIsual knOwledge eNhanced Multimodal This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 26540",
        "chosen": "This paper introduces LION, a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model. LION incorporates fine-grained spatial-aware visual knowledge and soft prompts of high-level semantic visual evidence into the multimodal large language model (MLLM), improving performance on various multi-modal benchmarks.",
        "rejected": "Multimodal Large Language Models (MLLMs) face challenges in extracting and reasoning visual knowledge. This work presents LION, enhancing MLLMs by injecting visual knowledge at two levels: progressive incorporation of fine-grained spatial-aware knowledge and soft prompting of high-level semantic visual evidence. LION shows superior performance on several multi-modal benchmarks."
    },
    {
        "prompt": "summary: Recent progress in Large Multimodal Models (LMM) has opened up great possibilities for various applications in the ﬁeld of human-machine interactions. However, developing LMMs that can comprehend, reason, and plan in complex and diverse 3D environments remains a challenging topic, especially considering the demand for understanding permutation-invariant point cloud representations of the 3D scene. Existing works seek help from multi-view images by projecting 2D features to 3D space, which inevitably leads to huge computational overhead and performance degradation. In this paper, we present LL3DA, a Large Language 3D Assistant that takes point cloud as the direct input and responds to both text instructions and visual interactions. The additional visual interaction enables LMMs to better comprehend human interactions with the 3D environment and further remove the ambiguities within plain texts. Experiments show that LL3DA achieves remarkable results and surpasses various 3D vision-language models on both 3D Dense Captioning and 3D Question Answering. 1. Introduction The recent surge in Large Language Model (LLM) families [14, 31, 46, 52, 63] opens up great opportunities for addressing various machine learning tasks in a generalized way [30, 32, 40, 60]. During this LLM carnival, researchers are also seeking generalized LLM solutions for various vision language tasks [37, 50, 62]. Among these, LLM-based 3D scene understanding is a valuable topic that would beneﬁt the development of autonomous driving [9, 23] and emThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 26428",
        "chosen": "This paper presents LL3DA, a Large Language 3D Assistant that takes point cloud as direct input and responds to both text instructions and visual interactions. The model enables better comprehension of human interactions with 3D environments and removes ambiguities in plain texts, achieving remarkable results on 3D Dense Captioning and 3D Question Answering.",
        "rejected": "Large Multimodal Models (LMM) struggle with understanding permutation-invariant point cloud representations of 3D scenes. LL3DA is introduced as a solution, taking point clouds directly as input and responding to text instructions and visual interactions. Experiments demonstrate LL3DA's superior performance in 3D Dense Captioning and 3D Question Answering tasks."
    },
    {
        "prompt": "summary: For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge. This work introduces a novel ”Low-Res Leads the Way” (LWay) training framework, merging Supervised Pre-training with Selfsupervised Learning to enhance the adaptability of SR models to real-world images. Our approach utilizes a lowresolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with superresolved outputs for LR reconstruction. Leveraging unseen LR images for self-supervised learning guides the model to adapt its modeling space to the target domain, facilitating fine-tuning of SR models without requiring paired high-resolution (HR) images. The integration of Discrete Wavelet Transform (DWT) further refines the focus on highfrequency details. Extensive evaluations show that our method significantly improves the generalization and detail restoration capabilities of SR models on unseen realworld datasets, outperforming existing methods. Our training regime is universally compatible, requiring no network architecture modifications, making it a practical solution for real-world SR applications. 1. Introduction Image super-resolution (SR) aims to restore high-resolution (HR) images from their low-resolution (LR) or degraded counterparts. The inception of the deep-learning-based SR model can be traced back to SRCNN [14]. Recently, advancements in deep learning models have substantially enhanced SR performance [1, 6, 8–10, 12, 25–27, 39, 51, 52, 54, 56], particularly in addressing specific degradation types like bicubic downsampling. Nevertheless, the efficacy of SR models is generally restricted by the degradation strategies employed during the training phase, posing great challenges in complex real-world applications. *Lei Zhu (leizhu@ust.hk) is the corresponding author. SL space on  synthetic data SSL space on  real test data Unseen  Real-world Image High quality Low fidelity Low quality High fidelity Ground Truth High quality High fidelity Ours pull SL space on  synthetic data SSL space on  real test data Zoom In Zoom In Zoom In Zoom In Zoom In Figure 1. Our proposed training method combine the benefits of supervised learning (SL) on synthetic data and self-supervised learning (SSL) on the unseen test images, achieve high quality and high fidelity SR results. In the realm of real-world SR, as shown in Figure 2, training approaches can primarily be categorized into three main paradigms. (a) Unsupervised Learning with Unpaired Data: Methods within this paradigm [2, 3, 15, 38, 40, 45, 46, 55] commonly utilize Generative Adversarial Networks (GAN) architecture to learn target distributions without paired data. Using one or multiple discriminators, they distinguish between generated images and actual samples, guiding the generator to model accurately. However, as this approach heavily relies on external data, it encounters significant challenges when facing scarce target domain data, particularly in real-world scenarios. The GAN framework for unsupervised learning also has some drawbacks. Firstly, it inherently struggles with stability during training, leading to noticeable artifacts in SR outputs. Secondly, it is difficult for a single 0/1 plane modelled by a discriminator to accurately separate the target domain [31]. This can result in imprecise distribution learning. (b) Supervised Learning with Paired Synthetic Data: BSRGAN [47] and Real-ESRGAN [42] have largely enhanced the SR model’s This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25857",
        "chosen": "This paper introduces a novel \"Low-Res Leads the Way\" (LWay) training framework for image super-resolution (SR). The framework combines Supervised Pre-training with Self-supervised Learning, utilizing a low-resolution reconstruction network to extract degradation embeddings and guide model adaptation to real-world images without requiring paired high-resolution images.",
        "rejected": "Image super-resolution (SR) faces challenges bridging synthetic datasets' performance and real-world degradation scenarios. A novel \"Low-Res Leads the Way\" (LWay) training framework merges supervised pre-training with self-supervised learning, significantly improving SR models' generalization and detail restoration capabilities on unseen real-world datasets."
    },
    {
        "prompt": "summary: Pose regression networks predict the camera pose of a query image relative to a known environment. Within this family of methods, absolute pose regression (APR) has recently shown promising accuracy in the range of a few centimeters in position error. APR networks encode the scene geometry implicitly in their weights. To achieve high accuracy, they require vast amounts of training data that, realistically, can only be created using novel view synthesis in a days-long process. This process has to be repeated for each new scene again and again. We present a new approach to pose regression, map-relative pose regression (marepo), that satisfies the data hunger of the pose regression network in a scene-agnostic fashion. We condition the pose regressor on a scene-specific map representation such that its pose predictions are relative to the scene map. This allows us to train the pose regressor across hundreds of scenes to learn the generic relation between a scene-specific map representation and the camera pose. Our map-relative pose regressor can be applied to new map representations immediately or after mere minutes of fine-tuning for the highest accuracy. Our approach outperforms previous pose regression methods by far on two public datasets, indoor and outdoor. Code is available: https://nianticlabs.github.io/marepo. 1. Introduction Today, neural networks have conquered virtually all sectors of computer vision, but there is still at least one task that they struggle with: visual relocalization. What is visual relocalization? Given a set of mapping images and their poses, expressed in a common coordinate system, build a scene representation. Later, given a query image, estimate its pose, i.e. position and orientation, relative to the scene. Successful approaches to visual relocalization rely on predicting image-to-scene correspondences, either via matching [8, 21, 38–40, 42, 57] or direct regression [4– 6, 14, 56], then solving for the pose using traditional and robust algorithms like PnP [18] and RANSAC [17]. Adopting a different perspective, approaches based on 20x Figure 1. Camera pose estimation performance vs. mapping time. The figure shows the median translation error of several pose regression relocalization methods on the 7-Scenes dataset and the time required (proportional to the bubble size) to train each relocalizer on the target scenes. Our proposed approach, marepo, achieves superior performance – by far – on both metrics, thanks to its integration of scene-specific geometric map priors within an accurate, map-relative, pose regression framework. pose regression [12, 25, 32, 46] attempt to perform visual relocalization without resorting to traditional pose solving, by using a single feed-forward neural network to infer poses from single images. The mapping data is treated as a training set where the camera extrinsics serve as supervision. Generally, pose regression approaches come in two flavors, but they both struggle with accuracy compared to correspondence-based methods. Absolute pose regression (APR) methods [7, 24, 25] involve training a dedicated pose regressor for each individual scene, enabling the prediction of camera poses to that particular scene. Though the scene coordinate space can be implicitly encoded in the weights of the neural networks, absolute pose regressors exhibit low pose estimation accuracy, primarily due to the often limited training data available for each scene, and struggle to generalize to unseen views [43]. Relative pose regression is a second flavor of pose regression methods [10, 16, 26, 51, 54]. The regressor is trained to predict the relative pose between two images. In a typical inference scenario, the regressor is applied to a pair formed by an unseen query and an image from the mapping set (typically selected via a nearest neighbor-type matchThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20665",
        "chosen": "This paper presents map-relative pose regression (marepo), a new approach to pose regression for visual re-localization. The method conditions the pose regressor on a scene-specific map representation, allowing training across hundreds of scenes and immediate application to new map representations. The approach outperforms previous methods on public datasets.",
        "rejected": "Pose regression networks predict camera poses from query images but require vast amounts of training data. This paper introduces map-relative pose regression (marepo), conditioning pose regressors on scene-specific maps, enabling training across multiple scenes. Marepo outperforms previous methods on public indoor and outdoor datasets."
    },
    {
        "prompt": "summary: We introduce Mind Artist (MindArt), a novel and efficient neural decoding architecture to snap artistic photographs from our mind in a controllable manner. Recently, progress has been made in image reconstruction with non-invasive brain recordings, but it’s still difficult to generate realistic images with high semantic fidelity due to the scarcity of data annotations. Unlike previous methods, this work casts the neural decoding into optimal transport (OT) and representation decoupling problems. Specifically, under discrete OT theory, we design a graph matching-guided neural representation learning framework to seek the underlying correspondences between conceptual semantics and neural signals, which yields a natural and meaningful selfsupervisory task. Moreover, the proposed MindArt, structured with multiple stand-alone modal branches, enables the seamless incorporation of semantic representation into any visual style information, thus leaving it to have multimodal reconstruction and training-free semantic editing ca†Corresponding author: Yu Qi. pabilities. By doing so, the reconstructed images of MindArt have phenomenal realism both in terms of semantics and appearance. We compare our MindArt with leading alternatives, and achieve SOTA performance in different decoding tasks. Importantly, our approach can directly generate a series of stylized “mind snapshots” w/o extra optimizations, which may open up more potential applications. Code is available at https://github.com/JxuanC/ MindArt. 1. Introduction As we venture into the frontiers of human creativity, one captivating question has surfaced: Can our brains serve as hidden “artistic cameras”? Let’s envision a world where painting tools, and photographic skills are going out of fashion. All you need is to think or gaze upon a visual object or immerse yourself in the wonders of nature. Then, your brain will be turned into an artistic lens transforming your perceptions into astonishing works of art, perhaps resembling a Van Gogh-style snapshot with vibrant yellow and orange sunflowers. To actualize aforesaid goals, we present This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27207",
        "chosen": "This paper introduces Mind Artist (MindArt), a neural decoding architecture for generating artistic photographs from human thoughts. The method casts neural decoding into optimal transport and representation decoupling problems, enabling multimodal reconstruction and training-free semantic editing capabilities. MindArt achieves state-of-the-art performance in different decoding tasks.",
        "rejected": "Mind Artist (MindArt) proposes a novel neural decoding architecture for generating artistic photographs from brain recordings. Utilizing discrete optimal transport theory, MindArt designs a graph matching-guided framework, achieving phenomenal realism in reconstructed images. It supports multimodal reconstruction and training-free semantic editing capabilities."
    },
    {
        "prompt": "summary: Learning-based stereo matching techniques have made significant progress. However, existing methods inevitably lose geometrical structure information during the feature channel generation process, resulting in edge detail mismatches. In this paper, the Motif Channel Attention Stereo Matching Network (MoCha-Stereo) is designed to address this problem. We provide the Motif Channel Correlation Volume (MCCV) to determine more accurate edge matching costs. MCCV is achieved by projecting motif channels, which capture common geometric structures in feature channels, onto feature maps and cost volumes. In addition, edge variations in the reconstruction error map also affect details matching, we propose the Reconstruction Error Motif Penalty (REMP) module to further refine the fullresolution disparity estimation. REMP integrates the frequency information of typical channel features from the reconstruction error. MoCha-Stereo ranks 1st on the KITTI2015 and KITTI-2012 Reflective leaderboards. Our structure also shows excellent performance in Multi-View Stereo. *Co-first author. †Corresponding author. Code is avaliable at MoCha-Stereo. 1. Introduction Stereo matching remains a foundational challenge in computer vision, bearing significant relevance to autonomous driving, virtualization, rendering, and related sectors [40]. The primary goal of the assignment is to establish a pixel-wise displacement map, or disparity, which can be used to identify the depth of the pixels in the scene. Edge performance of disparity maps is particularly vital in techniques requiring pixel-level rendering, such as virtual reality and augmented reality, where precise fitting between the scene model and image mapping is essential [23]. This underscores the need for a close alignment between the edges of the disparity map and the original RGB image. Traditional stereo matching relies on global [8], semiglobal [13], or local [2] grayscale relationships between left and right view pixels. These methods struggle to fully leverage scene-specific prior knowledge. Achieving optimal results often involves human observation, this tuning process can be resource-intensive in scenes with complex images This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27768",
        "chosen": "This paper proposes MoCha-Stereo, a motif channel attention stereo matching network. The approach introduces the Motif Channel Correlation Volume (MCCV) and Reconstruction Error Motif Penalty (REMP) module to improve edge matching and full-resolution disparity estimation. MoCha-Stereo ranks 1st on KITTI-2015 and KITTI-2012 Reflective leaderboards and shows excellent performance in Multi-View Stereo.",
        "rejected": "For image super-resolution, this study develops a new training framework combining supervised learning on synthetic data with self-supervised learning on real test data, termed \"Low-Res Leads the Way\". This approach enhances adaptability and fine-tuning of SR models without paired HR images, showing improved generalization on real-world datasets."
    },
    {
        "prompt": "summary: Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multi-viewconsistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; extensive quantitative and qualitative evaluations demonstrate the advantages of our approach over existing state-of-the-art avatar creation models on both novel view and novel expression synthesis tasks. The code for our project is publicly available. 1. Introduction The field of photorealistic controllable human avatar generation has been subject to several technological leaps in the recent decade. The introduction of large 3D scan collections has facilitated the construction of expressive, articulated models of 3D human bodies [47, 55], faces [5, 39, 56], and hands [65]. From the outset, one of the primary applications of these models was to reconstruct a 3D avatar from highly under-constrained setups, such as monocular video or a single image [5, 18, 37, 55]. While allowing for rich semantic information to be inferred, these 3D morphable models were limited in the level of photorealism due to their focus on minimally clothed bodies and face regions, as well as their reliance on the standard mesh-based computer graphics pipelines for rendering. Recently, the task of generating photorealistic avatars [57] gained significant attention from the research community due to its potential to revolutionize our ways of digital communication. Here, combining novel neural rendering techniques [50, 86] with articulated human models allowed for a new level of generated image quality. However, the best-performing models here still require a significant visual input, such as calibrated multi-view images [21, 78] or monocular video sequences of the subject [61, 82, 95, 99]. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 10359",
        "chosen": "This paper enhances the quality and functionality of generative diffusion models for creating controllable, photorealistic human avatars from a single image. By integrating a 3D morphable model into a multi-view-consistent diffusion approach, the framework improves novel view synthesis and enables accurate facial expression and body pose control. Extensive evaluations show advantages over existing avatar creation models.",
        "rejected": "Recent advances have enabled the generation of 3D human avatars from single images using generative diffusion models. This work integrates a 3D morphable model into a multi-view-consistent diffusion approach, enhancing photorealism and controllability of facial expressions and body poses. The proposed method creates fully 3D-consistent, animatable avatars with superior performance compared to existing avatar creation models."
    },
    {
        "prompt": "summary: Neural Radiance Field (NeRF) has been widely recognized for its excellence in novel view synthesis and 3D scene reconstruction. However, their effectiveness is inherently tied to the assumption of static scenes, rendering them susceptible to undesirable artifacts when confronted with transient distractors such as moving objects or shadows. In this work, we propose a novel paradigm, namely “Heuristics-Guided Segmentation” (HuGS), which significantly enhances the separation of static scenes from transient distractors by harmoniously combining the strengths of hand-crafted heuristics and state-of-the-art segmentation models, thus significantly transcending the limitations of previous solutions. Furthermore, we delve into the meticulous design of heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based heuristics and color residual heuristics, catering to a diverse range of texture profiles. Extensive experiments demonstrate the superiority and robustness of our method in mitigating transient distractors for NeRFs trained in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/ 1. Introduction Neural Radiance Fields (NeRF) [29] have garnered significant attention for their remarkable achievements in novel view synthesis. Utilizing multiple-view images, NeRF conceptualizes the 3D scene as a neural field [54] and produces highly realistic renderings through advanced volume rendering techniques. This capability has opened the door to a wide array of downstream applications including 3D reconstruction [22, 43, 48], content generation [23, 33, 36], *Corresponding author is Guanbin Li. This work was supported in part by the National Natural Science Foundation of China (NO. 62322608), in part by the CAAI-MindSpore Open Fund, developed on OpenI Community, in part by the Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University (No.VRLAB2023A01), in part by Shenzhen Science and Technology Program KQTD20210811090149095 and also the Pearl River Talent Recruitment Program 2019QN01X226. Seg. w/ Prior (“carton, bottle, car…”) Heuristics (Color Residual) HuGS (SfM + Color Residual) Static Map (a) Segmentation-based Method (b) Heuristics-based Method (c) NeRF-HuGS (Ours) NeRF Training and Rendering Static Map NeRF Training and Rendering Static Map NeRF Training and Rendering Train Images ··· Figure 1. Comparison between previous methods and the proposed Heuristics-Guided Segmentation (HuGS) paradigm. When training NeRF with static scenes disturbed by transient distractors, (a) segmentation-based methods rely on prior knowledge and cannot identify unexpected transient objects (e.g., pizza); (b) heuristics-based methods are more generalizable but inaccurate (e.g., tablecloth textures); (c) our method combines their strengths and produces highly accurate transient vs. static separations, thereby significantly improving NeRF results. semantic understanding [14, 42, 58], etc. However, the images used as NeRF training data must meet several strict conditions, one of which is the requirement for content consistency and stability. In other words, the native NeRF model operates under the assumption of a static scene. Any elements that exhibit motion or inconsistency throughout the entire data capture session, which we refer to as “transient distractors”, can introduce undesirable artifacts into the reconstructed 3D geometry. However, the presence of transient distractors is nearly inevitable in realworld scenarios. For instance, in outdoor settings, random appearances of pedestrians and vehicles may occur during This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 19436",
        "chosen": "This paper proposes NeRF-HuGS, a novel paradigm combining heuristics and segmentation models to improve NeRF's performance in non-static scenes. The method fuses Structure-from-Motion (SfM)-based heuristics and color residual heuristics, effectively separating static scenes from transient distractors. Experiments demonstrate the method's superiority in mitigating artifacts caused by moving objects or shadows.",
        "rejected": "NeRF has shown excellence in novel view synthesis but struggles with transient distractors. A new paradigm named Heuristics-Guided Segmentation (HuGS) combines hand-crafted heuristics and state-of-the-art segmentation models to improve separation between static scenes and transient distractors, significantly mitigating artifacts in NeRFs trained in non-static scenes."
    },
    {
        "prompt": "summary: Absolute Pose Regression (APR) methods use deep neural networks to directly regress camera poses from RGB images. However, the predominant APR architectures only rely on 2D operations during inference, resulting in limited accuracy of pose estimation due to the lack of 3D geometry constraints or priors. In this work, we propose a test-time refinement pipeline that leverages implicit geometric constraints using a robust feature field to enhance the ability of APR methods to use 3D information during inference. We also introduce a novel Neural Feature Synthesizer (NeFeS) model, which encodes 3D geometric features during training and directly renders dense novel view features at test time to refine APR methods. To enhance the robustness of our model, we introduce a feature fusion module and a progressive training strategy. Our proposed method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets. Code will be released at https:// github.com/ActiveVisionLab/NeFeS. 1. Introduction Camera relocalization is a crucial task that allows machines to understand their position and orientation in 3D space. It is an essential prerequisite for applications such as augmented reality, robotics, and autonomous driving, where the accuracy and efficiency of pose estimation are important. Recently, Absolute Pose Regression (APR) methods [21– 23] have been shown to be effective in directly estimating camera pose from RGB images using convolutional neural networks. The simplicity of APR’s architecture offers several potential advantages over classical geometry-based methods [5, 43, 45], involving end-to-end training, cheap computation cost, and low memory demand. Latest advances in APR, particularly the use of novel view synthesis (NVS) [10, 11, 29, 32, 33, 49] to generate new images from random viewpoints as data augmentation during training, have significantly improved the pose regression performance. Despite this, state-of-the-art (SOTA) (a) Before Pose Refinement (b) After Pose Refinement Figure 1. Our pose refinement (R) improves (coarse) pose predictions from other methods using novel feature synthesis to achieve pixel-wise alignment. Top left / right: 3D plots of predicted (green) and ground-truth (red) camera positions. Bottom left / right: alignment between rendered features and query image. APRs still have the following limitations: (i) They predict the pose of a query image by passing it through a CNN, which typically disregards geometry at inference time. This causes APR networks to struggle to generalize to viewpoints that the training data fails to cover [46]; (ii) The unlabeled data, often sampled from the validation/testing set, used for finetuning the APR network [8, 10, 11] may not be universally available in real-life circumstances, and this semi-supervised finetuning is also time-consuming. To address these limitations, we propose a novel testtime refinement pipeline for APR methods. Unlike prior works that explore extended Kalman filters [34], pose graph optimization [8], or pose auto-encoders [49], our method integrates an implicit representation based geometric refinement into an end-to-end learning framework, where gradients can be backpropagated to the APR network. We test our proposed method across different APR architectures to This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20987",
        "chosen": "This paper presents a test-time refinement pipeline for Absolute Pose Regression (APR) methods, enhancing their ability to use 3D information during inference. The proposed Neural Feature Synthesizer (NeFeS) model encodes 3D geometric features during training and renders dense novel view features at test time. The method achieves state-of-the-art single-image APR accuracy on indoor and outdoor datasets.",
        "rejected": "APR methods for camera pose estimation face limitations due to lack of 3D geometry constraints during inference. This work proposes a test-time refinement pipeline that leverages implicit geometric constraints through a robust feature field, introducing a Neural Feature Synthesizer (NeFeS) to encode 3D features and refine APR accuracy."
    },
    {
        "prompt": "summary: We explore the boundaries of scaling up a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. Our model advances the state-of-the-art on most vision-and-language benchmarks considered (20+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix. 1. Introduction The success of scaling language models [1–4] makes it appealing to similarly scale Vision-Language (V&L) models, and investigate the improvements, capabilities, and emergent properties of such models. Inspired by the work in [5], we present PaLI-X, a multilingual vision and language model with reusable scaled-up components, consisting of a pretrained large-capacity visual encoder (using [6] as the starting point) and a pretrained language-only encoder-decoder (using [7] as the starting point), further trained at-scale on a vision-and-language data mixture using a combination of self-supervision and full-supervision signals. One clear pattern that emerges from the combination of results from PaLI [5] and the work we present in this paper is that scaling both V&L components together brings increases in performance across a wide range of tasks. We show this by comparing against the same benchmarks used for PaLI (Fig. 1, Left), and also against new benchmarks for which the new capabilities of PaLI-X are evaluated (e.g., ChartQA, AI2D, DocVQA, InfographicVQA, as well as video understanding tasks). We observe that scaling leads to large improvements over the results of the PaLI model, and also over specialized large-scale models that are trained speciﬁcally to solve certain tasks, often with the help of (often much larger) text-only LLMs [8]. In particular, we ﬁnd that increasing both the effective capacity of the vision component (which [9] does more unilaterally) and of the language component (which [10] also does unilaterally) is beneﬁcial; the new PaLI-X model provides more balanced parameter allocation than any other prior work (roughly 40%-60% split of the total capacity). Aside from demonstrating the consistent impact of scale, the original contribution of PaLI-X consists in leveraging the mixture-of-objectives proposed in [7] for vision-andlanguage modeling, and showing that it results in a model that improves both state-of-the-art results and the Pareto frontier for ﬁne-tuning and few-shot (Fig. 1, Right). We also observe emergent properties based on PaLI-X’s results compared to previous models with similar architecture but smaller sizes. For instance, we report drastically improved performance on the counting ability (See Table 1 and Appendix B), both for the plain variety (count all instances of a class) and the complex variety (count instances based on a natural language description), that are not attributable to training design1. Additionally, we present qualitative insights into the model’s performance (Appendix A), with an 1Plain counting is usually achievable via good object detection, while complex counting requires a ﬁne-grained understanding of the alignment between language-based speciﬁcations and visually-based occurrences. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 14432",
        "chosen": "This paper explores scaling up a multilingual vision and language model, achieving new performance levels on various complex tasks. The model advances the state-of-the-art on most vision-and-language benchmarks and shows emerging capabilities like complex counting and multilingual object detection.",
        "rejected": "Scaling up multilingual vision and language models achieves improved performance across various tasks, including image captioning, question answering, and object detection. PaLI-X demonstrates enhanced capabilities like complex counting and multilingual object detection by scaling both visual and language components, resulting in balanced parameter allocation and state-of-the-art results on multiple benchmarks."
    },
    {
        "prompt": "summary: Due to the resource-intensive nature of training visionlanguage models on expansive video data, a majority of studies have centered on adapting pre-trained imagelanguage models to the video domain. Dominant pipelines propose to tackle the visual discrepancies with additional temporal learners while overlooking the substantial discrepancy for web-scaled descriptive narratives and concise action category names, leading to less distinct semantic space and potential performance limitations. In this work, we prioritize the refinement of text knowledge to facilitate generalizable video recognition. To address the limitations of the less distinct semantic space of category names, we prompt a large language model (LLM) to augment action class names into Spatio-Temporal Descriptors thus bridging the textual discrepancy and serving as a knowledge base for general recognition. Moreover, to assign the best descriptors with different video instances, we propose Optimal Descriptor Solver, forming the video recognition problem as solving the optimal matching flow across frame-level representations and descriptors. Comprehensive evaluations in zero-shot, few-shot, and fully supervised video recognition highlight the effectiveness of our approach. Our best model achieves a state-of-the-art zero-shot accuracy of 75.1% on Kinetics-600. 1. Introduction Large-scale contrastive language-image pre-training [25, 46, 65] have shown remarkable performance in various computer vision tasks. The visual-semantic joint space not only serves powerful visual representation but also enables few/zero-shot transferring to downstream tasks with the reference of natural language. However, training a similar model for video recognition can be costly since largescale video-language datasets are exponentially more massive [57] due to the extra temporal dimension. Hence, a feasible solution is to adapt the pre-trained image-text models for the task of video recognition. As depicted in Ski jumping competitions. Jumper in  mid air with extreme speed. The  background of the forest and slope. CLIP Pre-training Data Visual  Discrepancy Textual Discrepancy A video of a person  Ski jumping. Video Recognition Data Web-scaled Descriptive Narratives Concise Category Names Static Images Video Sequences Low semantic distinction High semantic distinction Spatio-Temporal Spatial Inductive Text Encoder Visual Encoder Temporal Learner Matching Dominant Pipeline : The textual discrepancy is overlooked, which may introduce ambiguity in matching : Dominant pipelines tackle the visual discrepancy via additional temporal learners  (Settings, common objects, certain steps, etc.) (Short phrases with constant hard-prompt) Figure 1. Motivation of our method. Dominant pipelines propose to tackle the visual discrepancies with additional temporal learners while overlooking the textual discrepancy between descriptive narratives and concise category names. This oversight results in a less separable latent space, which may hinder video recognition. Fig. 1, current methods devise a range of temporal learners to address the visual discrepancy while preserving textdomain knowledge in the semantic space of action category names, often by merging the category name with CLIP-style hard-prompts (e.g., “a video of a person {ski jumping}”) [41, 45, 53, 56, 60]. Despite providing essential inter-class correlations that can benefit general recognition, we speculate this paradigm overlooks the textual discrepancy between web-scaled descriptive narratives in CLIP pre-training and concise category names in downstream video recognition. Given that category names of video datasets generally consist of verbs and nouns, the nouns exhibit variability while the verbs tend to remain consistent. For instance, playing cello, playing organ & playing violin are distinct actions related to playing instruments. The sole differentiation between these category names lies in the noun itself, resulting in low discriminative text embeddings. This may lead to a less separable semantic space, potentially introducing ambiguity in recognition tasks [5]. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18888",
        "chosen": "This paper proposes refining text knowledge with optimal spatio-temporal descriptors for generalizable video recognition. The method prompts a large language model to augment action class names and proposes an Optimal Descriptor Solver. Evaluations show effectiveness in zero-shot, few-shot, and fully supervised video recognition.",
        "rejected": "To address discrepancies in adapting pre-trained image-language models to video recognition, this paper proposes augmenting action class names into Spatio-Temporal Descriptors using a large language model (LLM). An Optimal Descriptor Solver is introduced to match frame-level representations with descriptors, leading to significant improvements in zero-shot, few-shot, and fully supervised video recognition."
    },
    {
        "prompt": "summary: Recently, some large kernel convnets strike back with appealing performance and efﬁciency. However, given the square complexity of convolution, scaling up kernels can bring about an enormous amount of parameters and the proliferated parameters can induce severe optimization problem. Due to these issues, current CNNs compromise to scale up to 51 × 51 in the form of stripe convolution (i.e., 51 × 5 + 5 × 51) and start to saturate as the kernel size continues growing. In this paper, we delve into addressing these vital issues and explore whether we can continue scaling up kernels for more performance gains. Inspired by human vision, we propose a human-like peripheral convolution that efﬁciently reduces over 90% parameter count of dense grid convolution through parameter sharing, and manage to scale up kernel size to extremely large. Our peripheral convolution behaves highly similar to human, reducing the complexity of convolution from O(K2) to O(log K) without backﬁring performance. Built on this, we propose Parameter-efﬁcient Large Kernel Network (PeLK). Our PeLK outperforms modern vision Transformers and ConvNet architectures like Swin, ConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet classiﬁcation, semantic segmentation on ADE20K and object detection on MS COCO. For the ﬁrst time, we successfully scale up the kernel size of CNNs to an unprecedented 101 × 101 and demonstrate consistent improvements. 1. Introduction Convolutional Neural Networks (CNNs) have played a pivotal role in machine learning for decades [16, 19, 20, 35]. However, their dominance has been greatly challenged by Vision Transformers (ViTs) [6, 12, 24, 42, 47] over recent years. Some works [32, 44] attribute the powerful performance of ViTs to their large receptive ﬁelds: Facil*Work done during internship at Meituan Inc. †Corresponding author. itated by self-attention mechanism, ViTs can capture context information from a large spatial scope and model longrange dependencies. Inspired by this, recent advances in CNNs [11, 23, 25] have revealed that when equipped with large kernel size (e.g., 31 × 31), pure CNN architecture can perform on par with or even better than state-of-the-art ViTs on various vision tasks. Although large kernel convnets exhibit strong performance and appealing efﬁciency, a fatal problem exists: the square complexity O(K2) with respect to kernel size K. Due to this problem, directly scaling up kernels will bring about a huge amount of parameters. For instance, the parameter of a 31 × 31 kernel is more than 100× larger than that of a typical 3 × 3 counterpart in ResNet [16] and about 20× as many as that of the 7 × 7 kernel used in ConvNeXt [25]. The proliferated parameters subsequently induce severe optimization problem, making it useless or even harmful to directly scale up kernel size [11, 23, 25]. To solve, RepLKNet [11] re-parameterize a 5×5 kernel parallel to the large one to make up the optimization issue, SLaK [23] compromise to use stripe convolution to reduce the complexity to linear and scales up to 51 × 51 (i.e., 51 × 5 + 5 × 51). However, this is still a limited interaction range for the resolution of downstream tasks (e.g., 2048 × 512 on ADE20K) and more importantly, stripe convolution lacks the range perception of dense convolution, thus we conjecture it may undermine the model’s spatial perception capacity. In this paper, we ﬁrst conduct a comprehensive dissection of convolution forms under a uniﬁed modern framework (i.e., SLaK [23]). We empirically verify our conjecture that dense grid convolution outperforms stripe convolution with consistent improvements across multiple kernel sizes. This phenomenon holds not only for classiﬁcation task, but even more pronounced for downstream tasks, indicating the essential advantage of dense convolution over stripe form. Nevertheless, as mentioned above, the square complexity of large dense convolution leads to the proliferated parameters, causing rapidly increasing model size, greater optimization difﬁculty and thus preventing it from This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 5557",
        "chosen": "This paper introduces Parameter-efficient Large Kernel Networks (PeLK) with peripheral convolution, inspired by human vision. The network efficiently reduces parameters, scales up kernel size, and outperforms modern vision Transformers and ConvNet architectures on various vision tasks.",
        "rejected": "Large kernel convnets can achieve high performance but suffer from optimization issues as kernel sizes increase. Inspired by human vision, a peripheral convolution method reduces parameter counts by over 90% while scaling up kernel size to extremely large dimensions. The Parameter-efficient Large Kernel Network (PeLK) outperforms modern vision transformers and ConvNet architectures on various vision tasks, successfully scaling kernels to an unprecedented 101x101 size."
    },
    {
        "prompt": "summary: Domain Generalization (DG) aims to resolve distribution shifts between source and target domains, and current DG methods are default to the setting that data from source and target domains share identical categories. Nevertheless, there exists unseen classes from target domains in practical scenarios. To address this issue, Open Set Domain Generalization (OSDG) has emerged and several methods have been exclusively proposed. However, most existing methods adopt complex architectures with slight improvement compared with DG methods. Recently, visionlanguage models (VLMs) have been introduced in DG following the fine-tuning paradigm, but consume huge training overhead with large vision models. Therefore, in this paper, we innovate to transfer knowledge from VLMs to lightweight vision models and improve the robustness by introducing Perturbation Distillation (PD) from three perspectives, including Score, Class and Instance (SCI), named SCI-PD. Moreover, previous methods are oriented by the benchmarks with identical and fixed splits, ignoring the divergence between source domains. These methods are revealed to suffer from sharp performance decay with our proposed new benchmark Hybrid Domain Generalization (HDG) and a novel metric H2-CV, which construct various splits to comprehensively assess the robustness of algorithms. Extensive experiments demonstrate that our method outperforms state-of-the-art algorithms on multiple datasets, especially improving the robustness when confronting data scarcity. 1. Introduction Deep learning has attained remarkable success on various downstream tasks in computer vision, typically un*Corresponding author †Source code is available at https://github.com/znchen666/HDG. Figure 1. The balance between model performance and training time consumption. Model performance is evaluated on the average H-score of different splits based on the proposed HDG benchmark. Our method achieves superior performance with less training time compared with state-of-the-art (SOTA) methods in OSDG. der the presumption that both training and test samples are Independent and Identically Distributed (IID) with the same label space. However, real-world data often exhibits unpredictable distributions, leading to the failure of deep neural networks. To address such distribution shifts, Domain Generalization (DG) is first introduced to leverage data from multiple source domains to achieve generalization on unseen target domains, from the perspective of domain-invariant learning [16, 27, 32, 37, 39, 43], data augmentation [8, 23, 52, 58, 59], and learning strategies [2, 6, 20, 30, 48, 56]. However, it has been observed that most existing domain generalization methods assume a closed-set distribution, where the label space remains identical across the source and target domain. To address this limitation, Open Set Domain Generalization (OSDG) This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 23501",
        "chosen": "This paper addresses Open Set Domain Generalization (OSDG) by transferring knowledge from vision-language models to lightweight vision models and introducing Perturbation Distillation (PD) from three perspectives. The method improves robustness and outperforms state-of-the-art algorithms on multiple datasets.",
        "rejected": "Domain Generalization (DG) faces challenges when dealing with unseen classes in target domains. Open Set Domain Generalization (OSDG) addresses this issue, but existing methods often require complex architectures. This paper introduces Perturbation Distillation (PD) from three perspectives (Score, Class, Instance), named SCI-PD, which transfers knowledge from vision-language models to lightweight vision models, improving robustness and performance on multiple datasets, especially under data scarcity."
    },
    {
        "prompt": "summary: Weakly-supervised Video Anomaly Detection (wVAD) aims to detect frame-level anomalies using only videolevel labels in training. Due to the limitation of coarsegrained labels, Multi-Instance Learning (MIL) is prevailing in wVAD. However, MIL suffers from insufficiency of binary supervision to model diverse abnormal patterns. Besides, the coupling between abnormality and its context hinders the learning of clear abnormal event boundary. In this paper, we propose prompt-enhanced MIL to detect various abnormal events while ensuring clear event boundaries. Concretely, we design the abnormal-aware prompts by using abnormal class annotations together with learnable prompt, which can incorporate semantic priors into video features dynamically. The detector can utilize the semantic-rich features to capture diverse abnormal patterns. In addition, normal context prompt is introduced to amplify the distinction between abnormality and its context, facilitating the generation of clear boundary. With the mutual enhancement of abnormal-aware and normal context prompt, the model can construct discriminative representations to detect divergent anomalies without ambiguous event boundaries. Extensive experiments demonstrate our method achieves SOTA performance on three public benchmarks. The code is available at https://github. com/Junxi-Chen/PE-MIL. 1. Introduction To identify anomaly at frame level in video, Video Anomaly Detection (VAD) has become vital in critical areas, e.g., surveillance systems [19], medical imaging [33] and autonomous driving [1]. For great generalization ability across diverse scenes, researchers [6, 8, 29, 31, 32, 43, 45] *Corresponding authors Figure 1. (a) Illustration of prompt-enhanced MIL. In multi-modal feature space, text prompts integrate abnormal-aware semantic priors into visual features. NCP incorporates normal semantic into ambiguous context feature. In such manner, our method learns a more discriminative representation to deliver a precise anomaly detection. (b) Anomaly detection examples of our method. turn to weakly-supervised VAD (wVAD) which only leverages video-level labels. Primarily, wVAD faces two key challenges: 1) detecting complex anomalous patterns in varied scenarios where temporal relationship and visual appearance of anomalies exhibit substantial discrepancies; 2) generating clear abnormal event boundaries in the absence of fine-grained boundary annotations. To tackle wVAD, prior works [52, 55] generate noisy frame-level abnormal labels and reduce noise subsequently, but such manner limits the generalization ability to unseen scenarios. Recently, Multi-Instance Learning (MIL) is leveraged by most methods [5, 6, 8, 23, 29, 45] to tackle wVAD due to its ability to model patterns under coarseThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18319",
        "chosen": "This paper proposes prompt-enhanced Multi-Instance Learning (MIL) for weakly supervised video anomaly detection. The method uses abnormal-aware prompts and normal context prompts to capture diverse abnormal patterns and generate clear event boundaries. Experiments show state-of-the-art performance on public benchmarks.",
        "rejected": "Weakly-supervised Video Anomaly Detection (wVAD) aims to detect anomalies using only video-level labels. This paper proposes prompt-enhanced MIL, incorporating abnormal-aware prompts and normal context prompts to generate clear event boundaries and capture diverse abnormal patterns. Extensive experiments show that this method achieves state-of-the-art performance on three public benchmarks."
    },
    {
        "prompt": "summary: Segment Anything Model (SAM) has emerged as a transformative approach in image segmentation, acclaimed for its robust zero-shot segmentation capabilities and flexible prompting system. Nonetheless, its performance is challenged by images with degraded quality. Addressing this limitation, we propose the Robust Segment Anything Model (RobustSAM), which enhances SAM’s performance on lowquality images while preserving its promptability and zeroshot generalization. Our method leverages the pre-trained SAM model with only marginal parameter increments and computational requirements. The additional parameters of RobustSAM can be optimized within 30 hours on eight GPUs, demonstrating its feasibility and practicality for typical research laboratories. We also introduce the RobustSeg dataset, a collection of 688K image-mask pairs with different degradations designed to train and evaluate our model optimally. Extensive experiments across various segmentation tasks and datasets confirm RobustSAM’s superior performance, especially under zero-shot conditions, underscoring its potential for extensive real-world application. Additionally, our method has been shown to effectively improve the performance of SAM-based downstream tasks such as single image dehazing and deblurring. Project Page: https://robustsam.github.io † Part of the work done during internship at Snap Research. * Co-corresponding authors This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4081",
        "chosen": "This paper introduces RobustSAM, enhancing Segment Anything Model (SAM) for better performance on low-quality images. The method preserves SAM's promptability and zero-shot generalization with minimal parameter increments. Experiments confirm superior performance across various segmentation tasks.",
        "rejected": "The Segment Anything Model (SAM) excels in zero-shot segmentation but struggles with low-quality images. RobustSAM enhances SAM’s performance on such images with minimal additional parameters, preserving its promptability and zero-shot capabilities. It uses a pre-trained SAM model and can be optimized within 30 hours on eight GPUs. The RobustSeg dataset, comprising 688K image-mask pairs with various degradations, supports training and evaluation. Experiments confirm RobustSAM's superior performance, especially under zero-shot conditions, making it ideal for real-world applications like image dehazing and deblurring."
    },
    {
        "prompt": "summary: We propose SceneTex, a novel method for effectively generating high-quality and style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike previous methods that either iteratively warp 2D views onto a mesh surface or distillate diffusion latent features without accurate geometric and style cues, SceneTex formulates the texture synthesis task as an optimization problem in the RGB space where style and geometry consistency are properly reflected. At its core, SceneTex proposes a multiresolution texture field to implicitly encode the mesh appearance. We optimize the target texture via a score-distillationbased objective function in respective RGB renderings. To further secure the style consistency across views, we introduce a cross-attention decoder to predict the RGB values by cross-attending to the pre-sampled reference locations in each instance. SceneTex enables various and accurate texture synthesis for 3D-FRONT scenes, demonstrating significant improvements in visual quality and prompt fidelity over the prior texture generation methods. 1. Introduction Synthesizing high-quality 3D contents is an essential yet highly demanding task for numerous applications, including gaming, film making, robotic simulation, autonomous driving, and upcoming VR/AR scenarios. With an increasing number of 3D content datasets, the computer vision and graphics community has witnessed a soaring research interest in the field of 3D geometry generation [2, 12, 36, 38, 40, 60, 68, 73]. Despite achieving a remarkable success in 3D geometry modeling, generating the object appearance, i.e. textures, is still bottlenecked by laborious human efforts. It typically requires a substantially long time for designing and adjustment, and immense 3D modelling expertise with tools such as Blender. As such, automatic designing and augmenting the textures has not yet been fully industrialized due to a huge demand for human expertise and financial expenses. Leveraging the recent advances of 2D diffusion models, tremendous progress has been made for text-to-3D generation, especially for synthesizing textures of given shapes [8, 39, 50]. Seminal work such as Text2Tex [8] and Latent-Paint [39] have achieved great success in generating high-quality appearances for objects, facilitating highfidelity texture synthesis from input prompts. Despite the This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 21081",
        "chosen": "This paper proposes SceneTex, a method for generating high-quality, style-consistent textures for indoor scenes using depth-to-image diffusion priors. The method formulates texture synthesis as an optimization problem and introduces a cross-attention decoder for style consistency. Results show significant improvements in visual quality and prompt fidelity.",
        "rejected": "SceneTex is a novel method for generating high-quality, style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike previous methods, SceneTex formulates texture synthesis as an RGB-space optimization problem, ensuring geometry and style consistency. It employs a multi-resolution texture field and score-distillation-based objective function for optimization, and a cross-attention decoder for style consistency across views. SceneTex shows significant improvements in visual quality and prompt fidelity over prior methods, enabling accurate texture synthesis for 3D-FRONT scenes."
    },
    {
        "prompt": "summary: Recent works on text-to-3d generation show that using only 2D diffusion supervision for 3D generation tends to produce results with inconsistent appearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals with extra legs). Existing methods mainly address this issue by retraining diffusion models with images rendered from 3D data to ensure multi-view consistency while struggling to balance 2D generation quality with 3D consistency. In this paper, we present a new framework Sculpt3D that equips the current pipeline with explicit injection of 3D priors from retrieved reference objects without re-training the 2D diffusion model. Specifically, we demonstrate that high-quality and diverse 3D geometry can be guaranteed by keypoints supervision through a sparse ray sampling approach. Moreover, to ensure accurate appearances of different views, we further modulate the output of the 2D diffusion model to the correct patterns of the template views without altering the generated object’s style. These two decoupled designs effectively harness 3D information from reference objects to generate 3D objects while preserving the generation quality of the 2D diffusion model. Extensive experiments show our method can largely improve the multi-view consistency while retaining fidelity and diversity. Our project page is available at: https://stellarcheng.github.io/Sculpt3D/. 1. Introduction There has been growing research attention towards text-to3d generation. Compared to image generation, the data available for 3D generation is less in quantity and lower in quality. Thus, many studies [19, 28, 41] have begun to generate 3D objects using 2D text-to-image models [9, 30] as supervision to leverage their strong priors learned from billions of real images. ∗Corresponding authors. Lifelike tiger with fierce expression An antique glass perfume bottle Prolificdreamer Ours Reference Sets Figure 1. Comparison of objects generated by our method and ProlificDreamer. We retain the 2D model’s capability to produce high-fidelity objects and adaptively learn 3D information from reference templates retrieved from external datasets. These methods mainly contain two steps: the first step is to continuously sample images from different views of a randomly initialized 3D representation (e.g. NeRF [26], DMTet [33]). The second step uses a 2D diffusion model to individually judge whether each image is a high-quality image that conforms to the text description. Compared to 2D image generation, 3D generation not only requires producing high-quality images for each individual viewpoint but also needs to create plausible shapes and appearances as a whole 3D object. Thus, a high-quality 2D generative model and a mechanism that can accurately provide 3D priors are two keys to achieving decent 3D generation results. Since early works [19, 28, 41] mainly use the sole 2D diffusion model as supervision, they tend to produce inaccurate shapes (shape ambiguity) and appearances that are inconsistent across viewpoints (appearance ambiguity), as shown in Figure 1 left, where examples include incomplete bottles, tigers with multiple legs, and tails. Recently, some efforts have been made to expand the 3D datasets [10]. Following this, there were immediate attempts to retrain 2D diffusion models on these 3D datasets to learn 3D information [21, 22, 35]. Although these methThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 10228",
        "chosen": "This paper presents Sculpt3D, a framework for text-to-3D generation that improves multi-view consistency by injecting 3D priors from reference objects without retraining the 2D diffusion model. The method uses keypoints supervision and modulates the 2D diffusion model's output to ensure accurate appearances across different views. Experiments show significant improvements in multi-view consistency while maintaining fidelity and diversity.",
        "rejected": "Recent research in text-to-3D generation has struggled with multi-view consistency and shape accuracy. Sculpt3D proposes a novel framework that integrates 3D priors from reference objects into the 2D diffusion model without retraining, enhancing the quality and diversity of generated 3D objects."
    },
    {
        "prompt": "summary: Recently, transformer-based methods have achieved state-of-the-art prediction quality on human pose estimation(HPE). Nonetheless, most of these top-performing transformer-based models are too computation-consuming and storage-demanding to deploy on edge computing platforms. Those transformer-based models that require fewer resources are prone to under-fitting due to their smaller scale and thus perform notably worse than their larger counterparts. Given this conundrum, we introduce SDPose, a new self-distillation method for improving the performance of small transformer-based models. To mitigate the problem of under-fitting, we design a transformer module named Multi-Cycled Transformer(MCT) based on multiple-cycled forwards to more fully exploit the potential of small model parameters. Further, in order to prevent the additional inference compute-consuming brought by MCT, we introduce a self-distillation scheme, extracting the knowledge from the MCT module to a naive forward model. Specifically, on the MSCOCO validation dataset, SDPose-T obtains 69.7% mAP with 4.4M parameters and 1.8 GFLOPs. Furthermore, SDPose-S-V2 obtains 73.5% mAP on the MSCOCO validation dataset with 6.2M parameters and 4.7 GFLOPs, achieving a new state-of-the-art among predominant tiny neural network methods. 1. Introduction Human Pose Estimation (HPE) aims to estimate the position of each joint point of the human body in a given image. HPE tasks support a wide range of downstream tasks such as activity recognition[1], motion capture[2], etc. Recently with the ViT model being proven effective on many visual tasks, many transformer-based methods[3–5] have achieved excellent performance on HPE tasks. Compared with past CNN-based methods[6], transformer-based mod*Equal Contribution. †Corresponding Authors. Figure 1. Comparsions between other small models and our methods on MSCOCO validation dataset. Compared to other methods, our approach can significantly reduce the scale while maintaining the same performance, or greatly improve performance under the same scale. els are much more powerful in capturing the relationship between visual elements. However, most of them are large and computationally expensive. The state-of-the-art(SOTA) transformer-based model[5] has 632 million parameters and requires 122.9 billion floating-point operations. Such a large-scale model is difficult to deploy on edge computing devices and cannot accommodate the growing development of embodied intelligence. However, when the CNN or ViT used as a backbone is not of sufficient scale, transformerbased models are not able to learn the relationship between keypoints and visual elements well resulting in poor performance. Stacking more transformer layers is a viable approach[4], but this also increases the scale of the network resulting in larger parameters and the difficulty of edge deployment. To help small models learn better, one possible way is to distill knowledge from big model to small model[7, 8]. However, previous distillation methods have the following drawbacks: (1) To align the vector space, an additional manipulation is required during feature distillation[9] and leads to a potential performance decrease. (2) A huge extra This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1082",
        "chosen": "This paper introduces SD-Pose, a self-distillation method for improving the performance of small transformer-based models in human pose estimation. The method features a Multi-Cycled Transformer module and a self-distillation scheme, achieving state-of-the-art performance among small models on the MSCOCO validation dataset.",
        "rejected": "SDPose introduces a self-distillation method to improve small transformer-based models' performance on human pose estimation tasks. By employing a Multi-Cycled Transformer module and self-distillation scheme, SDPose significantly boosts model efficiency and accuracy, achieving state-of-the-art results among tiny neural network methods."
    },
    {
        "prompt": "summary: Category-level object pose estimation, aiming to predict the 6D pose and 3D size of objects from known categories, typically struggles with large intra-class shape variation. Existing works utilizing mean shapes often fall short of capturing this variation. To address this issue, we present SecondPose, a novel approach integrating object-specific geometric features with semantic category priors from DINOv2. Leveraging the advantage of DINOv2 in providing SE(3)-consistent semantic features, we hierarchically extract two types of SE(3)-invariant geometric features to further encapsulate local-to-global object-specific information. These geometric features are then point-aligned with DINOv2 features to establish a consistent object representation under SE(3) transformations, facilitating the mapping from camera space to the pre-defined canonical space, thus further enhancing pose estimation. Extensive experiments on NOCS-REAL275 demonstrate that SecondPose achieves a 12.4% leap forward over the state-of-the-art. Moreover, on a more complex dataset HouseCat6D which provides photometrically challenging objects, SecondPose still surpasses other competitors by a large margin. 1. Introduction Category-level pose estimation involves estimating the complete 9 degrees-of-freedom (DoF) object pose, encompassing 3D rotation, 3D translation, and 3D metric size, for arbitrary objects within a known set of categories. This task has garnered significant research interest due to its essential role in various applications, including the AR/VR industry [30, 38, 40, 55], robotics [51, 52, 54], and scene un∗Equal contributions. † Corresponding author (e-mail: guangyao.zhai@tum.de). Figure 1. Categorical SE(3)-consistent features. We visualize our fused features by PCA. Colored points highlight the most corresponding parts, where our proposed feature achieves consistent alignment cross instances (left vs. middle) and maintains consistency on the same instance of different poses (middle vs. right). derstanding [1, 7, 53]. In contrast to traditional instancelevel pose estimation methods [6,21], which rely on specific 3D CAD models for each target object, the category-level approach necessitates greater adaptability to accommodate inherent shape diversity within each category. Effectively addressing intra-class shape variations has thus become a central focus, crucial for real-world applications where objects within a category may exhibit significant differences in shape while sharing the same general category label. Mean Shape vs. Semantic Priors. One common approach to handle intra-class shape variation involves using explicit mean shapes as prior knowledge [24, 39, 57]. These methods typically consist of two functional modules: one for reconstructing the target object by slightly deforming the mean shape and another for regressing the 9D pose based on the reconstructed object [24,39] or enhanced interThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 9959",
        "chosen": "This paper presents SecondPose, a novel approach for category-level object pose estimation that integrates object-specific geometric features with semantic category priors from DINOv2. The method extracts SE(3)-invariant geometric features and aligns them with DINOv2 features, achieving significant performance improvements on multiple datasets.",
        "rejected": "SecondPose addresses intra-class shape variation in category-level object pose estimation by integrating geometric features with semantic category priors from DINOv2. This approach enhances the consistency and accuracy of pose predictions, demonstrating superior performance on NOCS-REAL275 and HouseCat6D datasets."
    },
    {
        "prompt": "summary: We propose a novel self-supervised embedding to learn how actions sound from narrated in-the-wild egocentric videos. Whereas existing methods rely on curated data with known audio-visual correspondence, our multimodal contrastive-consensus coding (MC3) embedding reinforces the associations between audio, language, and vision when all modality pairs agree, while diminishing those associations when any one pair does not. We show our approach can successfully discover how the long tail of human actions sound from egocentric video, outperforming an array of recent multimodal embedding techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal tasks. 1. Introduction Human activity often produces sounds. Closing a door, chopping vegetables, typing on a keyboard, talking with a friend— our interactions with the objects and people around us generate audio that reveals our physical behaviors. These sounds can be strongly associated with the subjects of our activity and how we perform it. For example, opening a water bottle sounds different than opening a cabinet; chopping sweet potatoes sounds different than chopping onions; chopping onions sounds different than mincing onions (the same object). Understanding the link between sounds and actions is valuable for a number of applications, such as multimodal activity recognition, cross-modal retrieval, content generation, or forecasting the physical effects of a person’s actions. How should AI learn about sounding actions? Existing work typically curates annotated datasets for supervised learning [9, 22, 28, 45], taking care to select events or actions that have associated sounds (e.g., lawnmowing, chopping), while others deliberately collect videos of object collisions (e.g., striking objects with a drumstick [43] or crashing into them with a robot [10, 17]), or develop physics-based simulations [16]. On the one hand, these approaches are appealing for their ability to focus on meaningful audio-visual correspondences. On the other hand, their curated nature risks limiting the scope of sounding actions that can be learned. Action: C digs the  soil with a hoe. Action: C moves a  metal cutting machine  with hands. Does the action sound? Does the action sound? Figure 1. We aim to distinguish sounds that are directly caused by human actions (bottom) from those that are not (top). Given egocentric training videos with language descriptions of the camera wearer’s (“C\") current action, we learn an embedding where the audio and visual features of any given clip are best aligned only when both are also consistent with the language. This allows discerning clips where the audio and vision may be correlated (e.g., the cutting machine running making loud noise in top row) versus those where the sounds are driven by human action (digging in bottom row)—importantly, without language at inference time. Instead, we aim to learn how human actions sound from narrated in-the-wild egocentric videos. See Figure 1. Given a pool of videos of everyday human activity, the goal is to learn a cross-modal representation where sounding actions cluster together based on how they look and sound. By sampling the videos freely, we can broaden the scope to discover the breadth of sounding actions without having to rely on a closed, pre-defined set of action categories. In particular, by focusing on unscripted egocentric video from wearable cameras in daily-life settings [11, 26], we aim to include subtle and long-tail scenarios unavailable in curated datasets, such as sounds of keys jangling when unlocking a door, scissors snipping when cutting the dog’s fur, or fingernails scratching on one’s own arm. Egocentric video is a particularly attractive source here because 1) human interaction sounds are more audible in near-field egocentric recordings and 2) passively captured long-form ego-video simply covers more 1 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27252",
        "chosen": "This paper proposes SoundingActions, a self-supervised embedding method for learning how actions sound from narrated egocentric videos. The method reinforces audio-visual-language associations when all modalities agree, outperforming recent techniques on multiple datasets and tasks.",
        "rejected": "This paper presents MC3 embedding, a self-supervised multimodal learning method for associating actions with sounds in egocentric videos. MC3 outperforms existing techniques in discovering how sounding actions cluster based on visual and audio cues, advancing cross-modal tasks like activity recognition and content generation."
    },
    {
        "prompt": "summary: Learning generalizable visual representations from Internet data has yielded promising results for robotics. Yet, prevailing approaches focus on pre-training 2D representations, being sub-optimal to deal with occlusions and accurately localize objects in complex 3D scenes. Meanwhile, 3D representation learning has been limited to single-object understanding. To address these limitations, we introduce a novel 3D pre-training framework for robotics named SUGAR that captures semantic, geometric and affordance properties of objects through 3D point clouds. We underscore the importance of cluttered scenes in 3D representation learning, and automatically construct a multi-object dataset benefiting from cost-free supervision in simulation. SUGAR employs a versatile transformer-based model to jointly address five pre-training tasks, namely cross-modal knowledge distillation for semantic learning, masked point modeling to understand geometry structures, grasping pose synthesis for object affordance, 3D instance segmentation and referring expression grounding to analyze cluttered scenes. We evaluate our learned representation on three robotic-related tasks, namely, zero-shot 3D object recognition, referring expression grounding, and language-driven robotic manipulation. Experimental results show that SUGAR’s 3D representation outperforms state-of-the-art 2D and 3D representations. 1. Introduction Visual perception plays an essential role in robotics and enables autonomous agents to understand and interact with their physical environment. Nevertheless, learning generalizable visual representations for robotics is challenging due to the scarcity of real robot data and the large variety of real-world scenes. Despite substantial efforts in robot data accumulation [2, 76, 77] and augmentation [39, 85], it remains prohibitively expensive to collect large-scale datasets comprising a broad range of robotic tasks. To alleviate the burden of data collection, recent endeavors [36, 37, 48, 49, 51, 62] have sought to leverage largescale internet data to pre-train 2D visual representations for Pre-training Robotics tasks Semantic a blue plastic  water bottle with a black lid mUlti-object scenes RGB Depth Label Geometry Affordance pick up the knife and leave it on the chopping board grasp the red cup and lift it Figure 1. We introduce SUGAR , a pre-training framework for robotic-related tasks, which learns semantic, geometry and affordance on both single- and multi-object scenes. robotics. For example, MVP [62], VIP [48] and VC-1 [49] use self-supervised learning on image or video datasets, while EmbCLIP [37], R3M [51] and Voltron [36] further perform cross-modal pre-training based on videos with aligned language descriptions. While these 2D representations have demonstrated promising performance, they still fall short in addressing occlusions in complex cluttered scenes [79] and accurately predicting robotic actions [7] in the 3D world. Recently, growing research attention has been paid to 3D visual representations for robotics. A majority of approaches train 3D-based models from scratch [7, 43, 69], potentially losing the generalization ability. Several recent works lift pretrained 2D features to the 3D space [20, 34, 56, 89], which compromise efficiency due to processing multi-view images and do not fully take advantage of 3D data. To improve 3D representation learning, prior endeavours propose selfsupervised pre-training of 3D models [44, 53, 59, 86]. Pretraining in existing work, however, is typically limited to single objects and complete point clouds, hence, ignoring This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18049",
        "chosen": "This paper introduces SUGAR, a 3D pre-training framework for robotics that captures semantic, geometric, and affordance properties of objects through 3D point clouds. The framework includes five pre-training tasks and demonstrates superior performance on robotic-related tasks compared to state-of-the-art 2D and 3D representations.",
        "rejected": "SUGAR is a 3D pre-training framework designed to learn semantic, geometric, and affordance properties of objects through point clouds. It excels in handling occlusions and accurately localizing objects in cluttered scenes, showing improved performance over state-of-the-art 2D and 3D representations in various robotic tasks."
    },
    {
        "prompt": "summary: Automatic text-to-3D generation that combines Score Distillation Sampling (SDS) with the optimization of volume rendering has achieved remarkable progress in synthesizing realistic 3D objects. Yet most existing text-to-3D methods by SDS and volume rendering suffer from inaccurate geometry, e.g., the Janus issue, since it is hard to explicitly integrate 3D priors into implicit 3D representations. Besides, it is usually time-consuming for them to generate elaborate 3D models with rich colors. In response, this paper proposes GSGEN, a novel method that adopts Gaussian Splatting, a recent stateof-the-art representation, to text-to-3D generation. GSGEN aims at generating high-quality 3D objects and addressing existing shortcomings by exploiting the explicit nature of Gaussian Splatting that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimiza†Corresponding author tion, a coarse representation is established under 3D point cloud diffusion prior along with the ordinary 2D SDS optimization, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative appearance refinement to enrich texture details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D assets with delicate details and accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components. Our code is available at https://github.com/gsgen3d/gsgen. 1. Introduction Diffusion model based text-to-image generation [1, 55, 57, 58] has achieved remarkable success in synthesizing photorealistic images from textual prompts. Nevertheless, for highquality text-to-3D content generation, the advancements lag behind that of image generation due to the inherent complexThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 21401",
        "chosen": "This paper proposes GSGEN, a text-to-3D generation method using Gaussian Splatting. The method includes geometry optimization and appearance refinement stages, generating high-quality 3D objects with accurate geometry and rich details. Evaluations show its effectiveness, particularly in capturing high-frequency components.",
        "rejected": "GSGEN proposes using Gaussian Splatting for text-to-3D generation to address geometry inaccuracies and time consumption issues. Through progressive optimization stages including geometry establishment and appearance refinement, GSGEN generates high-quality 3D objects with accurate geometries and rich colors."
    },
    {
        "prompt": "summary: Federated learning facilitates the collaborative learning of a global model across multiple distributed medical institutions without centralizing data. Nevertheless, the expensive cost of annotation on local clients remains an obstacle to effectively utilizing local data. To mitigate this issue, federated active learning methods suggest leveraging local and global model predictions to select a relatively small amount of informative local data for annotation. However, existing methods mainly focus on all local data sampled from the same domain, making them unreliable in realistic medical scenarios with domain shifts among different clients. In this paper, we make the first attempt to assess the informativeness of local data derived from diverse domains and propose a novel methodology termed Federated Evidential Active Learning (FEAL) to calibrate the data evaluation under domain shift. Specifically, we introduce a Dirichlet prior distribution in both local and global models to treat the prediction as a distribution over the probability simplex and capture both aleatoric and epistemic uncertainties by using the Dirichlet-based evidential model. Then we employ the epistemic uncertainty to calibrate the aleatoric uncertainty. Afterward, we design a diversity relaxation strategy to reduce data redundancy and maintain data diversity. Extensive experiments and analysis on five real multi-center medical image datasets demonstrate the superiority of FEAL over the state-of-the-art active learning methods in federated scenarios with domain shifts. The code will be available at https://github.com/JiayiChen815/FEAL. ∗Equal contribution. †Yong Xia is the corresponding author. This work was supported in part by Shenzhen Science and Technology Program under Grants JCYJ20220530161616036, National Natural Science Foundation of China under Grants 62171377 and 62271405, Ningbo Clinical Research Center for Medical Imaging under Grant 2021L003 (Open Project: 2022LYKFZD06), and Foshan HKUST Projects under Grants FSUST21-HKUST10E and FSUST21-HKUST11E. ? 𝑈! 𝐿! ? 𝑈\" 𝐿\" 𝑈# 𝐿# Annotate Distribute Aggregate ⋯ Client K Server Client 1 Client 2 Train (a) FAL scheme 0.0 2.5 5.0 7.5 10.0 12.5 Energy score 0.00 0.05 0.10 0.15 0.20 0.25 Density Client 1 Client 2 Client 3 Client 4 (b) KDE of energy score 1 2 3 4 Client 1 2 3 4 Client 1.0000 0.0001 0.0001 0.0001 0.0001 1.0000 0.0130 0.0001 0.0001 0.0130 1.0000 0.0031 0.0001 0.0001 0.0031 1.0000 0.0 0.2 0.4 0.6 0.8 1.0 (c) p-value Figure 1. Illustration of federated active learning (FAL) in the presence of domain shift. (a) FAL comprises model distribution, local training, model aggregation, and data annotation. (b) The KDE of energy scores depicts domain shifts across clients. (c) The low p-values in cross-client KDE of energy scores indicate the existence of significant domain shifts between all client pairs. 1. Introduction Federated learning enables collaborative learning across multiple clinical institutions (i.e., clients) to learn a unified model on the central server through model aggregation while preserving the data privacy at each client [21, 36, 57] (see Fig. 1 (a)). Unfortunately, such a learning pipeline reThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11439",
        "chosen": "This paper presents FEAL, a federated evidential active learning method for medical imaging that addresses domain shifts among clients. The method uses a Dirichlet prior distribution to capture uncertainties and a diversity relaxation strategy to maintain data diversity. Experiments on multi-center datasets demonstrate its superiority over existing methods.",
        "rejected": "FEAL introduces federated evidential active learning to calibrate data evaluation under domain shifts in medical federated learning scenarios. By leveraging Dirichlet-based evidential models, FEAL effectively selects informative local data for annotation, improving model performance across diverse domains."
    },
    {
        "prompt": "summary: Despite many attempts to leverage pre-trained text-toimage models (T2I) like Stable Diffusion (SD) [25] for controllable image editing, producing good predictable results remains a challenge. Previous approaches have focused on either fine-tuning pre-trained T2I models on specific datasets to generate certain kinds of images (e.g., with a specific object or person), or on optimizing the weights, text prompts, and/or learning features for each input image in an attempt to coax the image generator to produce the desired result. However, these approaches all have shortcomings and fail to produce good results in a predictable and controllable manner. To address this problem, we present TiNO-Edit, an SD-based method that focuses on optimizing the noise patterns and diffusion timesteps during editing, something previously unexplored in the literature. With this simple change, we are able to generate results that both better align with the original images and reflect the desired result. Furthermore, we propose a set of new loss functions that operate in the latent domain of SD, greatly speeding up the optimization when compared to prior losses, which operate in the pixel domain. Our method can be easily applied to variations of SD including Textual Inversion [13] and DreamBooth [27] that encode new concepts and incorporate them into the edited results. We present a host of image-editing capabilities enabled by our approach. Our code is publicly available at https://github.com/SherryXTChen/TiNO-Edit. 1. Introduction Computer-generated image synthesis has been studied for decades for its wide range of applications including content creation, marketing and advertising, visualization/simulation, entertainment, and storytelling. Re*Corresponding author email: xchen774@ucsb.edu Pure text-guided & Reference-guided image editing Original Ref w/o ref w/ref “sandwich” →“cake” + “a magenta hat” Stroke-guided image editing Original User input Result Original User input Result + “a sunflower” + “curtains” Image composition Original User input Result Original User input Result + “a scarecrow” “white” →“yellow plaid” Image editing with DreamBooth (DB) or Textual Inversion (TI) Original DB Result Original TI Result “photo” →⟨concept⟩ “sky” →⟨concept⟩ Figure 1. Overview of capabilities enabled by TiNO-Edit. TiNO-Edit offers various image-editing capabilities and can be run with DreamBooth (DB) [27] or Textual Inversion (TI) [13]. By leveraging diffusion timestep and noise optimization techniques, it can generate realistic and high quality outputs. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 6337",
        "chosen": "This paper introduces TiNO-Edit, a method for robust diffusion-based image editing that optimizes noise patterns and diffusion timesteps. The method also proposes new loss functions in the latent domain, improving optimization speed and enabling various image-editing capabilities.",
        "rejected": "TiNO-Edit focuses on optimizing noise patterns and diffusion timesteps during image editing using Stable Diffusion models. Introducing new latent domain loss functions, this method achieves better alignment with original images and desired edits, facilitating realistic and high-quality image synthesis."
    },
    {
        "prompt": "summary: We present a method to generate full-body selfies from photographs originally taken at arms length. Because selfcaptured photos are typically taken close up, they have limited field of view and exaggerated perspective that distorts facial shapes. We instead seek to generate the photo some one else would take of you from a few feet away. Our approach takes as input four selfies of your face and body, a background image, and generates a full-body selfie in a desired target pose. We introduce a novel diffusion-based approach to combine all of this information into high-quality, well-composed photos of you with the desired pose and background. 1. Introduction The prevalence of selfies has skyrocketed in recent years, with an estimated 93 million taken each day. Despite their popularity, they suffer from multiple shortcomings: (1) they capture only the upper portion of the subject, (2) the closeup camera viewpoint distorts faces and requires awkward poses (e.g., with arm reaching out), and (3) it is difficult to compose a shot that optimally captures both the subject and This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 6701",
        "chosen": "This paper presents a method to generate full-body selfies from arm's length photographs using a diffusion-based approach. The method takes four selfies, a background image, and generates a well-composed full-body selfie in a desired pose.",
        "rejected": "This work presents a diffusion-based method to generate full-body selfies from close-up photographs. Utilizing multiple input selfies and a background image, it creates well-composed photos of subjects in desired poses, overcoming limitations of traditional selfies such as distorted perspectives and limited field of view."
    },
    {
        "prompt": "summary: Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that generative AI models, e.g., Diffusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities. 1. Introduction Tumor synthesis enables the creation of artificial tumor examples in medical images [11, 42, 88], it is particularly valuable when there is a dearth or complete absence of pervoxel annotated real tumors (e.g., early-stage tumors) for effective AI training. Typically, to train AI models for tumor detection in multiple (N) organs, annotated real tumor examples from each of these organs are necessary, and ideally, in substantial numbers [14, 43, 54, 55, 97]. Furthermore, AI models often fail to generalize across images from different hospitals, which may vary due to variations *Correspondence to Zongwei Zhou (ZZHOU82@JH.EDU) in imaging protocols, patient demographics, and scanner manufacturers [61, 95, 96]. The challenge amplifies with the need for extensive manual annotations, a task that could demand up to 25 human years for annotating just one tumor type [1, 9, 83]. The task of collecting and annotating a comprehensive dataset encompassing tumors from multiple organs (N) and images from numerous hospitals (M) is daunting, considering both annotation cost and complexity (N × M). We hypothesize that tumor synthesis could solve this challenge by creating various tumor types across nontumor images from multiple hospitals, even when only one tumor type is available, thereby simplifying the complexity from N × M to 1 × M. Success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being generalizable in detecting real tumors in images sourced from different hospitals. Previous studies have introduced generative models to create synthetic medical data (not limited to tumors) such as polyp detection from colonoscopy videos [69], COVID-19 detection from Chest X-ray [26, 56, 87], and diabetic lesion detection from retinal images [78]—refer to §5 for a comprehensive review. However, these studies have primarily focused on enhancing the detection and segmentation of specific tumors without fully exploring the wider generalizability of these models across different organs and patient demographics. This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT)1. Early-stage tumors typically present small, round, or oval shapes with minimal deformation and exhibit relatively simple and uni1Note that, owing to the public dataset constraints, we have only verified the similarity across early hepatocellular carcinoma and intrahepatic cholangiocarcinoma from the liver, pancreatic ductal adenocarcinoma from the pancreas, and renal cell carcinoma from kidneys. 1 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11147",
        "chosen": "This paper discusses generalizable tumor synthesis in medical images using diffusion models, focusing on early-stage tumors with similar imaging characteristics across organs. The method creates realistic synthetic tumors and trains AI models for detecting real tumors across different domains.",
        "rejected": "The paper discusses tumor synthesis for training AI models in detecting and segmenting tumors across different organs and imaging domains. Successful tumor synthesis requires creating visually realistic and generalizable tumors, ensuring AI models can effectively detect real tumors in varied medical images."
    },
    {
        "prompt": "summary: Pretrained diffusion models and their outputs are widely accessible due to their exceptional capacity for synthesizing high-quality images and their open-source nature. The users, however, may face litigation risks owing to the models’ tendency to memorize and regurgitate training data during inference. To address this, we introduce Anti-Memorization Guidance (AMG), a novel framework employing three targeted guidance strategies for the main causes of memorization: image and caption duplication, and highly specific user prompts. Consequently, AMG ensures memorization-free outputs while maintaining high image quality and text alignment, leveraging the synergy of its guidance methods, each indispensable in its own right. AMG also features an innovative automatic detection system for potential memorization during each step of inference process, allows selective application of guidance strategies, minimally interfering with the original sampling process to preserve output utility. We applied AMG to pretrained Denoising Diffusion Probabilistic Models (DDPM) and Stable Diffusion across various generation tasks. The results demonstrate that AMG is the first approach to successfully eradicates all instances of memorization with no or marginal impacts on image quality and text-alignment, as evidenced by FID and CLIP scores. 1. Introduction Diffusion models [12, 23, 34] have attracted substantial interest, given their superiority in terms of diversity, fidelity, scalability [28] and controllability [24] over previous generative models including VAEs [17], normalizing flows [29], and GANs [10, 14–16]. With guidance techniques [7, 11], diffusion models can be further improved by the strategical diversity-fidelity trade-off. State-of-the-art diffusion models trained on vast web-scale datasets are widespreadly used and have seen deployment at a commercial scale [1, 30, 31]. Such widespread adoption, however, has significantly heightened the litigation risks for companies using these models, particularly due to allegations that the models memorize and reproduce training data during inference without informing the data owners and the users of diffusion Figure 1. Stable Diffusion’s capacity to memorize training data, manifested as pixel-level memorization (left) and object-level memorization (right). Our approach successfully guides pretrained diffusion models to produce memorization-free outputs. models. This potentially violates copyright laws and introduces ethical dilemmas, further complicated by the fact that the extensive size of training sets impedes detailed human review, leaving the intellectual property rights of the data sources largely undetermined. An ongoing example is that a legal action contends that Stable Diffusion is a 21st-century collage tool that remixes the copyrighted works of millions of artists whose work was used as training data [32]. Prior studies [4, 35, 36] have observed memorization in pretrained diffusion models, particularly during unconditional CIFAR-10 [18] and text-conditional LAION dataset [33] generations. While previous research proposed strategies to reduce memorization, these often lead to only modest improvements and fail to fully eliminate the issue. The effectiveness often come with reduced output quality and text-alignment [36], the need for retraining models [4], and extensive manual intervention [19]. Moreover, these strategies lack an automated way to differentiate potential memorization cases for targeted mitigation. For example, [19] relies on a predefined list of text prompts prone to causing memorization, and [36] applies randomization mechanisms uniformly without distinguishing between scenarios. In this paper, we undertake the following systematic efforts to address the issue of memorization. Firstly, we have This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 8425",
        "chosen": "This paper introduces Anti-Memorization Guidance (AMG), a framework to eliminate memorization in diffusion models while maintaining image quality and text alignment. AMG uses three guidance strategies targeting image and caption duplication and specific user prompts. It includes an automatic detection system for potential memorization during inference. Experiments show AMG successfully removes memorization with minimal impact on quality and alignment.",
        "rejected": "Towards Memorization-Free Diffusion Models. This research proposes a novel approach to mitigate memorization in diffusion models, focusing on enhancing generalization while maintaining sample quality. By integrating specific regularization techniques, the model can achieve superior performance on diverse datasets without overfitting."
    },
    {
        "prompt": "summary: 3D pose transfer that aims to transfer the desired pose to a target mesh is one of the most challenging 3D generation tasks. Previous attempts rely on well-defined parametric human models or skeletal joints as driving pose sources. However, to obtain those clean pose sources, cumbersome but necessary pre-processing pipelines are inevitable, hindering implementations of the real-time applications. This work is driven by the intuition that the robustness of the model can be enhanced by introducing adversarial samples into the training, leading to a more invulnerable model to the noisy inputs, which even can be further extended to directly handling the real-world data like raw point clouds/scans without intermediate processing. Furthermore, we propose a novel 3D pose Masked Autoencoder (3D-PoseMAE), a customized MAE that effectively learns 3D extrinsic presentations (i.e., pose). 3D-PoseMAE facilitates learning from the aspect of extrinsic attributes by simultaneously generating adversarial samples that perturb the model and learning the arbitrary raw noisy poses via a multi-scale masking strategy. Both qualitative and quantitative studies show that the transferred meshes given by our network result in much better quality. Besides, we demonstrate the strong generalizability of our method on various poses, different domains, and even raw scans. Experimental results also show meaningful insights that the intermediate adversarial samples generated in the training can successfully attack the existing pose transfer models. 1. Introduction As a promising and challenging task, 3D pose transfer has been consistently drawing research attention from the computer vision community [10, 27, 37, 41]. The task aims at transferring a source pose to a target identity mesh and keeping the intrinsic attributes (i.e., shape) of the identity mesh. Aside from pure research interests, transferring desired poses to target 3D models has various potential applications in the film industry, games, AR/VR, etc [8, 11, 12]. To achieve data-driven learning, existing 3D pose transfer methods rely on different prerequisites to the data sources, which severely limits their further real-world implementations. Firstly, many existing 3D pose transfer methods [30, 45] cannot directly be generalized to unseen This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 2295",
        "chosen": "This paper presents a 3D pose transfer method enhanced by adversarial samples and a 3D pose Masked Autoencoder (3D-PoseMAE). The method improves robustness to noisy inputs and generalizes well to raw point clouds/scans. The multi-scale masking strategy helps learn arbitrary raw noisy poses. Results show better quality transferred meshes and strong generalizability.",
        "rejected": "Towards Robust 3D Pose Transfer with Adversarial Learning. The study introduces an adversarial learning framework to improve the robustness of 3D pose transfer across different subjects. Through comprehensive experiments, it demonstrates significant improvements in accuracy and stability under various conditions."
    },
    {
        "prompt": "summary: Audio-visual segmentation (AVS) is a challenging task that involves accurately segmenting sounding objects based on audio-visual cues. The effectiveness of audio-visual learning critically depends on achieving accurate crossmodal alignment between sound and visual objects. Successful audio-visual learning requires two essential components: 1) a challenging dataset with high-quality pixel-level multi-class annotated images associated with audio files, and 2) a model that can establish strong links between audio information and its corresponding visual object. However, these requirements are only partially addressed by current methods, with training sets containing biased audiovisual data, and models that generalise poorly beyond this biased training set. In this work, we propose a new costeffective strategy to build challenging and relatively unbiased high-quality audio-visual segmentation benchmarks. We also propose a new informative sample mining method for audio-visual supervised contrastive learning to leverage discriminative contrastive samples to enforce cross-modal understanding. We show empirical results that demonstrate the effectiveness of our benchmark. Furthermore, experiments conducted on existing AVS datasets and on our new benchmark show that our method achieves state-of-the-art (SOTA) segmentation accuracy1. 1. Introduction The human nervous system exhibits multi-modal perception [44], combining input signals from different modalities to improve the detection and classification of multiple stimuli [44]. Such functionality has been emulated by recent papers [1–3, 5, 18, 32, 33] that aim to associate visual objects with their corresponding audio sequences, in a task known as audio-visual correspondence (AVC) [2, 3]. *First two authors contributed equally to this work. 1This work was supported by Australian Research Council through grant FT190100525. Bird Ours TPAVI Image Male Helic. Noise Figure 1. Current AVS datasets [54] tend to assume specific objects as consistent sound sources. Such a bias influences AVS methods, like TPAVI [54] (2nd row), to favour segmenting the presumed sound source, even when replacing the original audio with different sound types such as a person speaking (2nd column), bird chirping (3rd column), or background noise (4th row). Our paper proposes a new cost-effective strategy to build a relatively unbiased audio-visual segmentation benchmark and a supervised contrastive learning method that mines informative samples to better constrain the learning of audio-visual embeddings (last row). A particularly interesting AVC task is the audio-visual segmentation (AVS) [54, 55] that aims to segment all pixels of the sounding visual objects using a fully supervised model. A major challenge in AVS is achieving cross-modal alignment between sound and visual objects [43]. Current datasets poorly establish and evaluate this alignment, leading to undesired system behaviour and less effective evaluation. For instance, the dataset in [54] shows a “commonsense” bias because it assumes that certain objects are always the sound source in some scenarios. Fig. 1 shows an example of a scene from [54] with a bias toward the segmentation of the helicopter, even though other sound sources (e.g., person’s speech or bird’s singing) are plauThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 26497",
        "chosen": "This paper proposes a cost-effective strategy to build unbiased audio-visual segmentation benchmarks and a supervised contrastive learning method for audio-visual segmentation. The method mines informative samples to better constrain audio-visual embeddings. Experiments demonstrate state-of-the-art segmentation accuracy on existing datasets and the new benchmark.",
        "rejected": "Unraveling Instance Associations A Closer Look for Audio-Visual Segmentation. This work investigates the challenge of associating instances in audio-visual scenes. It presents an innovative method that enhances segmentation accuracy by leveraging multimodal information, achieving state-of-the-art results."
    },
    {
        "prompt": "summary: Significant progress in image deblurring has been achieved by deep learning methods, especially the remarkable performance of supervised models on paired synthetic data. However, real-world quality degradation is more complex than synthetic datasets, and acquiring paired data in real-world scenarios poses significant challenges. To address these challenges, we propose a novel unsupervised image deblurring framework based on self-enhancement. The framework progressively generates improved pseudosharp and blurry image pairs without the need for real paired datasets, and the generated image pairs with higher qualities can be used to enhance the performance of the reconstructor. To ensure the generated blurry images are closer to the real blurry images, we propose a novel re-degradation principal component consistency loss, which enforces the principal components of the generated low-quality images to be similar to those of re-degraded images from the original sharp ones. Furthermore, we introduce the self-enhancement strategy that significantly improves deblurring performance without increasing the computational complexity of network during inference. Through extensive experiments on multiple real-world blurry datasets, we demonstrate the superiority of our approach over other state-of-the-art unsupervised methods. 1. Introduction Image deblurring is a classical problem in the field of computer vision, which aims to recover a clear image from its blurred version. The image deblurring tasks can be divided into blind and non-blind deblurring, where the blind image deblurring with unknown degradation is more challenging in general. Conventional model-based methods for blind image deblurring typically involve two main steps: estimating the blur kernel, and then reconstructing the sharp image from the blurred input [10, 26, 40, 42]. These methods’ performance is largely constrained by the accuracy of blur kernal *Corresponding author CycleGAN UAUD USR-DA UIDGAN USDF FCLGAN SEMGUD  (Ours) GoPro HIDE RealBlur-J RealBlur-R Figure 1. Performance comparison of our proposed SEMGUD with other unsupervised methods [12, 23, 36, 39, 48, 50] on different datasets. estimation. For instance, in [27], the dark channel prior is used to estimate the blur kernel and reconstruct the sharp image. However, the blurry characteristics in real-world scenarios are quite complex, making it challenging to accurately estimate the optimal blur kernel. In addition, these methods often require complex iterative optimisation processes, which may lead to long inference time. In recent years, with the rapid development of deep learning technology, convolutional neural networks (CNNs) have been widely used in deblurring tasks, achieving significant success. The supervised methods [4–6, 15, 16, 19, 29, 34, 44– 46] focus on training deep neural network models using a large number of paired sharp and blurry images. This enables the network to learn the mapping from blurry images to sharp images without the need for blur kernel estimation, achieving end-to-end reconstruction of the blurry and sharp images. For example, DeepDeblur [24] proposes a multi-scale CNN to implement a coarse-to-fine processing pipeline and directly restores sharp images. However, in the real world cases, for the supervised learning methods, collecting paired datasets from the real world is challenging, and manually synthesized datasets are difficult to simulate the complex real image degradation processes. Compared to supervised deep learning methods, unsupervised deep learning methods [3, 25, 49, 50] for real world This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25691",
        "chosen": "This paper presents an unsupervised image deblurring framework based on self-enhancement. The framework generates improved pseudo-sharp and blurry image pairs progressively. A re-degradation principal component consistency loss ensures generated blurry images resemble real ones. The self-enhancement strategy improves deblurring performance without added inference complexity.",
        "rejected": "Unsupervised Blind Image Deblurring Based on Self-Enhancement. The paper addresses the problem of blind image deblurring using unsupervised learning methods. It introduces a self-enhancement technique that iteratively refines the image, leading to clearer outcomes without prior knowledge of blur kernels."
    },
    {
        "prompt": "summary: Phase unwrapping (PU) is a technique to reconstruct original phase images from their noisy wrapped counterparts, finding many applications in scientific imaging. Although supervised learning has shown promise in PU, its utility is limited in ground-truth (GT) scarce scenarios. This paper presents an unsupervised learning approach that eliminates the need for GTs during end-to-end training. Our approach leverages the insight that both the gradients and wrapped gradients of wrapped phases serve as noisy labels for GT phase gradients, along with sparse outliers induced by the wrapping operation. A recorruption-based self-reconstruction loss in the gradient domain is proposed to mitigate the adverse effects of label noise, complemented with a self-distillation loss for improved generalization. Additionally, by unfolding a variational model of PU that utilizes wrapped gradients of wrapped phases for its datafitting term, we develop a deep unrolling network that encodes physics of phase wrapping and incorporates special treatments on outliers. In the experiments on three types of phase data, our approach outperforms existing GT-free methods and competes well against the supervised ones. 1. Introduction In many imaging systems, the direct acquisition of original phases of a target signal is both challenging and costly. Typically, initial measurement yields a phase image wrapped in [−π, π). Let X ∈RM×N and Y ∈[−π, π)M×N denote the true phase image and its noisy wrapped counterpart, respectively. The wrapping process can be formulated as: Y = W(X + N), W : RM×N →[−π, π)M×N, (1) *Corresponding author: Yuhui Quan. This work is supported by National Natural Science Foundation of China (Grant No. 62372186), Natural Science Foundation of Guangdong Province (Grant No. 2022A1515011755, 2023A1515012841), Fundamental Research Funds for Central Universities (Grant No. x2jsD2230220), and Singapore MOE AcRF Tier 1 (Grant No. A-8000981-00-00). where W denotes the wrapping operator with its entry-wise operation defined as W(θ) = \u0000(θ + π) mod 2π \u0001 −π for θ ∈R, and N ∈RM×N denotes the measurement noise, typically assumed to be Gaussian [22]. Consequently, the original phase image, which exhibits strong smoothness, is degraded to a wrapped version with both incorrect values and artificial discontinuities induced by the modulo operation, which adversely affects subsequent processing. Phase unwrapping (PU) is about recovering the original unwrapped phase image X (up to an additive constant) from its noisy wrapped counterpart Y . Serving as a critical step in many optical imaging techniques for generating clear and coherent phase images, PU finds extensive applications in diverse domains such as quantitative phase imaging, magnetic resonance imaging, synthetic aperture radar interferometry, 3D depth sensing, phase contrast microscopy, fringe projection, and digital holographic interferometry; see e.g. [10, 14, 16, 23, 41, 47, 56, 57]. The model (1) is also usually formulated as follows: Y = X −2πK + N ′, K ∈ZM×N, (2) where K ∈ZM×N denotes the map of wrap counts, indicating the number of times a phase value has been wrapped around by 2π, and N ′ denotes the noise dependent on both X and N, whose distribution can be complex even when N follows a simple distribution such as the normal distribution. It can be seen that PU is a challenging ill-posed inverse problem involving a continuous variable X and a discrete variable K. Direct solutions by integrating wrapped phase differences are insufficient for ensuring spatial consistency in all directions of the unwrapped phases and may accumulate significant errors due to substantial noise. Limitations of existing works: Conventional approaches for PU include reliability-guided methods (e.g. [3, 9, 11, 23, 36]), filtering-based methods (e.g. [2, 45]), and modeldriven methods (e.g. [1, 12, 15]). These approaches are either sensitive to measurement noise, due to their heuristic rules, or overly simplistic for complex phase structures, due to their handcrafted image priors. In contrast, deep learning This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25182",
        "chosen": "This paper introduces an unsupervised learning approach for phase unwrapping (PU) that doesn't require ground-truth during training. The approach uses gradients and wrapped gradients of wrapped phases as noisy labels, with a self-reconstruction loss and self-distillation loss. It also develops a deep unrolling network encoding physics of phase wrapping. The method outperforms existing GT-free methods and competes with supervised ones.",
        "rejected": "Unsupervised Deep Unrolling Networks for Phase Unwrapping. This paper develops an unsupervised deep unrolling network designed to solve phase unwrapping problems efficiently. The proposed method combines traditional optimization algorithms with deep learning, offering faster computation times and improved accuracy."
    },
    {
        "prompt": "summary: Recent Large Language Models (LLMs) have been enhanced with vision capabilities, enabling them to comprehend images, videos, and interleaved vision-language content. However, the learning methods of these large multimodal models (LMMs) typically treat videos as predetermined clips, rendering them less effective and efficient at handling streaming video inputs. In this paper, we propose a novel Learning-In-Video-Stream (LIVE) framework, which enables temporally aligned, long-context, and realtime dialogue within a continuous video stream. Our LIVE framework comprises comprehensive approaches to achieve video streaming dialogue, encompassing: (1) a training objective designed to perform language modeling for continuous streaming inputs, (2) a data generation scheme that converts offline temporal annotations into a streaming dialogue format, and (3) an optimized inference pipeline to speed up interactive chat in real-world video streams. With BCorresponding Author. our LIVE framework, we develop a simplified model called VideoLLM-online and demonstrate its significant advantages in processing streaming videos. For instance, our VideoLLM-online-7B model can operate at over 10 FPS on an A100 GPU for a 5-minute video clip from Ego4D narration. Moreover, VideoLLM-online also showcases state-of-the-art performance on public offline video benchmarks, such as recognition, captioning, and forecasting. The code, model, data, and demo have been made available at showlab.github.io/videollm-online. 1. Introduction Building the future of always-on contextual AI agent that can promptly answer any human question, digitizing inputs as episodic memories and forecasting future plans given any query in online continuous setting represents a “holy grail” mission in AI research. Powered by advancements in large language models (LLMs) [8, 34, 60, 61, 63, 72], recent large multimodal models (LMMs) have unveiled impressive caThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18407",
        "chosen": "This paper proposes the LIVE framework for temporally aligned, long-context, and real-time dialogue within continuous video streams. LIVE includes a training objective for streaming inputs, a data generation scheme, and an optimized inference pipeline. The VideoLLM-online model demonstrates significant advantages in processing streaming videos and shows state-of-the-art performance on public offline video benchmarks.",
        "rejected": "VideoLLM-online Online Video Large Language Model for Streaming Video. This contribution presents VideoLLM, a large language model tailored for streaming video analysis. Capable of processing online videos in real-time, it showcases remarkable performance in understanding and generating textual descriptions synchronized with video content."
    },
    {
        "prompt": "summary: Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive languageimage pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTaminL significantly outperforms ViT-L by 2.0% ImageNet zeroshot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, openvocabulary detection and segmentation, and large multimodal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B). 1. Introduction The past decades have witnessed significant progress in computer vision, like visual recognition tasks. The advent of AlexNet [53] marked a significant milestone, catalyzing the extensive evolution and dominance of Convolutional text model “a photo of cat” contrastive loss vision model Data Scalability Model Scalability CNN+Transformer Feature Resolution designing benchmark modern  backbones in CLIP ImageNet avg. 38 dataset retrieval 81.8 80.3 67.2 64.7 65.7 VTAB dist. shift 62.5 72.4 70.2 61.8 61.1 LMM (VQAv2) LMM (LLAVA-Bench) 78.9 75.9 66.1 60.6 OV pan. seg  (ADE) OV sem. seg  (A-150) OV detection (OV-LVIS) 27.3 24.6 31.8 35.6 32.5 35.6 ViTamin-L ViT-L/14 ViTamin 38 zero-shot  classification/ retrieval tasks 10 open-vocab dense tasks 12 LMM tasks (e.g., VQA) Figure 1. Practices of designing scalable vision models in the vision-language era. We benchmark modern vision models with various model and data scales under CLIP setting using DataComp-1B [30], leading to findings about data and model scalability, feature resolution, and hybrid architecture, which motivate us to develop ViTamin for VLM. ViTamin-L achieves superior zero-shot performance over ViT-L/14 [60] on ImageNet [86] and average 38 datasets [30], and advances a suite of 22 downstream tasks for Open-Vocabulary (OV) detection [111] and segmentation [124], and Large Multi-modal Model (LMM) tasks [67]. Neural Networks (ConvNets) [8, 32, 37, 38, 46, 55, 72, 73] in computer vision. More recently, with the debut of Vision Transformer [23, 104], a growing number of transformerbased architectures [18, 71, 103, 108, 116, 121] have shown great potential to surpass the prior ConvNet counterparts. The rapid advancement of neural network design in computer vision can be attributed to a combination of factors. Among them, an important factor is the well-established benchmarks, allowing the community to examine the developments in a standardized way. Particularly, ImageNet [86] has become the de facto testing ground for new vision models. It not only sets a standard benchmark for visual recognition, but also serves as a mature pre-training dataset for transferring the network backbone to a variety of downstream tasks (e.g., detection and segmentation) [9, 10, 15, 38, 51, 66, 73, 95, 107, 117, 122, 123]. Recently, the emergence of vision-language models (VLMs) [50, 82] has changed the paradigm by leveraging the pre-training schedule on the extremely large scale noisy This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12954",
        "chosen": "This paper aims to build an evaluation protocol for vision models in the vision-language era under the CLIP framework. It introduces ViTamin, a new vision model tailored for VLMs. ViTamin-L outperforms ViT-L in ImageNet zero-shot accuracy and presents promising results on diverse benchmarks. ViTamin-XL surpasses models with ten times more parameters.",
        "rejected": "ViTamin Designing Scalable Vision Models in the Vision-Language Era. Focusing on scalable vision models, this paper introduces ViTamin, which integrates advancements in vision-language models. ViTamin is designed to handle large-scale visual recognition tasks more effectively, demonstrating its superiority in multiple benchmarks."
    },
    {
        "prompt": "summary: Recent innovations on text-to-3D generation have featured Score Distillation Sampling (SDS), which enables the zero-shot learning of implicit 3D models (NeRF) by directly distilling prior knowledge from 2D diffusion models. However, current SDS-based models still struggle with intricate text prompts and commonly result in distorted 3D models with unrealistic textures or cross-view inconsistency issues. In this work, we introduce a novel Visual Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the visual appearance knowledge in 2D visual prompt to boost text-to-3D generation. Instead of solely supervising SDS with text prompt, VP3D first capitalizes on 2D diffusion model to generate a high-quality image from input text, which subsequently acts as visual prompt to strengthen SDS optimization with explicit visual appearance. Meanwhile, we couple the SDS optimization with additional differentiable reward function that encourages rendering images of 3D models to better visually align with 2D visual prompt and semantically match with text prompt. Through extensive experiments, we show that the 2D Visual Prompt in our VP3D significantly eases the learning of visual appearance of 3D models and thus leads to higher visual fidelity with more detailed textures. It is also appealing in view that when replacing the self-generating visual prompt with a given reference image, VP3D is able to trigger a new task of stylized text-to-3D generation. Our project page is available at https://vp3d-cvpr24.github.io. 1. Introduction Generative Artificial Intelligence (especially for vision content generation) has aroused great attention in computer vision field [5, 6, 20, 26], leading to impressive advancements in text-to-image [30–32] and text-to-video generation [10, 14, 34]. These accomplishments can be attributed to the availability of large-scale image-text and video-text pair data [1, 33] and the emergence of robust diffusion-based generative models [12, 13, 25, 35]. Recently, researchers Text prompt: “A florist is making a bouquet with fresh flowers” (a) Magic3D (b) ProlificDreamer (c) VP3D (Ours) Visual prompt Figure 1. Exisiting text-to-3D generation techniques (e.g., Magic3D [17] and ProlificDreamer [39]) often suffer from degenerated results (e.g., over-saturated appearances or inaccurate geometries). Our VP3D novelly integrates a visual prompt to strength score distillation sampling, leading to better 3D results. have gone beyond text-driven image/video generation, and begun exploring diffusion models for text-driven content creation of 3D assets (e.g., text-to-3D generation). This direction paves a new way for practical 3D content creation and has a great potential impact for numerous applications like virtual reality, gaming and Metaverse. Compared to image generation, text-to-3D generation, however, is more challenging, due to the complexities associated with intricate 3D geometry and appearance (i.e., object shapes and textures). Moreover, the collection and annotation of 3D data are somewhat resourcefully expensive and thus cannot be easily scaled up to billion level as image data. To tackle this issue, a pioneering text-to-3D work (DreamFusion [27]) presents the first attempt of exploiting an off-the-shelf text-to-image diffusion model to generate promising 3D assets in a zero-shot fashion. The key design behind such success is Score Distillation Sampling (SDS), which directly optimizes the implicit 3D model of This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4896",
        "chosen": "This paper introduces VP3D, a Visual Prompt-guided text-to-3D diffusion model. VP3D uses a 2D visual prompt generated from input text to strengthen SDS optimization. It couples SDS with a differentiable reward function for better visual alignment. Experiments show VP3D significantly improves visual fidelity and texture details.",
        "rejected": "VP3D Unleashing 2D Visual Prompt for Text-to-3D Generation. VP3D is introduced as a method to generate 3D models from text prompts using 2D visual cues. This technique enhances the realism and detail of generated 3D objects, making significant contributions to the field of computer graphics."
    },
    {
        "prompt": "summary: Spatio-temporal grounding describes the task of localizing events in space and time, e.g., in video data, based on verbal descriptions only. Models for this task are usually trained with human-annotated sentences and bounding box supervision. This work addresses this task from a multimodal supervision perspective, proposing a framework for spatio-temporal action grounding trained on loose video and subtitle supervision only, without human annotation. To this end, we combine local representation learning, which focuses on leveraging ﬁne-grained spatial information, with a global representation encoding that captures higher-level representations and incorporates both in a joint approach. To evaluate this challenging task in a reallife setting, a new benchmark dataset is proposed, providing dense spatio-temporal grounding annotations in long, untrimmed, multi-action instructional videos for over 5K events. We evaluate the proposed approach and other methods on the proposed and standard downstream tasks, showing that our method improves over current baselines in various settings, including spatial, temporal, and untrimmed multi-action spatio-temporal grounding. 1. Introduction Spatio-temporal grounding (STG) describes the challenging task of locating events in space and time within video data based on text referential expressions. Methods in this ﬁeld usually rely on a combination of spatio-temporal bounding box annotation, together with a human-generated caption, describing the visual content of the bounding box [23, 54], which limits their generalizability beyond the given training scenario. Compared to that, as a second line of work, multimodal self-supervised learning tries to leverage “free” data sources, such as video and automatic speech Task: Spatio-Temporal Grounding - Find the temporal boundary of a  queried action in an untrimmed video and spatially localize the action.  \"Crack egg\" … background background … … … Evaluation Setup: Referential queries - \"Crack egg\", \"Mix egg\", etc. Training Setup:  Unlabeled videos with narrated instructions \"To this we will add  one carrot chopped  into jong jullien …\" \"… you can use the  forks to pull it a bit  appart…\" \"… stir it a bit so it mixes  well…\" background \"Mix egg\" Figure 1. Learning Spatio-temporal grounding in untrimmed videos. In training, we learn from unlabeled videos without human annotation. In evaluation, we perform spatio-temporal grounding using an action description such as “crack egg” as a query. The model needs to localize both the action’s temporal boundary and spatial region in the long untrimmed video. We visualize the heatmap from the annotation points as well as derived bounding boxes. recognition (ASR) captions from large-scale instructional videos to learn representations without human annotation [3, 4, 8, 35, 36]. The resulting models achieve state-ofthe-art performance on zero-shot tasks such as cross-modal video retrieval or classiﬁcation and also for zero-shot temporal action segmentation and detection based on free text queries [8, 28, 42, 47, 66], but usually lack spatial localization abilities. A third line of work focuses on labelfree spatial grounding, e.g. by training on image-caption This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18419",
        "chosen": "This paper addresses spatio-temporal grounding from a multimodal supervision perspective, proposing a framework trained on loose video and subtitle supervision. It combines local and global representation learning. A new benchmark dataset with dense annotations in long instructional videos is proposed. The method improves over baselines in various settings.",
        "rejected": "What When and Where Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action. This study explores self-supervised learning for spatio-temporal grounding in long, untrimmed videos containing multiple actions. It proposes a framework that accurately localizes and recognizes actions within complex scenes without manual annotations."
    },
    {
        "prompt": "summary: Existing methods for synthesizing 3D human gestures from speech have shown promising results, but they do not explicitly model the impact of emotions on the generated gestures. Instead, these methods directly output animations from speech without control over the expressed emotion. To address this limitation, we present AMUSE, an emotional speech-driven body animation model based on latent diffusion. Our observation is that content (i.e., gestures related to speech rhythm and word utterances), emotion, and personal style are separable. To account for this, AMUSE maps the driving audio to three disentangled latent vectors: one for content, one for emotion, and one for personal style. A latent diffusion model, trained to generate gesture motion sequences, is then conditioned on these latent vectors. Once trained, AMUSE synthesizes 3D human gestures directly from speech with control over the expressed emotions and style by combining the content from the driving speech with the emotion and style of another speech sequence. Randomly sampling the noise of the diffusion model further generates variations of the gesture with the same emotional expressivity. Qualitative, quantitative, and perceptual evaluations demonstrate that AMUSE outputs realistic gesture sequences. Compared to the state of the art, the generated gestures are better synchronized with the speech content, and better represent the emotion expressed by the input speech. Our code is available at amuse.is.tue.mpg.de. 1. Introduction Animating 3D bodies from speech has a wide range of applications, such as telepresence in AR/VR, avatar animation in games and movies, and to embody interactive digital assistants. While methods for speech-driven 3D body animation have recently shown great progress [5, 7, 31, 56, 101], existing methods do not adequately address one crucial factor: the impact of emotion from the driving speech signal on the generated gestures. Emotions and their expressions play *Now at Google. ... choose a major that is easy to ﬁnd a good job in the future...  ... choose a major that is easy to ﬁnd a good job in the future...  Gestures from neutral speech input. Gestures from surprise synchronized with neutral speech input. Figure 1. Goal. AMUSE generates realistic emotional 3D body gestures directly from a speech sequence (top). It provides user control over the generated emotion by combining the driving speech sequence with a different emotional audio (bottom). a fundamental role in human communication [29, 35, 65] and have become an important consideration when designing computer systems that interact with humans in a natural manner [78, 79]. They are of central concern when synthesizing human animations for a wide variety of application contexts, such as Socially Interactive Agents [61]. Because of this, speech-driven animation systems must not only align movement with the rhythm of the speech, but should also be capable of generating gestures that are perceived as expressing the suitable emotion. Many factors contribute to the perception of emotion and personal idiosyncrasies, such as facial expressions [19], gaze and eye contact [42], physiological responses [47], tone of voice [87], body language [66], and gestures [39]. When it comes to 3D animation, the most relevant factors This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1942",
        "chosen": "This paper presents AMUSE, an emotional speech-driven body animation model based on latent diffusion. AMUSE disentangles content, emotion, and personal style in latent vectors. It synthesizes 3D human gestures from speech with control over emotion and style. Evaluations show realistic gesture sequences better synchronized with speech content and emotion.",
        "rejected": "Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion. The paper discusses a method for generating realistic 3D body animations driven by emotional speech. Using disentangled latent diffusion models, it achieves nuanced and contextually appropriate animations, significantly advancing the state of the art in non-verbal communication synthesis."
    },
    {
        "prompt": "summary: We introduce a novel approach to single image denoising based on the Blind Spot Denoising principle, which we call MAsked and SHuffled Blind Spot Denoising (MASH). We focus on the case of correlated noise, which often plagues real images. MASH is the result of a careful analysis to determine the relationships between the level of blindness (masking) of the input and the (unknown) noise correlation. Moreover, we introduce a shuffling technique to weaken the local correlation of noise, which in turn yields an additional denoising performance improvement. We evaluate MASH via extensive experiments on real-world noisy image datasets. We demonstrate state-of-the-art results compared to existing self-supervised denoising methods. Website: https://hamadichihaoui.github.io/mash. 1. Introduction The removal of noise from real images, i.e., image denoising, is a fundamental and still open problem in image processing despite having a long history of dedicated research (see [9] for an overview of the classic and recent methods). In classic methods, the primary strategies involve manually designing image priors and optimization techniques to enhance both reconstruction accuracy and speed. In contrast, in the context of deep learning methods, neural networks naturally introduce a very powerful prior for images [24] and provide models that could perform denoising efficiently at inference time. These innate capabilities of neural networks opened the doors to a wide range of methods that could not only learn to denoise image from examples of noisy and clean image pairs, but, even more remarkably, directly from single noisy images [12, 15, 25, 26]. In this work, we push the limits of these advanced methods one step further. We focus on the family of methods called Blind Spot Denoising (BSD)[15], since it provides a powerful and general framework. Moreover, we consider the case where only a single image is used for denoising (i.e., we do not rely on a supporting dataset). As also observed by Wang et al [25], training on a dataset may not generalize well on new data, where the noise distribution is unknown. This is particularly true for real images, where noise is often correlated. In these settings, most modern methods find it challenging to handle non-iid data. In our work, similar to the approach in [21], we explore the more general setting of random masking beyond the single blind spot method introduced in [15]. In our analysis, we uncover valuable connections between the performance of Blind Spot Denoising (BSD) methods trained with various input masking techniques and the degree of noise correlation. Surprisingly, we observe that models trained with a higher masking ratio tend to perform better when dealing with highly correlated noise, whereas models trained with a lower masking ratio excel in denoising tasks with iid noise. This discovery offers two key contributions: 1) it provides a method to estimate the unknown level of noise correlation, and 2) it offers a strategy for achieving enhanced denoising performance. Furthermore, our analysis reveals that noise correlation significantly hampers the denoising capabilities of BSD models. This suggests that a more radical approach would be to directly eliminate the correlation in the input data. An intuitive method to achieve this would involve randomly permuting all pixels that correspond to the same clean-image color intensity. However, this presents a classic chicken and egg dilemma, as we would typically need the clean image to perform the permutation, yet the clean image is precisely what we are trying to restore. To tackle this challenge, we utilize an intermediate denoised image as a pseudo-clean image to define the permutation set. Furthermore, given that adjacent pixels are likely to have similar color intensities, we focus on shuffling only pixels within small neighborhoods. We incorporate these insights into a novel method called MAsked and SHuffled Blind Spot Denoising (MASH), which we elaborate on further in Sec. 3. Our contributions are summarized as follows • We provide an analysis of BSD, showcasing the impact of various masking ratios on correlated noise and presenting a method for estimating the noise correlation level; • We introduce MASH, an enhanced version of BSD that This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 3025",
        "chosen": "This paper introduces MASH, a novel approach to single image denoising based on Blind Spot Denoising principles. MASH focuses on correlated noise in real images and introduces a shuffling technique to weaken local noise correlation. Extensive experiments demonstrate state-of-the-art results compared to existing self-supervised denoising methods.",
        "rejected": "We introduce MASH, a novel Blind Spot Denoising technique that handles correlated noise in images by utilizing masked and shuffled input data. This method improves denoising performance on real-world noisy images compared to existing self-supervised methods."
    },
    {
        "prompt": "summary: Referring Image Segmentation (RIS) is a challenging task that requires an algorithm to segment objects referred by free-form language expressions. Despite significant progress in recent years, most state-of-the-art (SOTA) methods still suffer from considerable language-image modality gap at the pixel and word level. These methods generally 1) rely on sentence-level language features for languageimage alignment and 2) lack explicit training supervision for fine-grained visual grounding. Consequently, they exhibit weak object-level correspondence between visual and language features. Without well-grounded features, prior methods struggle to understand complex expressions that require strong reasoning over relationships among multiple objects, especially when dealing with rarely used or ambiguous clauses. To tackle this challenge, we introduce a novel Mask Grounding auxiliary task that significantly improves visual grounding within language features, by explicitly teaching the model to learn fine-grained correspondence between masked textual tokens and their matching visual objects. Mask Grounding can be directly used on prior RIS methods and consistently bring improvements. Furthermore, to holistically address the modality gap, we also design a cross-modal alignment loss and an accompanying alignment module. These additions work synergistically with Mask Grounding. With all these techniques, our comprehensive approach culminates in MagNet (Maskgrounded Network), an architecture that significantly outperforms prior arts on three key benchmarks (RefCOCO, RefCOCO+ and G-Ref), demonstrating our method’s effectiveness in addressing current limitations of RIS algorithms. Our code and pre-trained weights will be released. 1. Introduction Deep learning has greatly improved the performance of vision algorithms on many image segmentation tasks, such as semantic segmentation [5, 48], instance segmentation [2, 12, 24, 42] and panoptic segmentation [8, 36]. These † Project lead. B Corresponding author. diving man diving man (b) Fine-grained visual grounding is required to understand expressions used in uncommon or ambiguous contexts. Correct Wrong diving man Correct third remote from the left (a) Fine-grained visual grounding is required to reason over complicated relationships among multiple objects. third remote from the left third remote from the left Correct Wrong Wrong Figure 1. Importance of Fine-grained Visual Grounding for RIS. Most RIS algorithms lack well-grounded text features. As a result, they struggle in difficult cases illustrated in (a) and (b). Red mask are predictions of LAVT, one of the recent SOTA RIS methods. Yellow dotted boxes are the ground truths. tasks require grouping of image pixels under a fixed set of pre-defined categories and mainly differ in the granularity of grouping semantics required. In contrast to these unimodal segmentation tasks, Referring Image Segmentation (RIS) [9, 28] is a challenging multi-modal task that requires an algorithm to simultaneously understand fine-grained human language expression and make correct pixel-level correspondence to the referred object. Recently, it has gained widespread research attention due to its potential to improve human-robot interaction [1], interactive image editing [43, 52] and advanced driver-assistance systems [29]. The key challenge in RIS lies in how to reduce the modality gap between language and image features [14, 64, 71]. To tackle this challenge, we need to have an effective alignment between a given language expression and the corresponding image pixels for highlighting the referred target. Ideally, with robust pixel-wise language-image alignment, language and image features should have high feature similarity when referring to the same object and low feature simThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 26573",
        "chosen": "This paper presents MagNet, a mask-grounded network for referring image segmentation (RIS). The method introduces a Mask Grounding auxiliary task to improve visual grounding within language features and designs a cross-modal alignment loss and alignment module. MagNet significantly outperforms prior arts on key benchmarks.",
        "rejected": "This paper presents MagNet, an advanced referring image segmentation network incorporating Mask Grounding and cross-modal alignment loss to address language-image modality gaps, enhancing object-level correspondence between visual and language features for improved segmentation accuracy."
    },
    {
        "prompt": "summary: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer the pixel-wise knowledge from the labeled source domain to the unlabeled target domain. However, current UDA methods typically assume a shared label space between source and target, limiting their applicability in real-world scenarios where novel categories may emerge in the target domain. In this paper, we introduce Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) for the first time, where the target domain includes unknown classes. We identify two major problems in the OSDA-SS scenario as follows: 1) the existing UDA methods struggle to predict the exact boundary of the unknown classes, and 2) they fail to accurately predict the shape of the unknown classes. To address these issues, we propose Boundary and Unknown Shape-Aware openset domain adaptation, coined BUS. Our BUS can accurately discern the boundaries between known and unknown classes in a contrastive manner using a novel dilationerosion-based contrastive loss. In addition, we propose OpenReMix, a new domain mixing augmentation method that guides our model to effectively learn domain and sizeinvariant features for improving the shape detection of the known and unknown classes. Through extensive experiments, we demonstrate that our proposed BUS effectively detects unknown classes in the challenging OSDA-SS scenario compared to the previous methods by a large margin. The code is available at https://github.com/KHUAGI/BUS. 1. Introduction In semantic segmentation, a model predicts pixel-wise category labels given an input image. Semantic segmentation has a lot of applications, e.g., autonomous driving [1], human-machine interaction [2], and augmented re*Equal contribution †Corresponding authors (a) Ground truth. (b) UDA method (MIC [12]). (c) Confidence-based threshold. (d) Unknown head-expansion. (e) BUS (Ours). Figure 1. Visualization of prediction maps in the OSDA-SS scenario. The pixels detected by the white color mean the unknown classes. The naive UDA method (b) is completely unaware of the unknown classes. Even after applying simple techniques to help the UDA model recognize the unknown, it still struggles to accurately predict the shape of the unknown, as shown in (c) and (d). ality. Over the past decade, there has been notable advancement in supervised semantic segmentation driven by deep neural networks [3–6]. However, supervised semantic segmentation requires pixel-level annotations, which are labor-intensive and costly to collect. To mitigate the challenges, unsupervised domain adaptation (UDA) has emerged. Many studies [7–12] leverage the already-labeled source data to achieve high performance on the unlabeled target data. Notably, synthetic datasets such as GTA5 [13] and SYNTHIA [14] which are automatically generated by game engines present valuable resources for UDA research. UDA methods typically presume that source and target domains share the same label space. Such an assumption is not reasonable in real-world applications. In the target data, novel categories not presented in the source dataset (target-private categories) may emerge, leading to an OpenThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 23943",
        "chosen": "This paper addresses open-set domain adaptation for semantic segmentation, where the target domain includes unknown classes. The proposed BUS method accurately discerns boundaries between known and unknown classes and improves shape detection. The method demonstrates effective unknown class detection in challenging scenarios.",
        "rejected": "The authors propose BUS, an open-set domain adaptation method for semantic segmentation capable of detecting unknown classes by using dilation-erosion-based contrastive loss and OpenReMix augmentation, significantly outperforming previous methods in identifying boundaries and shapes of unknown classes."
    },
    {
        "prompt": "summary: This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework, where the input and output of the system are multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key advantages can be brought: 1) We can perform real-like conversations with individuals worldwide in a virtual meeting by utilizing our own primary languages. In contrast to Speech-to-Speech Translation (A2A), which solely translates between audio modalities, the proposed AV2AV directly translates between audio-visual speech. This capability enhances the dialogue experience by presenting synchronized lip movements along with the translated speech. 2) We can improve the robustness of the spoken language translation system. By employing the complementary information of audio-visual speech, the system can effectively translate spoken language even in the presence of acoustic noise, showcasing robust performance. To mitigate the problem of the absence of a parallel AV2AV translation dataset, we propose to train our spoken language translation system with the audio-only dataset of A2A. This is done by learning unified audio-visual speech representations through self-supervised learning in advance to train the translation system. Moreover, we propose an AV-Renderer that can generate raw audio and video in parallel. It is designed with zero-shot speaker modeling, thus the speaker in source audio-visual speech can be maintained at the target translated audio-visual speech. The effectiveness of AV2AV is evaluated with extensive experiments in a many-to-many language translation setting. Demo page is available on choijeongsoo.github.io/av2av. ∗Equal contribution. †Corresponding author. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1A2C2005529), Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities), and BK21 FOUR (Connected AI Education & Research Program for Industry and Society Innovation, KAIST EE, No. 4120200113769). It Multilingual  AV2AV Translation … “J'aimerais t'aider” “Ich würde dir gerne helfen” “Me gustaría ayudarte” “Mi piacerebbe aiutarti” De En Pt Es Fr AV Speech Unit  “I would like to help you” “Eu gostaria de ajudar você” … AV Speech Unit  Figure 1. Conceptual illustration of the proposed multilingual Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework. The system can directly translate between multilingual AV speech without requiring any text. Note that the proposed AV2AV can generate both audio speech and visual speech in listener-oriented (i.e., translated) languages. 1. Introduction In our increasingly interconnected world, where communication transcends linguistic boundaries, Neural Machine Translation (NMT) [1–6] has played a critical role in breaking down barriers in multilingual interaction. Despite their strong performances, NMT systems exhibit limitations in seamless application to virtual conferences or face-to-face interactions. This is due to their reliance on human intervention for text input or speech recognition, as these systems primarily operate with text modalities. Speech-to-Speech Translation (A2A1) [7–11] can mitigate this problem by directly translating spoken languages into the target language at the audio level. With the growth of A2A technologies [12], it is anticipated that individuals can effortlessly communicate with one another using their primary languages, irrespective of their nationalities. However, there still exists one unsolved problem in the aspects of multimedia, the discrepancy between the translated speech and the visual stim1Throughout this paper, we employ abbreviations for input and output modalities, using A for Audio, V for Visual, and T for Text. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27325",
        "chosen": "This paper proposes a direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework. The framework translates between multimodal audio-visual speech inputs and outputs, enhancing dialogue experiences with synchronized lip movements and improving translation robustness in noisy environments.",
        "rejected": "This work introduces AV2AV, a direct audio-visual speech translation framework that enhances dialogue experience by presenting synchronized lip movements along with translated speech, improving robustness against acoustic noise through self-supervised learning of unified audio-visual speech representations."
    },
    {
        "prompt": "summary: We address the problem of generalized category discovery (GCD) that aims to partition a partially labeled collection of images; only a small part of the collection is labeled and the total number of target classes is unknown. To address this generalized image clustering problem, we revisit the mean-shift algorithm, i.e., a classic, powerful technique for mode seeking, and incorporate it into a contrastive learning framework. The proposed method, dubbed Contrastive Mean-Shift (CMS) learning, trains an embedding network to produce representations with better clustering properties by an iterative process of mean shift and contrastive update. Experiments demonstrate that our method, both in settings with and without the total number of clusters being known, achieves state-of-the-art performance on six public GCD benchmarks without bells and whistles. 1. Introduction Clustering is one of the most fundamental problems in unsupervised learning, which aims to partition instances of a data collection into different groups [2, 15, 34, 42]. Unlike the classiﬁcation problem, it does not assume either predeﬁned target classes or labeled instances in its standard setup. However, in a practical scenario, some data instances may be labeled for a subset of target classes so that we can leverage them to cluster all the data instances while also discovering the remaining unknown classes. The goal of Generalized Category Discovery (GCD) [48] is to tackle such a semi-supervised image clustering problem given a small amount of incomplete supervision. Clustering is a transductive reasoning process based on the neighborhood data in the given data collection. To learn an image embedding for this clustering purpose, we are motivated to incorporate the neighborhood embeddings into learning. We revisit mean shift [8, 11, 12, 18, 44], i.e., a classic, powerful technique for mode seeking and clustering analysis. The mean-shift algorithm consists of iterative mode-seeking steps of updating each data point by kernel! !! pull push : image embedding \" : mean-shifted embedding # : augmented mean-shifted embedding #! : augmented image embedding \"! !\" : augmented image of ! !! : different images in a batch !\" push Figure 1. Contrastive Mean-Shift (CMS) learning. We integrate mean shift [11] into contrastive learning [7, 24, 59]. In training, image embeddings proceed a single-step mean shift with their kNNs. The contrastive learning objective pulls the mean-shifted embeddings of I and I+, while it pushes those from different image inputs. Colors denote different classes and k=4. weighted aggregation of its neighboring data points; this process is non-parametric and does not require any information about the target clusters, e.g., the number of clusters. For GCD, we develop a GPU-friendly mean-shift variant and incorporate it into a contrastive representation learning framework [7, 24, 59]. We introduce Contrastive Mean-Shift (CMS) learning for GCD. CMS learning aims to encode the semantic distance between image embeddings on a mean-shifted space via contrastive learning. Precisely, we perform a single-step mean shift for each image embedding by moving it toward the mean of its neighbors in an embedding space. To perform stable mean shifts in the embedding space, we use k nearest neighbors (kNNs) instead of typical distance-based neighbors [8, 11]. In learning, an image I and its augmented image I0 are both mean-shifted and pull each other on the embedding space while pushing I and different images (Fig. 1). The training objective with mean-shifted em1 This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 23094",
        "chosen": "This paper introduces Contrastive Mean-Shift (CMS) learning for generalized category discovery (GCD). The method integrates mean shift into a contrastive learning framework, achieving state-of-the-art performance on GCD benchmarks.",
        "rejected": "The paper discusses Contrastive Mean-Shift (CMS) learning, a method for generalized category discovery that integrates mean shift into a contrastive learning framework, achieving state-of-the-art performance on multiple benchmarks without requiring total cluster number information."
    },
    {
        "prompt": "summary: Photometric stereo leverages variations in illumination conditions to reconstruct surface normals. Display photometric stereo, which employs a conventional monitor as an illumination source, has the potential to overcome limitations often encountered in bulky and difficult-to-use conventional setups. In this paper, we present differentiable display photometric stereo (DDPS), addressing an often overlooked challenge in display photometric stereo: the design of display patterns. Departing from using heuristic display patterns, DDPS learns the display patterns that yield accurate normal reconstruction for a target system in an endto-end manner. To this end, we propose a differentiable framework that couples basis-illumination image formation with analytic photometric-stereo reconstruction. The differentiable framework facilitates the effective learning of display patterns via auto-differentiation. Also, for training supervision, we propose to use 3D printing for creating a real-world training dataset, enabling accurate reconstruction on the target real-world setup. Finally, we exploit that conventional LCD monitors emit polarized light, which allows for the optical separation of diffuse and specular reflections when combined with a polarization camera, leading to accurate normal reconstruction. Extensive evaluation of DDPS shows improved normal-reconstruction accuracy compared to heuristic patterns and demonstrates compelling properties such as robustness to pattern initialization, calibration errors, and simplifications in image formation and reconstruction. 1. Introduction Reconstructing high-quality surface normals is pivotal in computer vision and graphics for 3D reconstruction [32, 40], relighting [36, 39], and inverse rendering [45, 52]. Among various techniques, photometric stereo [50] leverages the intensity variation of a scene point under varied illumination conditions to reconstruct normals. Photometric stereo finds its application in various imaging systems including light stages [29, 35, 49, 56], handheld-flash cameras [3, 10, 37, 52], and display-camera systems [1, 28, 46]. Display photometric stereo uses monitors and cameras as a versatile and accessible system that can be conveniently placed on a desk [1, 28, 46]. Producing diverse illumination conditions can be simply achieved by displaying multiple patterns using pixels on the display as programmable point light sources. This convenient and intricate modulation of illumination conditions significantly enlarges the design space of illumination patterns for display photometric stereo. Nevertheless, existing approaches often rely on heuristic display patterns, resulting in sub-optimal reconstruction quality. In this paper, to exploit the large design space of illumination patterns in display photometric stereo, we propose differentiable display photometric stereo (DDPS). The key idea is to learn display patterns that lead to improved reconstruction of surface normals for a target system in an endto-end manner. To this end, we introduce a differentiable framework that combines basis-illumination image formation and an optimization-based photometric stereo method. This enables effective pattern learning by directly optimizThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11831",
        "chosen": "This paper presents differentiable display photometric stereo (DDPS), which learns display patterns for accurate normal reconstruction in an end-to-end manner. The framework couples basis-illumination image formation with analytic photometric-stereo reconstruction, showing improved normal-reconstruction accuracy.",
        "rejected": "DDPS is presented as a differentiable display photometric stereo framework that learns optimal illumination patterns via auto-differentiation for accurate surface normal reconstruction, demonstrating superior reconstruction accuracy and robustness against various errors."
    },
    {
        "prompt": "summary: This paper presents a new approach for the detection of fake videos, based on the analysis of style latent vectors and their abnormal behavior in temporal changes in the generated videos. We discovered that the generated facial videos suffer from the temporal distinctiveness in the temporal changes of style latent vectors, which are inevitable during the generation of temporally stable videos with various facial expressions and geometric transformations. Our framework utilizes the StyleGRU module, trained by contrastive learning, to represent the dynamic properties of style latent vectors. Additionally, we introduce a style attention module that integrates StyleGRU-generated features with contentbased features, enabling the detection of visual and temporal artifacts. We demonstrate our approach across various benchmark scenarios in deepfake detection, showing its superiority in cross-dataset and cross-manipulation scenarios. Through further analysis, we also validate the importance of using temporal changes of style latent vectors to improve the generality of deepfake video detection. 1. Introduction Recent generative algorithms are capable of producing high-quality videos; while this advancement has led to social concerns, as it becomes increasingly challenging to distinguish between generated videos and authentic ones. Generative models have the potential to expedite industries such as entertainment, gaming, fashion, design, and education. However, their misuse can have adverse effects on society. The high quality of the videos intensifies the potential for inappropriate utilization of the technique. To resolve the issue, researchers are actively engaged in developing the deepfake video detection algorithms [19, 29, 40]. Early deepfake detection research addressed spatial artifacts such as unnatural aspects [30] and frequency-level checkerboard of the generative model [33] in the genera*Corresponding author Style latent level Style latent variance DFD-real DFD-fake Youtube-real       Celeb-Synthesis 1    2     3     4    5     6    7     8     9   10   11   12   13  14   15  16   17   18 1    2     3     4   5     6   7     8     9   10   11   12   13  14   15  16   17   18 0.4 0.2 0.0 0.0 0.5 1.0 1.5 Figure 1. Variance of style flow for each style latent level. The xaxis shows the level of style latent vectors for fine style representations. We noticed that the level-wise differences vary across deepfake domains, but the variance of style latent vectors is particularly lower in certain levels of the style latent vectors for fake videos than in real videos. This happens due to the temporal smoothness of the style latent vectors to create temporally stable deepfake videos, and our results demonstrate that deepfake videos have a distinct variance in style flow compared to real videos. tion of a single frame. While spatial artifact-based detection methods have shown reasonable performance for single images, they failed in accounting for temporal artifacts in deepfake videos that consist of multiple frames. To address the limitation, recent studies [15, 41] integrate temporal cues such as flickering [14, 57] and discontinuity [16] to enhance the accuracy of deepfake video detection task. Nevertheless, existing methods [28, 51] that exploit visual and temporal cues of generated videos encountered performance degradation when attempting to identify videos produced by recent deepfake generation algorithms, which effectively suppress visual and temporal artifacts. Recent observations [36, 39] have presented a decline in the effectiveness of visual artifact-based algorithms against newly developed high-quality generation techniques [11, 37]. Temporal artifact-based algorithms [16] also experiThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1133",
        "chosen": "This paper proposes a new approach for fake video detection by analyzing style latent vectors and their temporal changes. The framework utilizes a StyleGRU module and a style attention module to detect visual and temporal artifacts, demonstrating superiority in cross-dataset and cross-manipulation scenarios.",
        "rejected": "A new approach for fake video detection is introduced, focusing on the analysis of style latent vectors' abnormal behavior in temporal changes. Utilizing StyleGRU and style attention modules, this method shows superiority in cross-dataset and cross-manipulation scenarios."
    },
    {
        "prompt": "summary: Advancements in neural signed distance fields (SDFs) have enabled modeling 3D surface geometry from a set of 2D images of real-world scenes. Baking neural SDFs can extract explicit mesh with appearance baked into texture maps as neural features. The baked meshes still have a large memory footprint and require a powerful GPU for real-time rendering. Neural optimization of such large meshes with differentiable rendering pose significant challenges. We propose a method to produce optimized meshes for large unbounded scenes with low triangle budget and high fidelity of geometry and appearance. We achieve this by combining advancements in baking neural SDFs with classical mesh simplification techniques and proposing a joint appearance-geometry refinement step. The visual quality is comparable to or better than state-of-the-art neural meshing and baking methods with high geometric accuracy despite significant reduction in triangle count, making the produced meshes efficient for storage, transmission, and rendering on mobile hardware. We validate the effectiveness of the proposed method on large unbounded scenes from mip-NeRF 360, Tanks & Temples, and Deep Blending datasets, achieving at-par rendering quality with 73× reduced triangles and 11× reduction in memory footprint. 1. Introduction Photorealistic reconstruction and rendering of real-world objects and scenes is a longstanding problem of importance with several applications in computer vision, robotics and AR/VR. In recent years, Neural Radiance Fields (NeRFs) [1, 2, 38, 39] have been quite successful at novel view synthesis. However, the scene representation is learned as an implicit volumetric representation that is expensive to evaluate. This is a crucial limitation for many applications, such as Augmented and Virtual Reality (AR/VR), often requiring real-time rendering with low memory and compute power. Figure 1. Example results using LTM showing mesh geometry (top-left) and novel-view synthesis (right). Bottom right plot compares performance of different methods on mip-NeRF360 outdoor dataset. Our method achieves comparable rendering quality to BakedSDF while reducing the mesh size considerably. Since modern graphics engines are highly optimized to rasterize triangle meshes, researchers have started exploring ways to extract explicit mesh representations from volumetric functions. MobileNeRF [5] uses a polygon mesh with texture maps storing feature vectors and opacity. Although their rendering quality is good, the mesh quality is far from an ideal surface. While NeRF2Mesh [50] and NeRFMeshing [44], based on volumetric density fields, excel at generating highly detailed geometry, they often result in noisy and bumpy surfaces and numerous floaters, especially in planar regions, which are common in large-scale scenes. In contrast, neural Signed Distance Function (SDF) methods [42, 52, 54] typically excel in reconstructing accurate surface geometry. However, they may not be able to capture small or detailed geometry, and their rendering quality is inferior to volumetric density-field methods. BakedSDF [55] overcomes these limitations by applying mip-NeRF 360 techniques to train SDF representation, though the resulting mesh representation has a high memory overhead. Previous neural SDF methods that rely on the Marching Cube algorithm [34] result in meshes with high storage overhead. This results in a significant challenge for practiThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 5053",
        "chosen": "This paper presents LTM, a method for producing optimized meshes for large unbounded scenes with low triangle budget and high fidelity. The method combines baking neural SDFs with classical mesh simplification techniques, achieving significant reduction in triangle count and memory footprint while maintaining visual quality.",
        "rejected": "The research proposes a method for optimizing large unbounded scene meshes derived from neural SDFs with classical mesh simplification techniques, achieving high fidelity in geometry and appearance while significantly reducing triangle count for efficient storage and transmission."
    },
    {
        "prompt": "summary: Omnidirectional cameras are extensively used in various applications to provide a wide field of vision. However, they face a challenge in synthesizing novel views due to the inevitable presence of dynamic objects, including the photographer, in their wide field of view. In this paper, we introduce a new approach called Omnidirectional Local Radiance Fields (OmniLocalRF) that can render staticonly scene views, removing and inpainting dynamic objects simultaneously. Our approach combines the principles of local radiance fields with the bidirectional optimization of omnidirectional rays. Our input is an omnidirectional video, and we evaluate the mutual observations of the entire angle between the previous and current frames. To reduce ghosting artifacts of dynamic objects and inpaint occlusions, we devise a multi-resolution motion mask prediction module. Unlike existing methods that primarily separate dynamic components through the temporal domain, our method uses multi-resolution neural feature planes for precise segmentation, which is more suitable for long 360◦ videos. Our experiments validate that OmniLocalRF outperforms existing methods in both qualitative and quantitative metrics, especially in scenarios with complex realworld scenes. In particular, our approach eliminates the need for manual interaction, such as drawing motion masks by hand and additional pose estimation, making it a highly effective and efficient solution. 1. Introduction Omnidirectional cameras such as Ricoh Theta or Insta360 allow capturing panoramic 360◦views in a single shot. Various applications with omnidirectional images such as spherical depth estimation [53, 58, 59], novel view synthesis [2, 3, 5–7, 11, 18, 31, 35] and geometry reconstruction [3, 18] aiming at large-scale static scenes have recently been explored. In particular, synthesizing 360◦novel views can provide continuous views from unobserved camera angles while maintaining its details. However, recent novel view synthesis methods struggle to apply to omnidirectional input for the following reasons. (b) OmniLocalRF (ours)  (a) Conventional neural rendering Input 360°video Figure 1. We introduce omnidirectional local radiance fields for photorealistic view synthesis of static scenery from 360◦videos. Our method effectively removes dynamic objects (including the photographer) without manual interaction. Also, it achieves highresolution details in the inpainted regions by means of bidirectional observations of omnidirectional local radiance fields. Refer to the supplemental video for more results. When capturing omnidirectional videos to record static environments, dynamic objects are prone to be captured as an extension of the field of view, and capturing a photographer is inevitable unless employing a dedicated hardware or remote controller. When synthesizing novel views, these captured objects are represented as ghosting artifacts onto the rendered results [37]. Despite these problems, existing methods have achieved 360◦view synthesis, relying on constrained capturing conditions, where it minimizes the advent of dynamic objects [3, 11] or requiring dedicated hardware [5–7, 31, 35], which are not suitable for casual 360◦ photography. Low-rank decomposition through robust principal component analysis [15, 16, 57] and existing optimizationbased methods [13, 14] effectively eliminate dynamic objects on the image domain. However, their applicability is limited to scenarios involving multiple images captured from the same viewpoints. Recent view synthesis This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 6871",
        "chosen": "This paper introduces OmniLocalRF, a method for rendering static-only scene views from omnidirectional videos by removing and inpainting dynamic objects. The approach combines local radiance fields with bidirectional optimization of omnidirectional rays, outperforming existing methods in complex real-world scenes.",
        "rejected": "This paper introduces a novel framework for spatio-temporal action grounding trained solely on video and subtitle supervision, eliminating the need for human annotations. The method combines local representation learning to capture fine-grained spatial information with global representation encoding for higher-level representations. A new benchmark dataset is proposed, featuring dense spatio-temporal grounding annotations in long, untrimmed multi-action instructional videos. Evaluations demonstrate that the proposed approach outperforms current baselines in various settings, including spatial and temporal localization tasks."
    },
    {
        "prompt": "summary: Video Transformers have become the prevalent solution for various video downstream tasks with superior expressive power and flexibility. However, these video transformers suffer from heavy computational costs induced by the massive number of tokens across the entire video frames, which has been the major barrier to train and deploy the model. Further, the patches irrelevant to the main contents, e.g., backgrounds, degrade the generalization performance of models. To tackle these issues, we propose training-free token merging for lightweight video Transformer (vid-TLDR) that aims to enhance the efficiency of video Transformers by merging the background tokens without additional training. For vid-TLDR, we introduce a novel approach to capture the salient regions in videos only with the attention map. Further, we introduce the saliency-aware token merging strat*: Equal contribution, †: Corresponding author. egy by dropping the background tokens and sharpening the object scores. Our experiments show that vid-TLDR significantly mitigates the computational complexity of video Transformers while achieving competitive performance compared to the base model without vid-TLDR. Code is available at https://github.com/mlvlab/vid-TLDR. 1. Introduction With the success of Transformers in computer vision, e.g., classification [14, 52], object detection [10, 32, 43, 61, 75, 77], segmentation [59, 64], a line of works [16, 33, 51, 57, 60, 76] have proposed video Transformers to comprehend the video for various downstream tasks. The attention mechanism in Transformers shows the desirable characteristics for video understanding such as the ability to capture the spatial and temporal dependencies at the same time. Consequently, This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 18771",
        "chosen": "This paper proposes vid-TLDR, a training-free token merging method for lightweight video Transformers. The method enhances efficiency by merging background tokens using a saliency-aware strategy, significantly mitigating computational complexity while maintaining competitive performance.",
        "rejected": "AMUSE is presented as an emotional speech-driven body animation model based on latent diffusion. It addresses the limitation of existing methods by explicitly modeling the impact of emotions on generated gestures. AMUSE maps audio input to three disentangled latent vectors representing content, emotion, and personal style. These vectors condition a latent diffusion model trained to generate gesture motion sequences. AMUSE synthesizes realistic 3D human gestures from speech while allowing control over expressed emotions and style. Comprehensive evaluations show that AMUSE produces more synchronized and emotionally expressive gestures compared to state-of-the-art methods."
    },
    {
        "prompt": "summary: Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Fields (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view image and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic GeoReasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified. 1. Introduction 3D scene understanding plays a pivotal role in many visionrelated tasks, including 3D reconstruction and 3D reasoning. The former domain focuses on low-level understanding, including developing efficient models that interpret and reconstruct observations from various data sources, such as RGB, RGBD, and physical sensors. In contrast, the latter emphasizes high-level understanding, like encoding, segmentation, and common sense knowledge learning. For real-world applications like robotic navigation or augmented reality, both these facets - reconstruction and reasoning - are crucial to enhance interactions with the real world [10]. Among the challenges in the reconstruction domain, novel view synthesis has consistently been a challenging objective. Being able to generate a novel view image based on a few existing views shows that a model com*Equal Contribution prehends the scene’s geometry and possesses a foundational understanding akin to reconstruction. The Neural Radiance Field (NeRF) [1, 4, 8, 19–22, 28, 35] has recently risen as an exciting research area, providing a novel way to tackle the task of novel view synthesis. By encoding the density and emitted radiance at each spatial location, NeRF is able to compress a scene into a learnable model given several images and the corresponding camera poses of the scene. By incorporating a volumetric rendering skill, images of unseen camera views can be generated with convincing quality. However, NeRF primarily focuses on reconstructing the color information of novel views and understanding the associated high-level semantic information (e.g., semantic segmentation or object detection) remains a significant challenge. Recent works [9, 15, 25, 30, 31, 37] aim to amplify the high-level scene understanding ability of NeRF by integrating NeRF with semantic segmentation. By sharing the information between the semantic object class and their corresponding appearances, these two tasks are capable of benefiting from each other’s insights [17]. Nevertheless, these semantic understanding methods stick closely to the original NeRF paradigm, focusing on exploiting the model to represent a specific scene. As mentioned in [17], such a strategy requires additional annotations of semantic segmentation maps when applied to a new scene, limiting realworld applicability and generalizability. To tackle this problem, generalizable NeRFs [3, 14, 18, 27, 32, 33, 36] have emerged as a promising solution. They adopt an on-the-fly approach for building a neural radiance field conditioned on extracted features from input images of different scenes rather than encoding the scene representation directly into the model. For instance, PixelNeRF [36] introduces the idea of conditioning a NeRF model with multi-view images across multiple scenes. This innovation boosts NeRF’s ability to generalize to unseen scenes and avoids retraining for each individual scene. Yet, it leaves uncertainties in its application to semantic understanding. As a pioneer in integrating a generalizable NeRF with semantic segmentation capabilities, Semantic-Ray (SThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 20806",
        "chosen": "This paper introduces GSNeRF, a generalizable semantic neural radiance field that produces novel-view images and associated semantic maps for unseen scenes. GSNeRF consists of Semantic Geo-Reasoning and Depth-Guided Visual Rendering stages. Experiments confirm GSNeRF's effectiveness in novel-view synthesis and semantic segmentation.",
        "rejected": "Generalizable Semantic Neural Radiance Fields (GSNeRF) integrates semantic information into novel view synthesis, enabling both image and semantic map generation. It uses a two-stage process: Semantic Geo-Reasoning extracts features from multi-view images, while Depth-Guided Visual rendering generates improved images and semantic renderings. GSNeRF demonstrates superior performance in novel view synthesis and semantic segmentation tasks compared to prior methods."
    },
    {
        "prompt": "summary: Deep Neural Networks (DNNs) are widely used for visual classiﬁcation tasks, but their complex computation process and black-box nature hinder decision transparency and interpretability. Class activation maps (CAMs) and recent variants provide ways to visually explain the DNN decisionmaking process by displaying ‘attention’ heatmaps of the DNNs. Nevertheless, the CAM explanation only offers relative attention information, that is, on an attention heatmap, we can interpret which image region is more or less important than the others. However, these regions cannot be meaningfully compared across classes, and the contribution of each region to the model’s class prediction is not revealed. To address these challenges that ultimately lead to better DNN Interpretation, in this paper, we propose CAPE, a novel reformulation of CAM that provides a uniﬁed and probabilistically meaningful assessment of the contributions of image regions. We quantitatively and qualitatively compare CAPE with state-of-the-art CAM methods on CUB and ImageNet benchmark datasets to demonstrate enhanced interpretability. We also test on a cytology imaging dataset depicting a challenging Chronic Myelomonocytic Leukemia (CMML) diagnosis problem. Code is available at: https://github.com/AIML-MED/CAPE. 1. Introduction Deep neural networks (DNNs), despite achieving superior performance on various tasks such as computer vision and natural language processing, are known to be black boxes [23] that lack the ability to explain their decisionmaking process. The black-box nature is commonly regarded as a result of the complex model structure characterized by stacked computation layers, involving non-linear functions and many model parameters. Explainable DNN decisions are crucial to many life-critical scenarios [26] †Corresponding author. such as AI-powered autonomous driving and medical diagnostics. Taking the example of healthcare applications [2], decision transparency is critical for doctors to understand and trust AI analysis, and to use AI to make insightful and accurate diagnoses or decisions. DNN interpretability is an emerging and actively studied research ﬁeld. For visual classiﬁcation tasks, a common type of DNN interpretability analysis is to explain DNN outputs via ﬁnding and displaying model attention on the input image, i.e., identifying which image regions the model focused on during the decision-making process. This type of visual explanation can be achieved via methods of gradient-based attention visualization [25], perturbationbased input manipulation [6, 21], and class activation map (CAM)-based visualization [11, 24]. In particular, CAM is an inherent intermediate step of DNN prediction which represents the actual region relevance produced by the network. CAM stands out due to its efﬁcient feedforward process, yet its attention values can not directly explain and compose model outcomes. Speciﬁcally, CAM values are class-wise relative probabilities. They only represent the relative region importance compared to the highest attention value within each class map. Thus, CAM values provide a limited explanation within the context of one target class. This means that they are incomparable between classes, and cannot explain image-level predictions. Take the CAM visualization in Fig. 1 as an example, CAM assigns similar attention values to two dog breed classes Siberian Husky and Alaskan Malamute. Differencing the two CAM maps between the breeds fails to yield meaningful comparisons. The limited analytical capability of current CAM-based approaches hinders their use in many downstream applications. For example, ﬁne-grained classiﬁcation analysis requires the model’s ability to discriminate regions between closely related concepts. In addition, for tasks such as weakly supervised semantic segmentation, CAM thresholding is employed to initialize a segmentation method [13] but the threshold choice is often arbitrary without any semantic meaning attached. In this paper, we reformulate CAM as a Probabilistic This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11072",
        "chosen": "This paper proposes CAPE, a reformulation of class activation maps (CAM) that provides a unified and probabilistically meaningful assessment of image region contributions. CAPE enhances DNN interpretation by revealing region contributions to class predictions, demonstrated on benchmark datasets and a medical imaging task.",
        "rejected": "CAPE reformulates Class Activation Maps (CAM) to offer a unified and probabilistically meaningful assessment of image region contributions. Unlike traditional CAMs that provide relative attention values, CAPE enables cross-class comparisons and reveals each region's contribution to class prediction. Experiments on CUB, ImageNet, and a cytology dataset demonstrate enhanced interpretability and diagnostic accuracy, making DNN decisions more transparent."
    },
    {
        "prompt": "summary: Music is a universal language that can communicate emotions and feelings. It forms an essential part of the whole spectrum of creative media, ranging from movies to social media posts. Machine learning models that can synthesize music are predominantly conditioned on textual descriptions of it. Inspired by how musicians compose music not just from a movie script, but also through visualizations, we propose MELFUSION, a model that can effectively use cues from a textual description and the corresponding image to synthesize music. MELFUSION is a text-to-music diffusion model with a novel “visual synapse”, which effectively infuses the semantics from the visual modality into the generated music. To facilitate research in this area, we introduce a new dataset MeLBench, and propose a new evaluation metric IMSM. Our exhaustive experimental evaluation suggests that adding visual information to the music synthesis pipeline significantly improves the quality of generated music, measured both objectively and subjectively, with a relative gain of up to 67.98% on the FAD score. We hope that our work will gather attention to this pragmatic, yet relatively under-explored research area. 1. Introduction Music is an essential tool for creative professionals and content creators. It can complement and set the mood for an accompanying still image, animation, video, or even text descriptions while creating a social media post. Finding music that matches a specific setting, can indeed be an arduous task. A conditional music generation approach, that can synthesize a music track by analyzing the visual content and the textual description can find a wide range of practical ∗Equal contribution. †Work done during internship at Adobe Research. MELFUSION (Ours) Long Caption A tranquil scenery is captured with the view of the night sky just before sunrise. The sky consists of dynamic spiralling clouds which symbolizes movement and aliveness. The stars are bright and prominent with strokes of yellows and whites represent a vivid yet peaceful moment. The village in the scene has houses whose windows emit warm and glowing light, giving a contrast to the cool, celestial tones of sky depicting pleasant emotions. OVL: 83.86 Caption Generator A soft musical track of folk acoustic genre played on violin. OVL: 88.45 We introduce a new problem setting and an approach to synthesize music, conditioned on both visual and textual modalities. A soft musical track of folk acoustic genre played on violin. An Alternate Approach Text-to-Music Figure 1. We present MELFUSION, a music diffusion model equipped with a novel “visual synapse”, that can effectively infuse image semantics into a text-to-music diffusion model. This task indeed requires a detailed understanding of the concepts in the image. An alternate approach like using a caption generator to convert image to text space to be further used with existing text-tomusic methods leads to a sub-optimal overall audio quality (OVL) score. Our approach can knit together complementary information from both modalities to synthesize high-quality music. applications in various fields including social media. Inspired by the progress in generative modeling of images, music generation has also garnered significant attention from the community [1, 30, 42]. Recently, Agostinelli et al. [1], Copet et al. [4] proposed conditioning in the form of melody or humming. While Sheffer and Adi [43] pursue image-guided audio generation. Despite these efforts, music generation conditioned on multiple modalities like text and image, is largely uncharted. Images are more expressive [13] than text-only information and capture more fine-grained semantic information about various visual aspects. For example, as depicted in Fig 1, to generate a musical track that goes well with a given image, without indeed using it, one has to make the tedious effort of producing long, descriptive captions (either generated by an image captioning model or human annotaThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 26826",
        "chosen": "This paper presents MELFUSION, a text-to-music diffusion model with a \"visual synapse\" that infuses visual semantics into music synthesis. The model improves music quality using visual and textual cues, validated on a new dataset and evaluation metric.",
        "rejected": "MELFUSION is a text-to-music diffusion model incorporating visual cues for music synthesis. By integrating a \"visual synapse,\" it effectively blends textual descriptions with corresponding images to produce high-quality music. An evaluation on MeLBench shows significant improvements in generated music quality, achieving up to a 67.98% gain on the FAD score. This approach highlights the potential of multimodal inputs in creative applications."
    },
    {
        "prompt": "summary: Open-vocabulary semantic segmentation presents the challenge of labeling each pixel within an image based on a wide range of text descriptions. In this work, we introduce a novel cost-based approach to adapt vision-language foundation models, notably CLIP, for the intricate task of semantic segmentation. Through aggregating the cosine similarity score, i.e., the cost volume between image and text embeddings, our method potently adapts CLIP for segmenting seen and unseen classes by fine-tuning its encoders, addressing the challenges faced by existing methods in handling unseen classes. Building upon this, we explore methods to effectively aggregate the cost volume considering its multi-modal nature of being established between image and text embeddings. Furthermore, we examine various methods for efficiently fine-tuning CLIP. 1. Introduction Open-vocabulary semantic segmentation aims to assign each pixel in an image to a class label from an unbounded range, defined by text descriptions. To handle the challenge of associating an image with a wide variety of text descriptions, pre-trained vision-language foundation models, e.g., CLIP [43] and ALIGN [22], have drawn attention as they exerted strong open-vocabulary recognition capabilities achieved through training on extensive image-text datasets. Nonetheless, these foundation models primarily receive image-level supervision during training, which introduces a notable disparity when applying them to the pixel-level segmentation tasks [66]. To address this gap, recent works [9, 14, 30, 55–57, 60] have reformulated the task into a region-level problem by utilizing mask proposal generators. While this partially bridges the discrepancy between the pre-training and the ∗Equal contribution. †Corresponding authors. 0 10 20 30 40 mIoU on ADE20K 🔥 ❄ 🔥 ❄ Feature Agg. Cost Agg. (a) mIoU of seen classes 0 10 20 30 40 mIoU on ADE20K 🔥 ❄ 🔥 ❄ Feature Agg. Cost Agg. 🔥: Fine-tuned CLIP ❄: Frozen CLIP (b) mIoU of unseen classes Figure 1. Comparison between feature and cost aggregation for open-vocabulary semantic segmentation task. In contrast to feature aggregation suffering severe overfitting to seen classes, cost aggregation can generalize to unseen classes and achieve significant performance improvements upon fine-tuning of CLIP. downstream task, a discernible gap persists between the conceptualization of regions and the entire image for CLIP. In this work, we investigate methods to transfer the holistic understanding capability of images to the pixel-level task of segmentation. While a straightforward approach would be to fine-tune the encoders of CLIP, existing methods struggle in such attempt [57, 60, 66] as they encounter significant overfitting problems to the seen classes. This results in the misalignment of the joint embedding space for unseen classes, as the CLIP features undergo decoder modules for aggregating them into segmentation masks, hence losing their alignment. Consequently, most methods [9, 14, 30, 55–57, 60] opt for freezing the encoders of CLIP instead, remaining the challenge underexplored. In this regard, we extend the exploration of adapting CLIP for open-vocabulary semantic segmentation and introduce a novel cost-based framework. We propose to aggregate the cosine similarity between image and text embeddings of CLIP, i.e., the matching cost, drawing parallels to the visual correspondence literature [26]. Surprisingly, we find that fine-tuning CLIP upon this framework effecThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4113",
        "chosen": "This paper introduces a cost-based framework for open-vocabulary semantic segmentation, adapting CLIP through cost volume aggregation. The method effectively segments seen and unseen classes, with fine-tuning improving performance on unseen classes.",
        "rejected": "A cost-based approach adapts vision-language models like CLIP for open-vocabulary semantic segmentation by aggregating cosine similarity scores between image and text embeddings. Fine-tuning CLIP encoders improves handling of seen and unseen classes, addressing limitations in existing methods. Cost aggregation proves effective in generalizing to unseen classes, significantly enhancing performance metrics over feature aggregation methods."
    },
    {
        "prompt": "summary: We present a new open-vocabulary detection framework. Our framework uses both image-level labels and detailed detection annotations when available. Our framework proceeds in three steps. We first train a language-conditioned object detector on fully-supervised detection data. This detector gets to see the presence or absence of ground truth classes during training, and conditions prediction on the set of present classes. We use this detector to pseudolabel images with image-level labels. Our detector provides much more accurate pseudo-labels than prior approaches with its conditioning mechanism. Finally, we train an unconditioned open-vocabulary detector on the pseudo-annotated images. The resulting detector, named DECOLA, shows strong zero-shot performance in openvocabulary LVIS benchmark as well as direct zero-shot transfer benchmarks on LVIS, COCO, Object365, and OpenImages. DECOLA outperforms the prior arts by 17.1 APrare and 9.4 mAP on zero-shot LVIS benchmark. DECOLA achieves state-of-the-art results in various model sizes, architectures, and datasets by only training on open-sourced data and academic-scale computing. Code is available at https://github.com/janghyuncho/DECOLA. 1. Introduction Object detection has seen immense progress over the past decade. Classical object detectors reason over datasets of fixed predefined classes. This simplifies the design, training, and evaluation of new methods, and allows for rapid prototyping [2–5, 17, 18, 25, 37, 42, 60, 70, 72, 75]. However, it complicates deployment to downstream applications too. A classical detector requires a new dataset to further finetune for every new concept it encounters. Collecting sufficient data for every new concept is not scalable [20]. Open-vocabulary detection offers an alternative [19, 43, 61, 65, 66, 73]. Open-vocabulary detectors reason about any arbitrary concept with free-form text, using the generalization ability of vision-language models. Yet, common open-vocabulary detectors reuse classical detectors and either replace the last classification layer DECOLA Language-conditioned Detections Image-level tags:  “cat”, “cola”, “mentos” text encoder conditioned Standard  OV Detector image-text dataset of size N filtering & argmax select scores training data with pseudo-labels argmax x N training data with pseudo-labels image-text dataset of size N x N x N x N (a) Standard self-training (b) DECOLA self-training Figure 1. An illustration of how standard open-vocabulary detectors and DECOLA generate pseudo-labels using image-level data. Standard detectors use image-level information later in the pipeline after initial box proposals, which may result in low coverage of unseen classes (e.g., “mentos” and “cola”). DECOLA adjusts the prediction to the information and ensures sufficient coverage. with [19, 43, 66, 73], or fuse box feature with [31, 65] text representation from pretrained vision-language model. The inner workings of the detector remain unchanged. In this paper, we introduce a transformer-based object detector that adjusts its inner workings to any arbitrary set of concepts represented in language. The detector considers only the queried set of concepts as foreground and disregards any other objects as background. It learns to adapt detection to the language embedding of queried concepts at run-time. Specifically, the detector conditions proposal generation with respect to the text embedding of each queried concept and refines the conditioned proposals into predictions. Our detection transformer conditioned on language (DECOLA) offers a powerful alternative to classical architectures in open-vocabulary detection. Adapting the detector to This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 16593",
        "chosen": "This paper presents DECOLA, an open-vocabulary detection framework using language-conditioned object detectors. The framework generates accurate pseudo-labels and trains an unconditioned detector, showing strong zero-shot performance on multiple benchmarks.",
        "rejected": "DECOLA, a language-conditioned object detector, leverages image-level labels and detection annotations to generate accurate pseudo-labels for training. It outperforms previous methods in zero-shot detection benchmarks on LVIS, COCO, Object365, and OpenImages datasets. DECOLA achieves state-of-the-art results across various model sizes and architectures, demonstrating strong generalization capabilities in open-vocabulary settings."
    },
    {
        "prompt": "summary: While GAN-based models have been successful in image stylization tasks, they often struggle with structure preservation while stylizing a wide range of input images. Recently, diffusion models have been adopted for image stylization but still lack the capability to maintain the original quality of input images. Building on this, we propose OSASIS: a novel one-shot stylization method that is robust in structure preservation. We show that OSASIS is able to effectively disentangle the semantics from the structure of an image, allowing it to control the level of content and style implemented to a given input. We apply OSASIS to various experimental settings, including stylization with outof-domain reference images and stylization with text-driven manipulation. Results show that OSASIS outperforms other stylization methods, especially for input images that were rarely encountered during training, providing a promising solution to stylization via diffusion models. The source code can be found at https://github.com/hansam95/OSASIS. *Work done during an internship at NAVER Cloud. †Corresponding author. 1. Introduction In the literature of generative models, image stylization refers to training a model in order to transfer the style of a reference image to various input images during inference [21, 22, 28]. However, collecting a sufficient number of images that share a particular style for training can be difficult. Consequently, one-shot stylization has emerged as a viable and practical solution, with generative adversarial networks (GANs) showing promising results [2, 16, 36, 38, 39]. Despite significant advancements in GAN-based stylization techniques, the accurate preservation of an input image’s structure continues to pose a significant challenge. This difficulty is particularly pronounced for input images that contain elements infrequently encountered during training, often characterized by complex structural nuances that diverge from those observed in more commonly presented images. Figure 1(a) illustrates this challenge, where entities such as hands and microphones, when processed through GAN-based stylization, diverge considerably from their original structural integrity. In addition, GAN-based stylThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 8302",
        "chosen": "This paper proposes OSASIS, a one-shot stylization method using diffusion models. OSASIS disentangles image semantics and structure, offering control over content and style levels. It outperforms other methods, especially on rarely encountered inputs.",
        "rejected": "OSASIS introduces a one-shot stylization method using diffusion models, focusing on robust structure preservation. It disentangles semantics from structure, allowing controlled content and style implementation. OSASIS excels particularly in handling rare input images, offering promising solutions for stylization via diffusion models. The approach surpasses other stylization methods in preserving structural integrity."
    },
    {
        "prompt": "summary: Video Frame Interpolation (VFI), which aims at generating high-frame-rate videos from low-frame-rate inputs, is a highly challenging task. The emergence of bio-inspired sensors known as event cameras, which boast microsecondlevel temporal resolution, has ushered in a transformative era for VFI. Nonetheless, the application of event-based VFI techniques in domains with distinct environments from the training data can be problematic. This is mainly because event camera data distribution can undergo substantial variations based on camera settings and scene conditions, presenting challenges for effective adaptation. In this paper, we propose a test-time adaptation method for eventbased VFI to address the gap between the source and target domains. Our approach enables sequential learning in an online manner on the target domain, which only provides low-frame-rate videos. We present an approach that leverages confident pixels as pseudo ground-truths, enabling stable and accurate online learning from low-frame-rate videos. Furthermore, to prevent overfitting during the continuous online process where the same scene is encountered repeatedly, we propose a method of blending historical samples with current scenes. Extensive experiments validate the effectiveness of our method, both in cross-domain and continuous domain shifting setups. The code is available at https://github.com/Chohoonhee/TTA-EVF. 1. Introduction Video Frame Interpolation (VFI) is a well-established problem in the field of computer vision, aiming to enhance the temporal resolution of videos. In recent times, deep learning-based VFI approaches [2, 3, 15, 20–22, 33, 35, 37, 38, 50, 60] have achieved remarkable performances across various benchmark datasets. However, accurate motion estimation becomes challenging in scenes with complex and non-linear motions due to the lack of information, often resulting in the generation of erroneous inter frames. Time Pre-trained TTA-EVF Frame 297 Frame 441 Frame 596 No Adaptation Test-Time Adaptation Frame 297 Frame 297 Frame 441 Frame 596 Figure 1. TTA-EVF efficiently produces high-quality results by adapting the network to the target domain in an online manner, alleviating the need for offline data supply and addressing the performance degradation observed when applying a well-trained eventbased VFI network to a domain with a different distribution. To address the inherent lack of intermediate information in frame cameras, recent researches [16, 24, 51, 52, 56] have explored the potential of event cameras as a promising solution for VFI, especially in scenarios involving complex motion. Event cameras sense dynamic changes in pixel intensity, triggering an event when the change surpasses a predefined threshold. The incorporation of event cameras as additional devices in VFI mitigates the challenges associated with modeling complex motion. Typically, frame-based VFI methods [17, 33, 38] are evaluated on test datasets (e.g., SNU-FILM [12], X4K1000FPS [43]) with distributions different from their training dataset (e.g., Vimeo90K [61]). Frame-based VFI demonstrates a certain level of generalization ability even without additional modules, providing evidence that it can be applied to real scenario applications, such as dealing with changes in camera devices. These device and environment This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25701",
        "chosen": "This paper proposes a test-time adaptation method for event-based video frame interpolation. The approach uses confident pixels as pseudo ground-truths for stable online learning, blending historical samples to prevent overfitting. Experiments validate its effectiveness in cross-domain and continuous domain shifting setups.",
        "rejected": "TTA-EVF proposes a test-time adaptation method for event-based Video Frame Interpolation (VFI). It leverages confident pixels as pseudo ground-truths for online learning from low-frame-rate videos, blending historical samples to prevent overfitting. Extensive experiments validate its effectiveness in cross-domain and continuous domain shifting setups, enhancing VFI performance without offline data supply."
    },
    {
        "prompt": "summary: Each photo in an image burst can be considered a sample of a complex 3D scene: the product of parallax, diffuse and specular materials, scene motion, and illuminant variation. While decomposing all of these effects from a stack of misaligned images is a highly ill-conditioned task, the conventional align-and-merge burst pipeline takes the other extreme: blending them into a single image. In this work, we propose a versatile intermediate representation: a twolayer alpha-composited image plus flow model constructed with neural spline fields – networks trained to map input coordinates to spline control points. Our method is able to, during test-time optimization, jointly fuse a burst image capture into one high-resolution reconstruction and decompose it into transmission and obstruction layers. Then, by discarding the obstruction layer, we can perform a range of tasks including seeing through occlusions, reflection suppression, and shadow removal. Tested on complex in-thewild captures we find that, with no post-processing steps or learned priors, our generalizable model is able to outperform existing dedicated single-image and multi-view obstruction removal approaches. 1. Introduction Over the last decade, as digital photos have increasingly been produced by smartphones, smartphone photos have increasingly been produced by burst fusion. To compensate for less-than-ideal camera hardware – typically restricted to a footprint of less than 1cm3 [6] – smartphones rely on their advanced compute hardware to process and fuse multiple lower-quality images into a high-fidelity photo [10]. This proves particularly important in low-light and highdynamic-range settings [22,39], where a single image must compromise between noise and motion blur, but multiple images afford the opportunity to minimize both [26]. But even as mobile night- and astro-photography applications [16, 17] use increasingly long sequences of photos as input, their output remains a static single-plane image. Given the typically non-static and non-planar nature of the real world, a core problem in burst image pipelines is thus Input Scene Burst Capture Neural Spline Field Layer Separation Recovered Obstruction + Transmission Figure 1. Fitting our two-layer neural spline field model to a stack of images we’re able to directly estimate and separate even severe, out-of-focus obstructions to recover hidden scene content. the alignment [32,45] and aggregation [5,63] of pixels into an image array – referred to as the align-and-merge process. While existing approaches treat pixel motion as a source of noise and artifacts, a parallel direction of work [9,20,69] attempts to extract useful parallax cues from this pixel motion to estimate the geometry of the scene. Recent work by Chugunov et al. [8] finds that maximizing the photometric consistency of an RGB plus depth neural field model of an image sequence is enough to distill dense depth estimates of the scene. While this method is able to jointly estimate high-quality camera motion parameters, it does not perform high-quality image reconstruction, and rather treats its image model as “a vehicle for depth optimization” [8]. In contrast, work by Nam et al. [50] proposes a neural field fitting approach for multi-image fusion and layer separation which focuses on the quality of the reconstructed “canonical view”. By swapping in different motion models, they can separate and remove layers such as occlusions, reflections, and moir´e patterns during image reconstruction – as opposed to in a separate post-processing step [19,54]. This approach, however, does not make use of a realistic camThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 25763",
        "chosen": "This paper introduces neural spline fields for burst image fusion and layer separation. The method constructs a two-layer alpha-composited image plus flow model, enabling tasks like occlusion removal and reflection suppression.",
        "rejected": "This work introduces a neural spline fields-based intermediate representation for burst photography, creating a two-layer alpha-composited image plus flow model. During test-time optimization, it fuses burst captures into a high-resolution reconstruction and decomposes them into transmission and obstruction layers. Discarding the obstruction layer facilitates tasks such as seeing through occlusions, reflection suppression, and shadow removal, outperforming existing single-image and multi-view approaches."
    },
    {
        "prompt": "summary: Recent advancements in Spatial Transcriptomics (ST) technology have facilitated detailed gene expression analysis within tissue contexts. However, the high costs and methodological limitations of ST necessitate a more robust predictive model. In response, this paper introduces TRIPLEX, a novel deep learning framework designed to predict spatial gene expression from Whole Slide Images (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing cellular morphology at individual spots, the local context around these spots, and the global tissue organization. By integrating these features through an effective fusion strategy, TRIPLEX achieves accurate gene expression prediction. Our comprehensive benchmark study, conducted on three public ST datasets and supplemented with Visium data from 10X Genomics, demonstrates that TRIPLEX outperforms current state-of-the-art models in Mean Squared Error (MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC). The model’s predictions align closely with ground truth gene expression profiles and tumor annotations, underscoring TRIPLEX’s potential in advancing cancer diagnosis and treatment. 1. Introduction The emergence of large-scale Spatial Transcriptomics (ST) technology has facilitated the quantification of mRNA expression across a multitude of genes within the spatial context of tissue samples [22]. ST technology segments centimeter-scale Whole Slide Images (WSIs) into hundreds of thousands of small spots, each providing its gene expression profile. Considering the substantial cost associated with ST sequencing technology, coupled with the widespread availability of WSIs, a pressing question is how to best predict spatial gene expression based on WSIs using rapidly evolving computer vision techniques. A number of studies have endeavored to address this *Corresponding author. challenge [7, 18, 24–26]. Approaches vary, with some predicting gene expression strictly from the tissue image confined within the spot’s boundaries [7], while others also take into account spatial dependencies between spot images [18, 26], or consider similarities to reference spots [24, 25]. However, we have noted several limitations inherent to these existing methodologies. Firstly, current methods primarily focus on spot images, neglecting the wealth of biological information available in the wider image context. By integrating both the specific spot and its surrounding environment, along with the holistic view of the entire histology image, we can access richer information, encompassing varied biological contexts. Secondly, models that consider interactions between spots [18, 26] face a limitation in processing the embedding of all patches in a WSI simultaneously. This approach, common in handling hundreds to thousands of patches within a WSI, limits the scalability of the patch embedding model due to resource constraints. Such limitations significantly impede the extraction of finegrained, rich representations from each spot, thereby affecting the model’s ability to perform detailed analysis of WSIs. Thirdly, model performance is frequently overestimated because of inadequate validation, such as using the limited size of dataset [7] sometimes without cross-validation [24] and training/testing with replicates from the same patient [18, 25, 26]. The limited size of ST datasets means that exclusive reliance on a single dataset for model evaluation can hinder an accurate assessment of the model’s capabilities, thereby emphasizing the necessity for cross-validation. The issue is compounded when replicate data from the same patient, often featuring nearly identical image-gene expression pairs, are used in both training and testing phases. This can lead to an inflated perception of a model’s effectiveness, as it may not accurately reflect the model’s ability to generalize to new, unseen data. Lastly, the use of disparate datasets, diverse normalization methods, and varied evaluation techniques in existing research studies compounds the challenge of conducting fair comparisons of the models. Addressing these limitations, we present TRIPLEX, an innovative deep learning framework designed to leverage This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 11591",
        "chosen": "This paper presents TRIPLEX, a deep learning framework predicting spatial gene expression from whole slide images. TRIPLEX uses multi-resolution features and outperforms state-of-the-art models in prediction accuracy.",
        "rejected": "TRIPLEX introduces a deep learning framework for predicting spatial gene expression from Whole Slide Images (WSIs). By integrating multi-resolution features, it captures cellular morphology at individual spots, local context around these spots, and global tissue organization. TRIPLEX employs an effective fusion strategy to achieve accurate predictions, outperforming state-of-the-art models in MSE, MAE, and PCC on three public ST datasets and Visium data. The model's predictions closely align with ground truth gene expressions and tumor annotations, highlighting its potential for advancing cancer diagnosis and treatment."
    },
    {
        "prompt": "summary: Existing research based on deep learning has extensively explored the problem of daytime image dehazing. However, few studies have considered the characteristics of nighttime hazy scenes. There are two distinctions between nighttime and daytime haze. First, there may be multiple active colored light sources with lower illumination intensity in nighttime scenes, which may cause haze, glow and noise with localized, coupled and frequency inconsistent characteristics. Second, due to the domain discrepancy between simulated and real-world data, unrealistic brightness may occur when applying a dehazing model trained on simulated data to real-world data. To address the above two issues, we propose a semi-supervised model for real-world nighttime dehazing. First, the spatial attention and frequency spectrum ﬁltering are implemented as a spatial-frequency domain information interaction module to handle the ﬁrst issue. Second, a pseudo-label-based retraining strategy and a local window-based brightness loss for semi-supervised training process is designed to suppress haze and glow while achieving realistic brightness. Experiments on public benchmarks validate the effectiveness of the proposed method and its superiority over state-of-the-art methods. The source code and Supplementary Materials are placed in the https://github.com/Xiaofeng-life/SFSNiD. 1. Introduction Nighttime and daytime images may contain hazy effects, which may cause their quality to be degraded [7, 15, 39, 50]. Therefore, two valuable research ﬁelds are proposed, which are daytime single image dehazing (DaSID) [2, 38, 54] and nighttime single image dehazing (NiSID) [14, 19, 31], respectively. Compared with the daytime hazy image, the imaging of the nighttime hazy image is more complex [28, 49]. Currently, NiSID is still a challenging problem. Existing research on DaSID [20, 25, 27, 34, 43, 48, 52, *Corresponding author (a) Hazy (b) IM-YellowHaze [26] (c) IM-NightHaze [26] (d) IM-NHR [51] (e) GE-UNREAL-NH [31] (f) Ours Figure 1. Visualization of real-world dehazed images, where the “IM-” and “GE-” denote the dehazed results obtained by training on imaging model (IM) and game engine (GE) simulated datasets, respectively. The curve ﬁgure represents the pixel histogram, where the x and y coordinates represent the pixel values and corresponding numbers, respectively. The x and y coordinates of the bar ﬁgure represent the color channel and the corresponding average pixel value, respectively. 53] have achieved impressive performance. Various effective DaSID algorithms have been proposed and veriﬁed on benchmark daytime datasets [21]. However, these DaSID algorithms are designed for the properties of daytime hazy and haze-free images, without taking into account the characteristics of nighttime hazy and haze-free images. Currently, NiSID research is divided into two types, namely non-deep learning-based NiSID and deep learningbased NiSID. On the one hand, the prior hypotheses and statistical laws are explored [50, 51]. The maximum reﬂectance prior to estimate the varying ambient illumination is proposed by [50]. The illumination estimation, color This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 2631",
        "chosen": "This paper proposes a semi-supervised model for real-world nighttime dehazing. The method includes a spatial-frequency domain information interaction module and strategies to handle nighttime haze characteristics and domain discrepancies.",
        "rejected": "A novel semi-supervised model is proposed for real-world nighttime dehazing, addressing the unique challenges of nighttime hazy scenes such as localized glow, noise, and frequency inconsistencies due to multiple active light sources. The approach includes a spatial-frequency domain information interaction module and a pseudo-label-based retraining strategy with a local window-based brightness loss to ensure realistic brightness while suppressing haze and glow. Experiments on public benchmarks demonstrate the method's effectiveness and superiority over existing techniques, emphasizing its capability to handle complex nighttime imaging conditions effectively."
    },
    {
        "prompt": "summary: Recent advancements have shown the potential of leveraging both point clouds and images to localize anomalies. Nevertheless, their applicability in industrial manufacturing is often constrained by significant drawbacks, such as the use of memory banks, which leads to a substantial increase in terms of memory footprint and inference times. We propose a novel light and fast framework that learns to map features from one modality to the other on nominal samples and detect anomalies by pinpointing inconsistencies between observed and mapped features. Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. Furthermore, we propose a layer pruning technique to improve memory and time efficiency with a marginal sacrifice in performance. 1. Introduction Industrial Anomaly Detection (AD) aims to identify unusual characteristics or defects in products, serving as a vital component within quality inspection processes. Collecting data to exemplify anomalies is challenging due to their rarity and unpredictability. Therefore, most works focus on unsupervised approaches, i.e., algorithms trained only on samples without defects, also referred to as nominal samples. Currently, most existing AD methods are geared toward analyzing RGB images. However, in many industrial settings, anomalies are hard to recognize effectively based solely on colour images, e.g., due to varying light conditions conducive to false detection and surface deviations that may not appear as unlikely colours. Deploying colour images and surface information acquired by 3D sensors can tackle the above issues and substantially improve AD. Recently, researchers have started to explore novel avenues thanks to the introduction of benchmark datasets for *These authors contributed equally to this work. Figure 1. Performance, speed and memory occupancy of Multimodal Anomaly Detection methods. The chart reports defect segmentation performance (AUPRO@30%) vs inference speed (Frame Rate on an NVIDIA 4090 GPU). 3D anomaly detection, such as MVTec 3D-AD [5] and Eyecandies [6]. Indeed, both provide RGB images alongside pixel-registered 3D information for all data samples, thereby fostering the development of new, multimodal AD approaches [17, 34, 39]. Unsupervised multimodal AD methods like BTF [17] and M3DM [39] rely on large memory banks of multimodal features. They achieve excellent performance (AUPRO@30% metric in Fig. 1) at the cost of extensive memory requirements and slow inference (Fig. 1). In particular, M3DM outperforms BTF by leveraging frozen feature extractors trained by self-supervision on large datasets, i.e., ImageNet and Shapenet, for 2D and 3D features, respectively. Another recent multimodal method, AST [34], follows a teacher-student paradigm conducive to a faster architecture (Fig. 1). Yet, AST does not exploit the spatial structure of the 3D data but employs this information just as an additional input channel in a 2D network architecture. This results in inferior performance compared to M3DM and BTF (Fig. 1). In this paper, we propose a novel paradigm to exploit the relationship between features extracted from different modalities and improve multimodal AD. The core idea beThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 17234",
        "chosen": "This paper presents a multimodal industrial anomaly detection framework that maps features from one modality to another on nominal samples. The approach detects anomalies by identifying inconsistencies between observed and mapped features. Experiments show state-of-the-art performance on the MVTec 3D-AD dataset with faster inference and lower memory usage than previous methods.",
        "rejected": "Recent research proposes a lightweight framework for anomaly detection in industrial settings that maps features between point clouds and images, detecting inconsistencies to identify anomalies. This method outperforms existing techniques in accuracy and speed on the MVTec 3D-AD dataset while using less memory."
    },
    {
        "prompt": "summary: Lifelong Person Re-identiﬁcation (L-ReID) aims to learn from sequentially collected data to match a person across different scenes. Once an L-ReID model is updated using new data, all historical images in the gallery are required to be re-calculated to obtain new features for testing, known as “re-indexing”. However, it is infeasible when raw images in the gallery are unavailable due to data privacy concerns, resulting in incompatible retrieval between the query and the gallery features calculated by different models, which causes signiﬁcant performance degradation. In this paper, we focus on a new task called Re-indexing Free Lifelong Person Re-identiﬁcation (RFL-ReID), which requires achieving effective L-ReID without re-indexing raw images in the gallery. To this end, we propose a Continual Compatible Representation (C2R) method, which facilitates the query feature calculated by the continuously updated model to effectively retrieve the gallery feature calculated by the old model in a compatible manner. Speciﬁcally, we design a Continual Compatible Transfer (CCT) network to continuously transfer and consolidate the old gallery feature into the new feature space. Besides, a Balanced Compatible Distillation module is introduced to achieve compatibility by aligning the transferred feature space with the new feature space. Finally, a Balanced Anti-forgetting Distillation module is proposed to eliminate the accumulated forgetting of old knowledge during the continual compatible transfer. Extensive experiments on several benchmark LReID datasets demonstrate the effectiveness of our method against state-of-the-art methods for both RFL-ReID and LReID tasks. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/C2R CVPR2024. 1. Introduction Person re-identiﬁcation (ReID) aims to identify the same person across different areas at different times [42]. Exist*Corresponding author. Query Images Query Features (a) Traditionial Lifelong ReID ID:1 Re-indexed Gallery Features ID:5 ID:1      ID:2 Old Gallery Images ReID w/ Re-indexing ReID w/o Re-indexing Ɍ Ɂ Query Features (b) Re-indexing Free Lifelong ReID Old Gallery Features Re-indexing New Model Query Images ID:1      ID:2 Old Gallery Images ID:3 ID:5 Ɂ Ɂ Data Privacy× New Model Old Gallery Features Feature Replacing Figure 1. Comparison between (a) the traditional Lifelong Person Re-identiﬁcation (L-ReID) task and (b) the Re-indexing Free Lifelong Person Re-identiﬁcation (RFL-ReID) task. ing methods [26, 32, 44] have made remarkable progress based on deep learning methods [37] and large-scale datasets [34, 41, 43]. However, their performance is often limited when training data are continuously collected from a series of different scenarios due to the well-known catastrophic forgetting challenge [3]. Recently, Lifelong person ReID (L-ReID) has aroused great concerns involving acquiring knowledge from streaming data and performing well across all data [5, 18, 19, 38]. Its challenge is to balance the anti-forgetting of old knowledge with the acquisition of new knowledge. To this end, existing L-ReID methods usually adopt the exemplar replay [38] and the knowledge distillation [27] to preserve This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 16614",
        "chosen": "This paper introduces the Re-indexing Free Lifelong Person Re-identification (RFL-ReID) task and proposes the Continual Compatible Representation (C2R) method. C2R facilitates effective retrieval without re-indexing raw images in the gallery, using a Continual Compatible Transfer (CCT) network and Balanced Compatible Distillation module. Experiments demonstrate effectiveness on benchmark datasets.",
        "rejected": "This paper introduces Re-indexing Free Lifelong Person Re-identification (RFL-ReID), aiming to eliminate the need for re-indexing gallery images during model updates. It presents a Continual Compatible Representation (C2R) approach to maintain retrieval compatibility across model updates without accessing original images, showing improved performance on benchmark datasets."
    },
    {
        "prompt": "summary: Recent advances in instruction tuning have led to the development of State-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these models, the impact of visual adversarial attacks on LMMs has not been thoroughly examined. We conduct a comprehensive study of the robustness of various LMMs against different adversarial attacks, evaluated across tasks including image classification, image captioning, and Visual Question Answer (VQA). We find that in general LMMs are not robust to visual adversarial inputs. However, our findings suggest that context provided to the model via prompts—such as questions in a QA pair—helps to mitigate the effects of visual adversarial inputs. Notably, the LMMs evaluated demonstrated remarkable resilience to such attacks on the ScienceQA task with only an 8.10% drop in performance compared to their visual counterparts which dropped 99.73%. We also propose a new approach to real-world image classification which we term query decomposition. By incorporating existence queries into our input prompt we observe diminished attack effectiveness and improvements in image classification accuracy. This research highlights a previously under explored facet of LMM robustness and sets the stage for future work aimed at strengthening the resilience of multimodal systems in adversarial environments. 1. Introduction Large Multi-modal Models (LMMs) have demonstrated remarkable abilities in a range of applications, from image classification and Visual Question Answering (VQA) to image captioning and semantic segmentation [1, 13, 22, 23, 28]. These models excel in generalizing to new domains with data-efficient solution, a feat attributed to advancements in Instruction Tuning [42]. Such techniques, traditionally applied to text-only models, have now been extended to multi-modal models, opening new avenues for efficient fine-tuning with significantly less data [13, 28]. †University of Central Florida LLaVA(adv): A group of people are sitting in a tub, with one person holding a toothbrush. ✗ Query: Is the photo taken indoor or outdoor? LLaVA(adv): Outdoor. ✓ Query: What is this image about? Query: Is there a tree in the image? LLaVA(adv): Yes. ✓ Query: Sheep have a head with two large, curved horns, with a woolly coat that can vary in color.. Question: What animal is in the image? LLaVA(adv): Sheep. ✓ Query: What is in the background of the image? LLaVA(adv): Trees. ✓ Query: What animal is in the image? LLaVA(adv): None. ✗ LLaVA: Two sheep are standing on a ledge, looking over a wall.. ✓ LLaVA: Sheep. ✓ Figure 1. QA pairs for LLaVA [28] given an adversarial image. “LLaVA” and “LLaVA(adv)” refer to LLaVA’s response to the user query with clean and adversarial image, respectively. For the readers, there are two sheep in the scene, and the adversarial attack was based on maximizing the distance between the image and the text “a photo of a sheep”. In the first two QA pairs, we can see that LLaVA(adv)’s answer is completely wrong. However, it can still answer the following questions correctly, because they are not pertinent to the object being attacked (sheep). Also note the contrast between the second and last QA pairs. LLaVA(adv) answers the question correctly after additional context has been provided. These observations help drove some of the findings in this paper. Source: COCO 2014 [26] Despite the recent advancements in LMMs, the impact of adversarial examples still remains under explored. Typically adversarial examples are generated end-to-end, targeting the final loss of the whole model, and focusing on a single modality. However, in the era of combining different pre-trained models with additional projectors or adaptors [8, 28, 44], it is imperative to reevaluate the effectiveness of these adversarial approaches. For example, let’s consider LLaVA [28] which uses CLIP as its visual component and LLAMA as text component (with some additional projector to bridge the gap), will an attack on one of the two components compromise its overall performance? From a practical perspective, given the substantial size of LMMs, attacking the entire model is often prohibitively expensive [7], making the above question an increasingly important one to answer since traditional adversarial attacks This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 24625",
        "chosen": "This paper evaluates the robustness of large multimodal models (LMMs) against visual adversarial attacks across tasks like image classification and VQA. Findings indicate LMMs are not robust, but context via prompts mitigates effects. A query decomposition approach for image classification is proposed.",
        "rejected": "The study examines the robustness of Large Multimodal Models (LMMs) against visual adversarial attacks, finding that LMMs are generally not robust but can be mitigated with contextual prompts. The proposed query decomposition method enhances image classification accuracy under adversarial conditions."
    },
    {
        "prompt": "summary: Acquiring large-scale, well-annotated datasets is essential for training robust scene text detectors, yet the process is often resource-intensive and time-consuming. While some efforts have been made to explore the synthesis of scene text images, a notable gap remains between synthetic and authentic data. In this paper, we introduce a novel method that utilizes Neural Radiance Fields (NeRF) to model real-world scenes and emulate the data collection process by rendering images from diverse camera perspectives, enriching the variability and realism of the synthesized data. A semi-supervised learning framework is proposed to categorize semantic regions within 3D scenes, ensuring consistent labeling of text regions across various viewpoints. Our method also models the pose, and view-dependent appearance of text regions, thereby offering precise control over camera poses and significantly *Corresponding author. E-mail: lianzhouhui@pku.edu.cn This work was supported by National Natural Science Foundation of China (Grant No.: 62372015), Center For Chinese Font Design and Research, and Key Laboratory of Intelligent Press Media Technology. improving the realism of text insertion and editing within scenes. Employing our technique on real-world scenes has led to the creation of a novel scene text image dataset (https://github.com/cuijl-ai/TextNeRF). Compared to other existing benchmarks, the proposed dataset is distinctive in providing not only standard annotations such as bounding boxes and transcriptions but also the information of 3D pose attributes for text regions, enabling a more detailed evaluation of the robustness of text detection algorithms. Through extensive experiments, we demonstrate the effectiveness of our proposed method in enhancing the performance of scene text detectors. 1. Introduction The detection of text in natural images are pivotal in advancing numerous computer vision applications, including industrial automation, image retrieval, robot navigation, and autonomous driving. Despite the remarkable progress in scene text detection, the inherent complexity of natural scenes and the diverse manifestations of text present are still This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 22272",
        "chosen": "This paper proposes TextNeRF, a method using Neural Radiance Fields to synthesize scene text images by modeling real-world scenes and rendering images from diverse perspectives. A semi-supervised learning framework ensures consistent labeling of text regions. The method improves realism of text insertion and editing within scenes.",
        "rejected": "A novel method is introduced for synthesizing scene text images using Neural Radiance Fields (NeRF), providing enriched variability and realism. The technique includes a semi-supervised learning framework for consistent labeling across viewpoints, enhancing the training of scene text detectors through a new dataset."
    },
    {
        "prompt": "summary: The lifting of a 3D structure and camera from 2D landmarks is at the cornerstone of the discipline of computer vision. Traditional methods have been confined to specific rigid objects, such as those in Perspective-n-Point (PnP) problems, but deep learning has expanded our capability to reconstruct a wide range of object classes (e.g. C3DPO [18] and PAUL [24]) with resilience to noise, occlusions, and perspective distortions. However, all these techniques have been limited by the fundamental need to establish correspondences across the 3D training data, significantly limiting their utility to applications where one has an abundance of “in-correspondence” 3D data. Our approach harnesses the inherent permutation equivariance of transformers to manage varying numbers of points per 3D data instance, withstands occlusions, and generalizes *Both authors advised equally. to unseen categories. We demonstrate state-of-the-art performance across 2D-3D lifting task benchmarks. Since our approach can be trained across such a broad class of structures, we refer to it simply as a 3D Lifting Foundation Model (3D-LFM) – the first of its kind. 1. Introduction Lifting 2D landmarks from a single-view RGB image into 3D has long posed a complex challenge in the field of computer vision because of the ill-posed nature of the problem. This task is important for a range of applications from augmented reality to robotics, and requires an understanding of non-rigid spatial geometry and accurate object descriptions [2, 11, 25]. Historically, efforts in single-frame 2D3D lifting have encountered significant hurdles: reliance on object-specific models, poor scalability, and limited adaptability to diverse and complex object categories. Traditional methods, while advancing in specific domains like human This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 10466",
        "chosen": "This paper introduces 3D-LFM, a 3D lifting foundation model leveraging transformer permutation equivariance for 2D-3D lifting tasks. The model handles varying points per 3D data instance, withstands occlusions, and generalizes to unseen categories, showing state-of-the-art performance.",
        "rejected": "This work develops a transformer-based approach called 3D Lifting Foundation Model (3D-LFM) for lifting 2D landmarks into 3D structures, demonstrating state-of-the-art performance across various benchmarks. The model's capability to handle occlusions and unseen categories marks a significant advancement in the field."
    },
    {
        "prompt": "summary: Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the common dropout techniques to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this study, we delve into this contrasting phenomenon through the lens of modality bias and uncover that an excessive modality bias towards the audio modality induced by dropout constitutes the fundamental cause. Next, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between the modality bias and the robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality, maintaining performance and robustness simultaneously. Finally, to address an entirely missing modality, we adopt adapters to dynamically switch decision strategies. The effectiveness of our proposed approach is evaluated through comprehensive experiments on the MISP2021 and MISP2022 datasets. Our code is available at https://github. com/dalision/ModalBiasAVSR. 1. Introduction Audio-Visual Speech Recognition (AVSR) is a multimodal application inspired by human speech perception. It outperforms single-modality models by incorporating noiseinvariant complementary information from visual cues, especially in noisy environments. Driven by increasingly large open-source datasets and models [1–4], AVSR has achieved significant advancements across various bench*Corresponding author. This work was supported by the National Natural Science Foundation of China under Grant No. 62171427. marks with a simple end-to-end design [5, 6]. Recent research on AVSR focuses on more challenging real-life scenarios. Techniques such as reinforcement learning [7] and carefully designed fusion architecture [8–10] are used to accommodate varying noise levels and overlapping speech. Self-supervised learning [11] and automatic labeling techniques [12] are applied facing insufficient audiovisual pairs. Meanwhile, various synchronization modules have been developed for audio-visual alignment.[13–15]. However, restricted to the open-source datasets [1, 2, 16], most studies often assume that each video is recorded in relatively high quality, without blurring, corruption, or loss. Moreover, there is growing evidence to suggest that current advanced AVSR systems are highly susceptible to perturbations in video modality [17, 18], resulting in significant performance degradation even perform worse than singlemodality models [19, 20]. Missing video modality is a crucial and common problem for AVSR applied in real-life scenarios [1, 17, 19, 20]. It arises from various causes, including losses induced by network latency or hardware limitations, as well as errors in lip movement tracking due to occlusion and side-face. Most researchers utilize dropout techniques 1 on video training data to improve robustness against missing modalities [19– 23]. It has been demonstrated to effectively mitigate the out-of-distribution (OOD) issue and alleviate performance degradation without additional inference consumption or complex modules. However, it leads to new challenges on real-life scenarios with low-quality input. In our early experiments on MISP datasets [24, 25], a contradictory phenomenon could be observed in Figure 1: while applying the dropout strategy to video training data enhance the robustness against missing video modality, it also leads to performance degradation when dealing with complete data input. 1Distinguished from the classic dropout that randomly deactivates nodes during neural network training, dropout in this paper specifically refers to a data augmentation technique that partially or entirely replaces original video frames with padding frames. This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 27445",
        "chosen": "This paper studies dropout-induced modality bias in Audio-Visual Speech Recognition (AVSR) systems. It presents the Modality Bias Hypothesis (MBH) and proposes a Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on audio, maintaining performance and robustness.",
        "rejected": "The paper explores modality bias in Audio-Visual Speech Recognition (AVSR) systems, proposing a Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on audio modality, ensuring robustness against missing video frames without compromising performance on complete data."
    },
    {
        "prompt": "summary: Retrieval tasks play central roles in real-world machine learning systems such as search engine, recommender system, and retrieval-augmented generation (RAG). Achieving decent performance in these tasks often requires fine-tuning various pretrained models on specific datasets and selecting the best candidate, a process that can be both time and resource consuming. To tackle the problem, we introduce a novel and efficient method, called RetMMD, that leverages Maximum Mean Discrepancy (MMD) and kernel methods to assess the transferability of pretrained models in retrieval tasks. RetMMD is calculated on pretrained model and target dataset without any fine-tuning involved. Specifically, given some query, we quantify the distribution discrepancy between relevant and irrelevant document embeddings, by estimating the similarities within their mappings in the finetuned embedding space through kernel method. This discrepancy is averaged over multiple queries, taking into account the distribution characteristics of the target dataset. Experiments suggest that the proposed metric calculated on pretrained models closely aligns with retrieval performance post fine-tuning. The observation holds across a variety of datasets, including image, text, and multi-modal domains, indicating the potential of using MMD and kernel methods for transfer learning evaluation in retrieval scenarios. In addition, we also design a way of evaluating dataset transferability for retrieval tasks, with experimental results demonstrating the effectiveness of the proposed approach. 1. Introduction Developing transfer learning evaluation metrics for retrieval tasks is of great importance in machine learning and information retrieval. Reliable metrics enable assessing the effectiveness of transfer learning models in retrieval-based applications, such as search engines, recommendation systems, and Retrieval Augmented Generation (RAG) with Large Language Models (LLMs) [1, 24, 39, 53]. For example, in*Partial work done at Salesforce. tegrating retrieval mechanisms into LLMs allows them to access and leverage external knowledge sources, significantly enhancing their ability to provide accurate, up-to-date and contextually relevant responses. In these contexts, the goal is often to retrieve the most relevant items from a large dataset given a specific query or user profile. A well-designed transfer learning evaluation metric can help assess whether the learned representations from a pretrained model are effectively capturing the underlying semantics of the data and improving retrieval performance. Additionally, such a metric can facilitate comparison between different models or different fine-tuning strategies, thus guiding researchers and practitioners in model selection and further optimization. Existing transfer learning evaluation metrics primarily focus on classification tasks [3, 37, 40, 50, 54]. Although classification and retrieval tasks are often used together and can be treated as complementary tasks, there exists fundamental differences between them. For instance, classification assigns data items to predefined categories or classes, while retrieval is the process of searching relevant information from a collection of items, often using similarity measures calculated from their embeddings. Another distinguishing factor is the asymmetric distribution of item embeddings in retrieval tasks. This asymmetric distribution can often result in a complex and nuanced performance landscape for retrieval models. Furthermore, the discrepancy between distributions of relevant and irrelevant documents in retrieval tasks is often dependent on the specific query. This means that any transfer learning evaluation metric must take into account this query-dependent variability to provide an accurate measure of model performance. In addition, one also needs to take account into the importance of each examined query, to be able to provide less biased predictions regarding overall retrieval performance in downstream tasks. Given these unique challenges, we propose a method designed specifically to assess model transferability in retrieval tasks. Our approach utilizes the Maximum Mean Discrepancy (MMD) [15] to quantify the discrepancy between distributions of relevant and irrelevant document embeddings in embedding spaces. We employ a kernel-based method to estimate these discrepancies, which are then averaged over This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 22390",
        "chosen": "This paper introduces RetMMD, a method using Maximum Mean Discrepancy (MMD) and kernel methods to assess pretrained model transferability in retrieval tasks. Experiments show the metric aligns with retrieval performance post fine-tuning across various datasets.",
        "rejected": "A novel metric named RetMMD is introduced for evaluating transferability of pretrained models in retrieval tasks, leveraging Maximum Mean Discrepancy (MMD) and kernel methods. Experiments demonstrate that RetMMD aligns closely with retrieval performance post fine-tuning, offering a promising evaluation approach across various domains."
    },
    {
        "prompt": "summary: It is especially challenging to achieve real-time human motion tracking on a standalone VR Head-Mounted Display (HMD) such as Meta Quest and PICO. In this paper, we propose HMD-Poser, the first unified approach to recover full-body motions using scalable sparse observations from HMD and body-worn IMUs. In particular, it can support a variety of input scenarios, such as HMD, HMD+2IMUs, HMD+3IMUs, etc. The scalability of inputs may accommodate users’ choices for both high tracking accuracy and easy-to-wear. A lightweight temporalspatial feature learning network is proposed in HMD-Poser to guarantee that the model runs in real-time on HMDs. Furthermore, HMD-Poser presents online body shape estimation to improve the position accuracy of body joints. Extensive experimental results on the challenging AMASS dataset show that HMD-Poser achieves new state-of-theart results in both accuracy and real-time performance. We also build a new free-dancing motion dataset to evaluate HMD-Poser’s on-device performance and investigate the performance gap between synthetic data and real-captured sensor data. Finally, we demonstrate our HMD-Poser with a real-time Avatar-driving application on a commercial HMD. Our code and free-dancing motion dataset are available here. 1. Introduction Human motion tracking (HMT), which aims at estimating the orientations and positions of body joints in 3D space, is highly demanded in various VR applications, such as gaming and social interaction. However, it is quite challenging to achieve both accurate and real-time HMT on HMDs. There are two main reasons. First, since only the user’s head and hands are tracked by HMD (including hand controllers) in the typical VR setting, estimating the user’s full!\"#$%&' !(#$%&')*+&,!.#$%&')/+&, .\"0\"(01 +2345  .12\"678%&'$!59611$:';<-#  .\"0\"(01  3\"6-1 ;(-16=\"5782>1\"057?1$&8@10 +2A1612.1$ 82$%&'$'1=7.1 '67=72B \"2$C=\"5\"6 82$%&' 72 >1\"0D57?1 C$=\"67\"(01$24?(16$8A /';<$+&,-$!;35782\"0# $ $ Figure 1. HMD-Poser can handle scalable input scenarios, including (a) HMD, (b) HMD+2IMUs wherein two IMUs are worn on the lower legs, (c) HMD+3IMUs wherein a third IMU is added to the pelvis, etc. HMD-Poser runs on HMD and outputs full-body motion data to drive an Avatar in real-time. body motions, especially lower-body motions, is inherently an under-constrained problem with such sparse tracking signals. Second, computing resources are usually highly restricted in portable HMDs, which makes deploying a realtime HMT model on HMDs even harder. Prior works have focused on improving the accuracy of full-body tracking. One category of methods utilizes three 6DOFs (degrees of freedom) from HMD to estimate fullbody motions, and they could be roughly classified into This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 874",
        "chosen": "This paper proposes HMD-Poser, a unified approach for real-time full-body motion tracking on VR HMDs using sparse observations from HMD and body-worn IMUs. The approach supports various input scenarios and includes a lightweight network and online body shape estimation.",
        "rejected": "HMD-Poser is presented as the first unified approach for real-time full-body motion tracking using sparse observations from HMD and body-worn IMUs. The lightweight temporal-spatial feature learning network ensures real-time operation on standalone VR headsets, achieving state-of-the-art results in both accuracy and performance."
    },
    {
        "prompt": "summary: Generative models have been very popular in the recent years for their image generation capabilities. GAN-based models are highly regarded for their disentangled latent space, which is a key feature contributing to their success in controlled image editing. On the other hand, diffusion models have emerged as powerful tools for generating highquality images. However, the latent space of diffusion models is not as thoroughly explored or understood. Existing methods that aim to explore the latent space of diffusion models usually relies on text prompts to pinpoint specific semantics. However, this approach may be restrictive in areas such as art, fashion, or specialized fields like medicine, where suitable text prompts might not be available or easy to conceive thus limiting the scope of existing work. In this paper, we propose an unsupervised method to discover latent semantics in text-to-image diffusion models without reThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 24209",
        "chosen": "This paper presents NoiseCLR, a contrastive learning approach for unsupervised discovery of interpretable semantics in the latent space of diffusion models without relying on text prompts.",
        "rejected": "This paper explores the latent space of text-to-image diffusion models, aiming to discover latent semantics without relying on restrictive text prompts. Existing methods often depend on text inputs to pinpoint specific semantics, which can be limiting in specialized fields like art, fashion, or medicine where suitable text prompts may not be readily available. The proposed unsupervised method aims to address this limitation by discovering meaningful latent variables directly from the data, enhancing the applicability and flexibility of diffusion models in generating high-quality images across various domains."
    },
    {
        "prompt": "summary: We introduce a new family of minimal problems for reconstruction from multiple views. Our primary focus is a novel approach to autocalibration, a long-standing problem in computer vision. Traditional approaches to this problem, such as those based on Kruppa’s equations or the modulus constraint, rely explicitly on the knowledge of multiple fundamental matrices or a projective reconstruction. In contrast, we consider a novel formulation involving constraints on image points, the unknown depths of 3D points, and a partially specified calibration matrix K. For 2 and 3 views, we present a comprehensive taxonomy of minimal autocalibration problems obtained by relaxing some of these constraints. These problems are organized into classes according to the number of views and any assumed prior knowledge of K. Within each class, we determine problems with the fewest—or a relatively small number of—solutions. From this zoo of problems, we devise three practical solvers. Experiments with synthetic and real data and interfacing our solvers with COLMAP demonstrate that we achieve superior accuracy compared to stateof-the-art calibration methods. The code is available at github.com/andreadalcin/MinimalPerspectiveAutocalibration. 1. Introduction Autocalibration is the fundamental process of determining intrinsic camera parameters using only point correspondences, without external calibration objects or known scene geometry [11–14, 23, 28, 33, 34, 36, 38, 47, 51]. 1.1. Contribution This paper presents a comprehensive characterization of two- and three-view minimal autocalibration problems in the case of a perspective camera with constant intrinsics. We introduce practical and efficient solvers for minimal autocalibration by introducing a novel formulation that extends the minimal Euclidean reconstruction problem of four points in three calibrated views [24, 40] to the uncalibrated case. Our approach jointly estimates camera intrinsics, encoded in the calibration matrix K, and unknown 3D point depths, and seamlessly integrates any partial knowledge of the camera intrinsics. This gives rise to a variety of two- and three-view minimal autocalibration problems, for which we provide a complete taxonomy in Tab. 1. We develop a general theory of minimal relaxations to address cases where our formulation leads to an over-constrained problem. These minimal relaxations of our depth formulation can be completely enumerated, and each instance of a specific autocalibration problem can be solved offline by applying numerical homotopy continuation (HC) methods to one such relaxation. Crucially, the offline analysis with HC methods also enables us to identify the most efficiently solvable minimal relaxations. Our practical contributions include implementing a numerical solver for full camera calibration, i.e., calibration of all 5 unknown parameters of a perspective camera. We also consider common assumptions—namely, zero-skew and square pixels—and design fast solvers for specialized problems with a partially calibrated camera. These solvers can be fast enough for many online calibration applications, and can also bootstrap solutions using RANSAC-based frameworks with high accuracy in offline calibration settings. Among the strengths of our approach, we avoid well-known degeneracies of Kruppa’s equations [48] and recover K directly instead of relying on estimates of the dual image of the absolute conic (DIAC), which may not be positivesemidefinite. Experiments show that our solvers outperform existing autocalibration methods in terms of accuracy in both synthetic and real image sequences despite increased runtime. Interfacing our solvers with COLMAP [44] further highlights the applicability of our approach. C1 <latexit sha1_base64=\"2XSB6vctFt2ibIztuHwmJKhq67w=\">ACEnicdVA9TwJBFNzDL8Qv1NJmIzGxIncGoyWRxh KjIAlcyLvlgRv29i670wI4SfYGf0vdsbWP+BfsfI4KR0qsnMvGTeBLGSlz3w8ktLa+sruXCxubW9s7xd29po0SI7AhIhWZVgAWldTYIEkKW7FBCAOFt8GwNvVv79FYGekbGsXohzDQsi8FUCpd17pet1hy24Gvki8GSmxGerd4menF4kRE1C gbVtz43JH4MhKROCp3EYgxiCANsp1RDiNYfZ1Un/CixQBGP0XCpeCbiz4sxhNaOwiBNhkB3dt7rR5rsf+ZU/MtrJ9Q/98dSxwmhFtMWJBVmLawMh0IeU8aJILpW8il5gIMEKGRHIRIxSRdrJCO5c1Ps0iaJ2WvUj69qpSqF7PZ8uyAHbJj5rEzVmW XrM4aTLABe2BP7Nl5dF6cV+ftO5pzZjf7Bec9y97/p3q</latexit> xiq <latexit sha1_base64=\"ZDWfS4W5wCjdMfUNT7krdrNOck=\">ACFXicdVC7TsNAEDyHVwivACXNiQiJKrJREJQ RNJRBIg8psaLzZROnM/mbo2IrPwDHYJ/oUO01PwKFbZxAQlMNZqZlWbHC6UwaNsfVmFhcWl5pbhaWlvf2Nwqb+0TBpDk0eyEB3PGZACgVNFCihE2pgvieh7Y3PU79B9qIQF3hJATXZyMlhoIzTKTWfT8Wt9N+uWJX7Qx0njg5qZAcjX7 5szcIeOSDQi6ZMV3HDtGNmUbBJUxLvchAyPiYjaCbUMV8MG6ctZ3Sg8gwDGgImgpJMxF+XsTMN2bie0nSZ3htZr1hoND8Z6biX143wuGpGwsVRgiKpy1QSMhaGK5FshHQgdCAyNK3gApFOdMEbSgjPNEjJLRSslYzuw086R1VHVq1ePLWqV +ls9WJHtknxwSh5yQOrkgDdIknNyQB/JEnq1H68V6td6+owUrv9klv2C9fwEC25/e</latexit> Xp <latexit sha1_base64=\"B3Quf9Hiy5SE6Dkge+HzD7gqzs=\">ACEnicdVC7SgNBFJ2NrxhfUubwSBYhV2JaBm0sY xoHpAsYXZyE4fMzg4zd4UQ8gl2ov9iJ7b+gL9i5e6hSZ6qsM58K5J9BSWHTdD6ewtLyulZcL21sbm3vlHf3WjaKDYcmj2RkOgGzIWCJgqU0NEGWBhIaAfjy9Rv34OxIlK3ONHgh2ykxFBwhol0+nrfrniVt0MdJF4OamQHI1+bM3iHgcgkIu mbVdz9XoT5lBwSXMSr3YgmZ8zEbQTahiIVh/mlWd0aPYMoyoBkOFpJkIPy+mLR2EgZJMmR4Z+e9YaTQ/mem4l9eN8bhuT8VSscIiqctUEjIWlhuRDIQ0IEwgMjSt4AKRTkzDBGMoIzRIyTxUrJWN78NIukdVL1atXT61qlfpHPViQH5JAcE4+ckTq 5Ig3SJyMyAN5Is/Oo/PivDpv39GCk9/sk19w3r8ACKePg=</latexit> Ci <latexit sha1_base64=\"Sw+bqk05SmHaZ5o12fyk3S+/h/E=\">ACEnicdVA9TwJBFHyHX4hfqKXNRmJiRe4MRksijS VGQRK4kL3lgRv29i670wI4SfYGf0vdsbWP+BfsfI4KR0qsnMvGTeBLGSlz3w8ktLa+sruXCxubW9s7xd29po0SI7AhIhWZVsAtKqmxQZIUtmKDPAwU3gbD2tS/vUdjZaRvaBSjH/KBln0pOKXSda0ru8WSW3YzsEXizUgJZqh3i5+dXiSEDUJ xa1te25M/pgbkLhpNBJLMZcDPkA2ynVPETrj7OqE3aUWE4Ri9EwqVgm4s+LMQ+tHYVBmgw53dl5rx9psv+ZU/Evr51Q/9wfSx0nhFpMW5BUmLWwsh0IGQ9aZCIT9CJjUT3HAiNJxIVIxSRcrpGN589MskuZJ2auUT68qperFbLY8HMAhHIMHZ1C FS6hDAwQM4AGe4Nl5dF6cV+ftO5pzZjf78AvO+xfZnp4i</latexit> CM <latexit sha1_base64=\"dYU0D6NtPg3R3NjgwHeJ2ItpBow=\">ACEnicdVDLSsNAFJ3UV62vqks3g0VwVRKp6LYjR uhon1AG8pkeluHTiZh5kYoZ/gTvRf3Ilbf8BfcWUSs9BWz+pwzrlw7vFCKQza9odVWFpeWV0rpc2Nre2d8q7e20TRJpDiwcy0F2PGZBCQsFSuiGpjvSeh4k0bqd+5BGxGoW5yG4PpsrMRIcIaJdNMYXA3KFbtqZ6CLxMlJheRoDsqf/WHAIx8U csmM6Tl2iG7MNAouYVbqRwZCxidsDL2EKuaDceOs6oweRYZhQEPQVEiaifDzIma+MVPfS5I+wzsz740CheY/MxX/8noRjs7dWKgwQlA8bYFCQtbCcC2SgYAOhQZElr4FVCjKmWaIoAVlnCdilCxWSsZy5qdZJO2TqlOrnl7XKvWLfLYiOSCH5Jg45Iz UySVpkhbhZEweyBN5th6tF+vVevuOFqz8Zp/8gvX+BarOngY=</latexit> Xq <latexit sha1_base64=\"/Wl+QO+p+fKb+w0keoeVATkLEVs=\">ACEnicdVC7TsNAEDyHVwivACXNiQiJKrIRCMoIGs ogyENKrGh92YRTzmdzt0aKonwCHYJ/oUO0/AC/QoUdUkACU41mZqXZCWIlLbnuh5NbWFxaXsmvFtbWNza3its7dRslRmBNRCoyzQAsKqmxRpIUNmODEAYKG8HgIvMb92isjPQNDWP0Q+hr2ZMCKJWum527TrHklt0J+DzxpqTEpqh2ip/tbiSEDUJ Bda2PDcmfwSGpFA4LrQTizGIAfSxlVINIVp/NKk65geJBYp4jIZLxSci/rwYQWjtMAzSZAh0a2e9XqTJ/mdm4l9eK6HemT+SOk4ItchakFQ4aWGFkelAyLvSIBFkbyGXmgswQIRGchAiFZN0sUI6ljc7zTypH5W94/LJ1XGpcj6dLc/2D47ZB47ZRV 2yaqsxgTrswf2xJ6dR+fFeXevqM5Z3qzy37Bef8ClOePw=</latexit> K−1xip <latexit sha1_base64=\"N1nOjePq3T8BaE3KbCkBXKuc4U=\">ACG3icdVA9SwNBFNzM8avqKXNYhBsDHcS0TJoI9hEMB +QnGFv8xKX7O0tu+/EcORv2In+FzuxtfCvWHk5U2iUw0z82DeBFoKi674czNLywuLedW8qtr6xubha3tuo1iw6HGIxmZsAsSKGghgIlNLUBFgYSGsHgfOw37sBYEalrHGrwQ9ZXoic4w1RqX94kh97ovpMIPeoUim7JzUBniTchRTJBtVP4bHcjHoegkEtmb ctzNfoJMyi4hFG+HVvQjA9YH1opVSwE6ydZ5xHdjy3DiGowVEiaifDzImGhtcMwSJMhw1s7fUihfY/cyz+5bVi7J36iVA6RlB83AKFhKyF5UakSwHtCgOIbPwWUKEoZ4YhghGUcZ6KcTpdPh3Lm5mltSPSl65dHxVLlbOJrPlyC7ZIwfEIyekQi5IldQIJ5o8 kCfy7Dw6L86r8/YdnXMmNzvkF5z3L0dFohg=</latexit> xip <latexit sha1_base64=\"Rom81qI/rQ7PWgLYA0G6G21RiME=\">ACFXicdVA9SwNBFNyLXzF+RS1tFoNgFe5E0TJoYx nBfEASwt7mJa7Z21t234nhyH+wE/0vdmJr7V+x8u68QhOdapiZB/PG1JYdN0Pp7CwuLS8Ulwtra1vbG6Vt3eaNowMhwYPZWjaPrMghYIGCpTQ1gZY4Eto+eOL1G/dgbEiVNc40dAL2EiJoeAME6l534+FnvbLFbfqZqDzxMtJheSo98uf3UHIowAUc sms7Xiuxl7MDAouYVrqRhY042M2gk5CFQvA9uKs7ZQeRJZhSDUYKiTNRPh5EbPA2kngJ8mA4Y2d9YahQvufmYp/eZ0Ih2e9WCgdISietkAhIWthuRHJRkAHwgAiS98CKhTlzDBEMIyzhMxSkYrJWN5s9PMk+ZR1TunlwdV2rn+WxFskf2ySHxyCmp kUtSJw3CyS15IE/k2Xl0XpxX5+07WnDym13yC87FwEun90=</latexit> Figure 1. Illustrating the setup of equations (1) and (4). This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 5064",
        "chosen": "This paper introduces a new family of minimal problems for reconstruction from multiple views, focusing on autocalibration. The approach involves constraints on image points, unknown depths, and a partially specified calibration matrix K, with practical solvers developed for minimal autocalibration problems.",
        "rejected": "The research introduces a novel family of minimal problems for autocalibration in multi-view reconstruction, focusing on a new approach that relaxes traditional constraints such as those based on Kruppa’s equations or modulus constraint. By formulating the problem with constraints on image points, unknown depths of 3D points, and partially specified calibration matrices, the study presents a comprehensive taxonomy of minimal autocalibration problems for 2 and 3 views. Practical solvers developed from this framework demonstrate superior accuracy compared to state-of-the-art methods when tested on both synthetic and real datasets. The approach also integrates seamlessly with existing tools like COLMAP, showcasing its practical utility and robustness."
    },
    {
        "prompt": "summary: In this work, we tackle the problem of domain generalization for object detection, specifically focusing on the scenario where only a single source domain is available. We propose an effective approach that involves two key steps: diversifying the source domain and aligning detections based on class prediction confidence and localization. Firstly, we demonstrate that by carefully selecting a set of augmentations, a base detector can outperform existing methods for single domain generalization by a good margin. This highlights the importance of domain diversification in improving the performance of object detectors. Secondly, we introduce a method to align detections from multiple views, considering both classification and localization outputs. This alignment procedure leads to better generalized and well-calibrated object detector models, which are crucial for accurate decision-making in safety-critical applications. Our approach is detector-agnostic and can be seamlessly applied to both single-stage and two-stage detectors. To validate the effectiveness of our proposed methods, we conduct extensive experiments and ablations on challenging domain-shift scenarios. The results consistently demonstrate the superiority of our approach compared to existing methods. Our code and models are available at: https://github.com/msohaildanish/DivAlign. 1. Introduction In recent years, we have witnessed remarkable performance improvements in supervised object detection [39, 47, 48, 53]. The success of these methods rely on the assumption that the training and testing data are sampled from the same distribution. However, in many real-world applications, such as autonomous driving, this assumption is often violated and these object detectors usually show degraded performance due to a phenomenon called domainshift [3, 5, 49]. Shifts in real-world domains are typically caused by environmental alterations, like varying weather and time conditions. These changes manifest in diverse contrasts, brightness levels, and textures among others. A prominent line of research that attempts to alleviate the impact of domain-shift is known as unsupervised domain adaptation (UDA) [20, 24, 42, 49, 63, 68, 72]. Given the labeled data from the source domain and unlabelled data from the target domain, the aim of UDA methods is to align the source and target data distributions so that the trained model can generalize well to the target domain [67]. An obvious limitation of UDA methods is that they require precollecting data and re-training the model for different target domains. Collecting data, even without annotation, for all possible domain shifts and training the model when shift happens is difficult and sometime not possible. To cope with the domain-shift problem, a more realistic albeit challenging problem is domain generalization [2, 11, 23, 26, 32, 34, 35, 40, 58]. The goal of domain generalization (DG) is to learn a generalizable model typically from multiple source domains, available during training, that can perform well on the unseen target domain. A dominant class of DG methods attempts to learn domain-invariant feature space across multiple source domains [1, 11, 33, 35]. The performance of these methods is sensitive to the number of available source domains [7, 70]. In many realistic scenarios, acquiring labeled data from multiple source domains is often costly and timeconsuming, eventually restricting the potential utilization of such methods. For these reasons, generalizing from a single-source domain is a more practical setting. Recent surveys [29, 70] reveal that very little work has been done on DG for object detection [38, 60], despite the fact that object detectors occupy an important position in many safety-critical and security-sensitive applications e.g., autonomous driving. An object detector should be able to provide accurate as well as calibrated detections in different This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 17732",
        "chosen": "This paper proposes a method to improve domain generalization for object detection from a single source domain. The approach involves diversifying the source domain through careful augmentation selection and aligning detections based on class prediction confidence and localization. Experiments show superior performance compared to existing methods.",
        "rejected": "In this work, the authors tackle domain generalization for object detection with a single source domain. They introduce diversification through augmentations and align detections based on class prediction confidence and localization, leading to better-generalized models. The approach is validated on challenging domain-shift scenarios, showing superior performance."
    },
    {
        "prompt": "summary: The existing facial datasets, while having plentiful images at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with proﬁle or pitched faces. This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k highquality images of faces at extreme poses. To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings. Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3Daware GAN, diffusion-based text-to-image face generation, and face reenactment. Speciﬁcally, training with EFHQ helps models generalize well across diverse poses, significantly improving performance in scenarios involving extreme views, conﬁrmed by extensive experiments. Additionally, we utilize EFHQ to deﬁne a challenging cross-view face veriﬁcation benchmark, in which the performance of SOTA face recognition models drops 5-37% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild. 1. Introduction The human face is a central focus in computer vision, with extensive research dedicated to tasks like detection, recognition, generation, and manipulation [39, 42]. Numerous large-scale facial datasets[9, 12, 14, 24, 28, 30, 45, 48, 54, 55] have facilitated the face-related studies. Along with deep learning techniques, many tasks have made a considerable leap in performance in recent years. For instance, face recognition systems can recognize near-frontal faces in the wild with near-perfect accuracy [13, 18]. Facial generative models, such as [5, 15, 49], can synthesize realistic This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 22605",
        "chosen": "This paper introduces EFHQ, a large-scale dataset of extreme pose face images. Created by curating existing datasets, EFHQ helps improve model performance on profile and pitched faces. The dataset also defines a challenging cross-view face verification benchmark.",
        "rejected": "This paper introduces EFHQ, a dataset containing 450k high-quality images of faces at extreme poses, curated from existing datasets. Training with EFHQ improves model generalizability across diverse poses, especially in extreme view scenarios. Additionally, a cross-view face verification benchmark using EFHQ reveals significant drops in SOTA models' performance compared to frontal-face scenarios."
    },
    {
        "prompt": "summary: Modeling and visualizing relationships between tasks or datasets is an important step towards solving various meta-tasks such as dataset discovery, multi-tasking, and transfer learning. However, many relationships, such as containment and transferability, are naturally asymmetric and current approaches for representation and visualization (e.g., t-SNE [44]) do not readily support this. We propose TASK2BOX, an approach to represent tasks using box embeddings—axis-aligned hyperrectangles in low dimensional spaces—that can capture asymmetric relationships between them through volumetric overlaps. We show that TASK2BOX accurately predicts unseen hierarchical relationships between nodes in ImageNet and iNaturalist datasets, as well as transferability between tasks in the Taskonomy benchmark. We also show that box embeddings estimated from task representations (e.g., CLIP [36], Task2Vec [4], or attribute based [15]) can be used to predict relationships between unseen tasks more accurately than classifiers trained on the same representations, as well as handcrafted asymmetric distances (e.g., KL divergence). This suggests that low-dimensional box embeddings can effectively capture these task relationships and have the added advantage of being interpretable. We use the approach to visualize relationships among publicly available image classification datasets on popular dataset hosting platform called Hugging Face. 1. Introduction The success of deep learning has led to the proliferation of datasets for solving a wide range of computer vision problems. Yet, there are few tools available to enable practitioners to find datasets related to the task at hand, and to solve various meta-tasks related to it. We present TASK2BOX, a method to represent tasks using axis-aligned hyperrectangles (or box embeddings). TASK2BOX is framed as a learnable mapping from dataset representation to boxes, and can be trained to predict various relationships between novel tasks such as transferability, hierarchy, and overlap. Box embeddings [48] extend order embeddings [46] by using volumetric relationships between axis-aligned hyperThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 28827",
        "chosen": "This paper presents TASK2BOX, an approach using box embeddings to model asymmetric relationships between tasks. The method accurately predicts hierarchical relationships and transferability between tasks, offering an interpretable way to visualize task relationships.",
        "rejected": "TASK2BOX proposes representing tasks as box embeddings to capture asymmetric relationships between them. This method accurately predicts unseen hierarchical and transferability relationships among tasks, outperforming classifiers trained on the same representations. Visualization of image classification datasets on Hugging Face demonstrates its effectiveness."
    },
    {
        "prompt": "summary: Reconstructing dynamic objects from monocular videos is a severely underconstrained and challenging problem, and recent work has approached it in various directions. However, owing to the ill-posed nature of this problem, there has been no solution that can provide consistent, highquality novel views from camera positions that are significantly different from the training views. In this work, we introduce Neural Parametric Gaussians (NPGs) to take on this challenge by imposing a two-stage approach: first, we fit a low-rank neural deformation model, which then is used as regularization for non-rigid reconstruction in the second stage. The first stage learns the object’s deformations such that it preserves consistency in novel views. The second stage obtains high reconstruction quality by optimizing 3D Gaussians that are driven by the coarse model. To this end, we introduce a local 3D Gaussian representation, where temporally shared Gaussians are anchored in and deformed by local oriented volumes. The resulting combined model can be rendered as radiance fields, resulting in high-quality photo-realistic reconstructions of the non-rigidly deforming objects. We demonstrate that NPGs achieve superior results compared to previous works, especially in challenging scenarios with few multi-view cues.1 1Project Page: https://geometric-rl.mpi-inf.mpg.de/npg/ 1. Introduction Reconstructing 3D objects from 2D observations is a core problem in computer vision with numerous applications in several industries, such as the movie and game industry, AR/VR, and robotics. Tremendous progress has been seen in static scene reconstruction during the last few years. The real world is, however, dynamic, and most recorded scenes are captured in a casual setting, with sparse coverage from a single camera. Thus, addressing these two aspects during reconstruction is of fundamental importance. The success of neural approaches on static scenes has encouraged their use for dynamic scene reconstruction from monocular videos, both in its classical [24, 40, 45] and hybrid [4, 10, 28] forms. These methods either learn a per-frame scene representation with limited time consistency [10, 24] or utilize a time-invariant canonical space, which is used to track the observations at each timestep [41, 45]. However, as pointed out in Gao et al. [12], they are evaluated on datasets that contain multi-view signals, such as camera teleportation—i.e., alternating samples from different cameras to construct a temporal sequence—and limited object motion, and their performance suffers drastically when evaluated on more realistic monocular sequences. Such sequences usually contain faster object motion compared to camera motion. Strong regularization is required in order to propagate information between different timesteps with the correct data association. As we will demonstrate This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 10715",
        "chosen": "This paper proposes Neural Parametric Gaussians (NPGs) for reconstructing dynamic objects from monocular videos. The two-stage approach first fits a deformation model, then refines it with 3D Gaussians, achieving high-quality reconstructions of non-rigidly deforming objects.",
        "rejected": "NPGs address dynamic object reconstruction from monocular videos by imposing a two-stage approach: fitting a low-rank neural deformation model followed by optimizing 3D Gaussians. This method achieves high-quality photo-realistic reconstructions, demonstrating superiority in scenarios with few multi-view cues."
    },
    {
        "prompt": "summary: Even the best current algorithms for estimating body 3D shape and pose yield results that include body selfintersections. In this paper, we present CLOAF, which exploits the diffeomorphic nature of Ordinary Differential Equations to eliminate such self-intersections while still imposing body shape constraints. We show that, unlike earlier approaches to addressing this issue, ours completely eliminates the self-intersections without compromising the accuracy of the reconstructions. Being differentiable, CLOAF can be used to ﬁne-tune pose and shape estimation baselines to improve their overall performance and eliminate self-intersections in their predictions. Furthermore, we demonstrate how our CLOAF strategy can be applied to practically any motion ﬁeld induced by the user. CLOAF also makes it possible to edit motion to interact with the environment without worrying about potential collision or loss of body-shape prior. 1. Introduction Feed-forward approaches to estimating human body 3D shape and pose from a single image have become remarkably effective [6, 25, 35]. The very recent transformer-based architecture of [10] embodies the current state-of-the-art. It is pre-trained on 300 million images and ﬁne-tuned on most SMPL data sets in existence. However, as good as these methods have become, they can still produce unrealistic poses with substantial self-intersections of body parts, as illustrated by Fig. 1. This is a serious issue if video-based motion capture is to be used in ﬁelds, such as robotics or realistic animation, where preventing self-intersections is of utmost importance. Most current approaches to addressing this issue [4, 13, 24, 28] are iterative. They penalize self-intersections explicitly by minimizing an interpenetration loss. This requires explicitly detecting self-intersections and then performing *This work was supported in part by the Swiss National Science Foundation Collision Rate (%) HMR2.0 Optim CLOAF Methods 39.2% 9.2% 0.0% Figure 1. Self-intersections in SOTA methods. Top rows. HMR2.0 [10] (ﬁrst row) and PARE [19] (second row), two of the best current methods, produce bodies shown in blue with selfintersections. CLOAF removes them and generates the results shown in gold. Bottom row. HMR 2.0 [10] recovers bodies with self-intersections in 39.2% of frames of the 3DPW-test set. A recent post-processing method such as [28] brings this down to 9.2%. CLOAF drops this number all the way to zero. a separate optimization step, which makes the whole process non-differentiable and precludes its use during training. Another approach is to eliminate self-intersections in the training databases [26]. While all these methods help, they do not guarantee the absence of self-collisions at inference time. In this paper, we propose a different approach. It prevents self-intersections in a differentiable manner and without an explicit detection step. To this end, we rely on the fact that if the scene ﬂow from one body to another is the This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 1176",
        "chosen": "This paper presents CLOAF, a method using Ordinary Differential Equations to eliminate self-intersections in human body 3D shape and pose estimation. The differentiable approach improves reconstruction accuracy without explicit detection steps.",
        "rejected": "CLOAF uses differential equations to eliminate body self-intersections while preserving shape constraints during 3D human pose estimation. Unlike iterative methods that minimize interpenetration loss, CLOAF ensures no self-intersections without compromising accuracy, making it suitable for realistic animation and robotics applications."
    },
    {
        "prompt": "summary: We present 3D Paintbrush, a technique for automatically texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly integrate into standard graphics pipelines. We opt to simultaneously produce a localization map (to specify the edit region) and a texture map which conforms to it. This approach improves the quality of both the localization and the stylization. To enhance the details and resolution of the textured area, we leverage multiple stages of a cascaded diffusion model to supervise our local editing technique with generative priors learned from images at different resolutions. Our technique, referred to as Cascaded Score Distillation (CSD), simultaneously distills scores at multiple resolutions in a cascaded fashion, enabling control over both the granularity and global understanding of the supervision. We demonstrate the effectiveness of 3D Paintbrush to locally texture different semantic regions on a variety of shapes. Project page: https://threedle.github.io/3dpaintbrush 1. Introduction The ability to edit existing high-quality 3D assets is a fundamental capability in 3D modeling workflows. Recent works have shown exceptional results for text-driven 3D data creation [32, 38, 48, 53, 58, 59], but focus on making global edits. While some progress has been made on local editing using an explicit localization of the edit region [49, 67], these regions are often coarse and lack fine-grained detail. Highly-detailed and accurate localizations are important for constraining the edits to be within a specific region, preventing changes unrelated to the target edit. Furthermore, while meshes with texture maps are the de facto standard in graphics pipelines, existing local editing work does not natively operate on meshes nor produce texture maps for them. In this work we develop 3D Paintbrush, a method for automatically texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly integrate into standard graphics pipelines. 3D Paintbrush is controlled via intuitive, free-form text input, allowing users to describe their edits using open vocabulary on a wide range of meshes. Specifically, given an input mesh and a text prompt, 3D Paintbrush produces the corresponding high-quality texture map and a localization region to confine it. To enhance the details and resolution of the locally textured area, we introduce Cascaded Score Distillation (CSD) which leverages multiple stages of a cascaded diffusion model. Our explicit localization masks can be used to layer our edit texture onto existing textures. We opt to represent both our localization map and texture map as neural fields encoded by multi-layer perceptions. Our method synthesizes both a fine-grained localization mask and high-quality texture in tandem. SimultaneThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 4473",
        "chosen": "This paper introduces 3D Paintbrush, a technique for texturing local semantic regions on meshes via text descriptions. Using cascaded diffusion models, the method produces high-quality texture maps that integrate into standard graphics pipelines.",
        "rejected": "3D Paintbrush presents a technique for texturing local semantic regions on meshes via text descriptions. By leveraging cascaded diffusion models, it simultaneously produces localization and texture maps, enhancing detail and resolution. The method allows for intuitive control over edits, seamlessly integrating into standard graphics pipelines."
    },
    {
        "prompt": "summary: When building classification systems with demographic fairness considerations, there are two objectives to satisfy: 1) maximizing utility for the specific task and 2) ensuring fairness w.r.t. a known demographic attribute. These objectives often compete, so optimizing both can lead to a trade-off between utility and fairness. While existing works acknowledge the trade-offs and study their limits, two questions remain unanswered: 1) What are the optimal tradeoffs between utility and fairness? and 2) How can we numerically quantify these trade-offs from data for a desired prediction task and demographic attribute of interest? This paper addresses these questions. We introduce two utilityfairness trade-offs: the Data-Space and Label-Space Tradeoff. The trade-offs reveal three regions within the utilityfairness plane, delineating what is fully and partially possible and impossible. We propose U-FaTE, a method to numerically quantify the trade-offs for a given prediction task and group fairness definition from data samples. Based on the trade-offs, we introduce a new scheme for evaluating representations. An extensive evaluation of fair representation learning methods and representations from over 1000 pre-trained models revealed that most current approaches are far from the estimated and achievable fairness-utility trade-offs across multiple datasets and prediction tasks. 1. Introduction As learning-based systems are increasingly being deployed in high-stakes applications, there is a dire need to ensure that they do not propagate or amplify any discriminative tendencies inherent in the training datasets. An ideal solution would impart fairness to prediction models while retaining the performance of the same model when learned without fairness considerations. Realizing this goal necessitates optimizing two objectives: maximizing utility in predicting a label Y for a target task (e.g., face identity) while minimizing the unfairness w.r.t. a demographic attribute S (e.g., age or gender). However, when the statistical dependence between Y and S Possible Impossible Possible with Extra Data DST LST Utility (Y) Unfairness (S) (a) 0 10 20 30 EOD (%) 30 40 50 60 70 80 90 100 Accuracy (%) DST LST Zero-Shot Supervised (b) Figure 1. The utility-fairness trade-offs. (a) Classification systems can be evaluated by their utility (e.g., accuracy) w.r.t. a target label Y and their unfairness w.r.t. a demographic label S. We introduce two trade-offs, Data Space Trade-Off (DST) and Label Space Trade-Off (LST). (b) We empirically estimate DST and LST on CelebA and evaluate the utility (high cheekbones) and fairness (gender & age) of over 100 zero-shot and 900 supervised models. is not negligible, learning with fairness considerations will necessarily degrade the performance of the target predictor, i.e., a trade-off will exist between utility and fairness. The existence of a utility-fairness trade-off has been well established, theoretically [11, 22, 29, 36, 37] and empirically [29], in multiple prior works. However, the focus of this body of work has been limited in multiple respects. First, prior work [29, 36] focused on just one type of tradeoff, ignoring other possible trade-offs between utility and fairness. Second, prior work [36, 37] focused on establishing bounds or identifying the end-points of the trade-off of interest rather than attempting its precise characterization. Third, the majority of the prior work [11, 29, 36, 37] has investigated the utility-fairness trade-offs for one definition of group fairness, namely, demographic parity (DP). There are multiple fairness definitions [2], including those more practically relevant than DP, such as Equalized Opportunity (EO), for which the trade-offs have not been studied. Despite these attempts, several questions related to the utility-fairness trade-offs remain outstanding. 1. What are the optimal utility-fairness trade-offs? 2. For a given prediction task and a demographic attribute, we wish to be fair w.r.t., how can we empirically estimate the trade-offs from data? This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 12037",
        "chosen": "This paper addresses the utility-fairness trade-offs in classification systems with demographic fairness considerations. The authors introduce methods to quantify these trade-offs and evaluate fair representation learning methods.",
        "rejected": "The paper discusses utility-fairness trade-offs in classification systems considering demographic fairness. It introduces Data-Space and Label-Space Trade-offs, delineating what is possible, partially possible, and impossible. U-FaTE quantifies these trade-offs for given tasks and demographic attributes, revealing that most current approaches fall short of achievable fairness-utility balances."
    },
    {
        "prompt": "summary: Estimating large, extreme inter-image rotations is critical for numerous computer vision domains involving images related by limited or non-overlapping fields of view. In this work, we propose an attention-based approach with a pipeline of novel algorithmic components. First, as rotation estimation pertains to image pairs, we introduce an inter-image distillation scheme using Decoders to improve embeddings. Second, whereas contemporary methods compute a 4D correlation volume (4DCV) encoding inter-image relationships, we propose an Encoder-based cross-attention approach between activation maps to compute an enhanced equivalent of the 4DCV. Finally, we present a cascaded Decoder-based technique for alternately refining the crossattention and the rotation query. Our approach outperforms current state-of-the-art methods on extreme rotation estimation. We make our code publicly available1. 1. Introduction Estimating the relative pose between a pair of images is a crucial task in computer vision, which is used in various applications such as indoor navigation, augmented reality, autonomous driving, 3D reconstruction [40, 44], camera localization [5, 45, 47], simultaneous localization and mapping [12, 38], and novel view synthesis [35, 42]. The current approach to image registration involves extracting features, matching them, and establishing correspondence between them. However, this approach is ineffective for input pairs with little or no overlap, making it difficult to establish sufficient feature correspondences for matching, such as in the images shown in Fig. 1. Numerous applications [1, 32, 49] necessitate precise estimation of inter-image rotations. The prevalent approach for extreme 3D rotation estimation between images with limited or no overlap, as in Fig. 1, relates to the seminal work of Coughlan and Yuille [10]. They introduced a technique premised on linear structures within an image, primarily arising from three mutually orthogonal directions Figure 1. The estimation of extreme 3D image rotations. First row: Images pair with a small overlap. Second row: non-overlapping image pairs. The proposed scheme estimates the relative rotation between image pairs. - one vertical (building walls) and two horizontal (ground pavements, roads, etc.). Similarly, ”Single View Metrology” by Criminisi et al. [11] and extensions [26, 41, 61] utilize parallel image lines and corresponding vanishing points [19] for camera calibration. Furthermore, relative camera rotation can be estimated via illumination cues [2], by analyzing lighting and shadow directions. In this work, we propose a deep-learning approach for estimating significant, extreme inter-image rotations. Unlike classical formulations [10, 11] that explicitly detect hand-crafted cues such as lines, shadows, and vanishing points, our method directly regresses the relative rotation from input images through a deep neural network. Inspired by recent successful applications of Transformers [53] in computer vision tasks including object detection [8] and imThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 2588",
        "chosen": "This paper proposes an attention-based approach with novel components for estimating extreme 3D image rotations. The method uses inter-image distillation and cross-attention to outperform state-of-the-art rotation estimation techniques.",
        "rejected": "An attention-based approach for estimating large inter-image rotations is proposed, involving inter-image distillation and Encoder-based cross-attention to enhance 4D correlation volume computation. A cascaded Decoder-based refinement technique alternately refines cross-attention and rotation query, outperforming state-of-the-art methods on extreme rotation estimation."
    },
    {
        "prompt": "summary: Existing 3D scene understanding methods are heavily focused on 3D semantic and instance segmentation. However, identifying objects and their parts only constitutes an intermediate step towards a more ﬁne-grained goal, which is effectively interacting with the functional interactive elements (e.g., handles, knobs, buttons) in the scene to accomplish diverse tasks. To this end, we introduce SceneFun3D, a large-scale dataset with more than 14.8k highly accurate interaction annotations for 710 high-resolution realworld 3D indoor scenes. We accompany the annotations with motion parameter information, describing how to interact with these elements, and a diverse set of natural language descriptions of tasks that involve manipulating them in the scene context. To showcase the value of our dataset, we introduce three novel tasks, namely functionality segmentation, task-driven affordance grounding and 3D motion estimation, and adapt existing state-of-the-art methods to tackle them. Our experiments show that solving these tasks in real 3D scenes remains challenging despite recent progress in closed-set and open-set 3D scene understanding methods. 1. Introduction Datasets of 3D indoor environments have been extensively used for computer vision, robotics, embodied AI and mixed reality [5, 7, 11]. To perceive 3D environments, 3D instance segmentation has served as a fundamental task to provide object-level knowledge to agents enabling scene interaction. Going a step further, a line of work [46, 49] has studied the task of part-object segmentation focusing on the lower-level object parts, e.g., the drawers of a cabinet. However, these two tasks serve only as a proxy since in the real-world setting, agents need to successfully detect and interact with the functional interactive elements (e.g., knobs, handles, buttons) of the objects in the scene. Detecting these elements This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 14531",
        "chosen": "This paper introduces SceneFun3D, a dataset with detailed interaction annotations for 3D indoor scenes. The dataset supports tasks like functionality segmentation and 3D motion estimation, highlighting challenges in real-world scene understanding.",
        "rejected": "SceneFun3D introduces a dataset with interaction annotations for real-world 3D indoor scenes, including motion parameters and natural language task descriptions. Novel tasks such as functionality segmentation and task-driven affordance grounding are introduced, showcasing the complexity of interacting with functional elements in 3D environments despite recent progress in scene understanding."
    },
    {
        "prompt": "summary: Recently, building on the foundation of neural radiance ﬁeld, various techniques have emerged to learn unsigned distance ﬁelds (UDF) to reconstruct 3D non-watertight models from multi-view images. Yet, a central challenge in UDF-based volume rendering is formulating a proper way to convert unsigned distance values into volume density, ensuring that the resulting weight function remains unbiased and sensitive to occlusions. Falling short on these requirements often results in incorrect topology or large reconstruction errors in resulting models. This paper addresses this challenge by presenting a novel two-stage algorithm, 2S-UDF, for learning a high-quality UDF from multi-view images. Initially, the method applies an easily trainable density function that, while slightly biased and transparent, aids in coarse reconstruction. The subsequent stage then reﬁnes the geometry and appearance of the object to achieve a high-quality reconstruction by directly adjusting the weight function used in volume rendering to ensure that it is unbiased and occlusion-aware. Decoupling density and weight in two stages makes our training stable and robust, distinguishing our technique from existing UDF learning approaches. Evaluations on the DeepFashion3D, DTU, and BlendedMVS datasets validate the robustness and effectiveness of our proposed approach. In both quantitative metrics and visual quality, the results indicate our superior performance over other UDF learning techniques in reconstructing 3D non-watertight models from multi-view images. Our code is available at https: //bitbucket.org/jkdeng/2sudf/. 1. Introduction As the success of neural radiance ﬁeld (NeRF) [29], numerous volume rendering based 3D modeling methods are pro*Corresponding author GT Ours NeuralUDF NeUDF Figure 1. We learn a UDF from multiview images for nonwatertight model reconstruction. As illustrated in the cross sections of learned UDFs, our learned UDF approximates to the ground truth. In contrast, the learned UDF of NeuralUDF [25] is choppy leading to signiﬁcant artifacts, e.g., unexpected pit. The learned UDF of NeUDF [23] is almost closed struggling to generate open surface. posed to learn signed distance ﬁelds (SDF) for 3D model reconstruction from multi-view images [7, 34, 36, 40]. These approaches map signed distance value to a density function, thereby enabling the use of volume rendering to learn an implicit SDF representation. To calculate pixel colors, they compute the weighted sum of radiances along each light ray. Achieving an accurate surface depiction requires the density function to meet three essential criteria. Firstly, the weights, which are derived from the density function, must reach their maximum value when the distance is zero, ensuring unbiasedness. Secondly, as a ray traverses through the surface, the accumulated density should tend towards inﬁnity, rendering the surface opaque — a property referred to as occlusion-awareness. Finally, the density function should be bounded to prevent numerical issues. The popular SDF approaches, such as NeuS [34] and VolSDF [40], adopt an S-shaped density function that meets all these requirements. While SDF-based methods excel at reconstructing watertight models, they have limitations in representing open models. This is due to the intrinsic nature of SDF, which This CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore. 5084",
        "chosen": "This paper presents 2S-UDF, a two-stage algorithm for learning unsigned distance fields from multi-view images. The method improves reconstruction of non-watertight models by decoupling density and weight functions, ensuring unbiased and occlusion-aware results.",
        "rejected": "Building on neural radiance fields, this work explores unsigned distance fields (UDFs) for reconstructing non-watertight 3D models. Challenges in converting unsigned distances to volume density are addressed to ensure unbiased weight functions sensitive to occlusions, aiming to improve topology correctness and reduce reconstruction errors."
    }
]